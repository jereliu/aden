
@article{zhan_fast_2017,
	title = {A fast small-sample kernel independence test for microbiome community-level association analysis},
	volume = {73},
	issn = {1541-0420},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/biom.12684/abstract},
	doi = {10.1111/biom.12684},
	abstract = {To fully understand the role of microbiome in human health and diseases, researchers are increasingly interested in assessing the relationship between microbiome composition and host genomic data. The dimensionality of the data as well as complex relationships between microbiota and host genomics pose considerable challenges for analysis. In this article, we apply a kernel RV coefficient (KRV) test to evaluate the overall association between host gene expression and microbiome composition. The KRV statistic can capture nonlinear correlations and complex relationships among the individual data types and between gene expression and microbiome composition through measuring general dependency. Testing proceeds via a similar route as existing tests of the generalized RV coefficients and allows for rapid p-value calculation. Strategies to allow adjustment for confounding effects, which is crucial for avoiding misleading results, and to alleviate the problem of selecting the most favorable kernel are considered. Simulation studies show that KRV is useful in testing statistical independence with finite samples given the kernels are appropriately chosen, and can powerfully identify existing associations between microbiome composition and host genomic data while protecting type I error. We apply the KRV to a microbiome study examining the relationship between host transcriptome and microbiome composition within the context of inflammatory bowel disease and are able to derive new biological insights and provide formal inference on prior qualitative observations.},
	language = {en},
	number = {4},
	urldate = {2018-03-04},
	journal = {Biometrics},
	author = {Zhan, Xiang and Plantinga, Anna and Zhao, Ni and Wu, Michael C.},
	month = dec,
	year = {2017},
	keywords = {Kernel, Microbiome composition, Multivariate association test, Omnibus test, RV coefficient},
	pages = {1453--1463},
	file = {Snapshot:/home/jeremiah/Zotero/storage/JFYX5X5Q/abstract.html:text/html}
}

@article{seoane_pathway-based_2014,
	title = {A pathway-based data integration framework for prediction of disease progression},
	volume = {30},
	issn = {1367-4803},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3957070/},
	doi = {10.1093/bioinformatics/btt610},
	abstract = {Motivation: Within medical research there is an increasing trend toward deriving multiple types of data from the same individual. The most effective prognostic prediction methods should use all available data, as this maximizes the amount of information used. In this article, we consider a variety of learning strategies to boost prediction performance based on the use of all available data., Implementation: We consider data integration via the use of multiple kernel learning supervised learning methods. We propose a scheme in which feature selection by statistical score is performed separately per data type and by pathway membership. We further consider the introduction of a confidence measure for the class assignment, both to remove some ambiguously labeled datapoints from the training data and to implement a cautious classifier that only makes predictions when the associated confidence is high., Results: We use the METABRIC dataset for breast cancer, with prediction of survival at 2000 days from diagnosis. Predictive accuracy is improved by using kernels that exclusively use those genes, as features, which are known members of particular pathways. We show that yet further improvements can be made by using a range of additional kernels based on clinical covariates such as Estrogen Receptor (ER) status. Using this range of measures to improve prediction performance, we show that the test accuracy on new instances is nearly 80\%, though predictions are only made on 69.2\% of the patient cohort., Availability:
https://github.com/jseoane/FSMKL, Contact:
J.Seoane@bristol.ac.uk, Supplementary information:
Supplementary data are available at Bioinformatics online.},
	number = {6},
	urldate = {2018-03-04},
	journal = {Bioinformatics},
	author = {Seoane, José A. and Day, Ian N. M. and Gaunt, Tom R. and Campbell, Colin},
	month = mar,
	year = {2014},
	pmid = {24162466},
	pmcid = {PMC3957070},
	pages = {838--845},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/STYUCM72/Seoane et al. - 2014 - A pathway-based data integration framework for pre.pdf:application/pdf}
}

@article{buzkova_permutation_2011,
	title = {Permutation and parametric bootstrap tests for gene—gene and gene—environment interactions},
	volume = {75},
	issn = {0003-4800},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2904826/},
	doi = {10.1111/j.1469-1809.2010.00572.x},
	abstract = {Permutation tests are widely used in genomic research as a straightforward way to obtain reliable statistical inference without making strong distributional assumptions. However, in this paper we show that in genetic association studies it is not typically possible to construct exact permutation tests of gene-gene or gene-environment interaction hypotheses. We describe an alternative to the permutation approach in testing for interaction, a parametric bootstrap approach. Using simulations, we compare the finite-sample properties of a few often-used permutation tests and the parametric bootstrap. We consider interactions of an exposure with single and multiple polymorphisms. Finally, we address when permutation tests of interaction will be approximately valid in large samples for specific test statistics.},
	number = {1},
	urldate = {2018-03-03},
	journal = {Annals of human genetics},
	author = {Bůžková, Petra and Lumley, Thomas and Rice, Kenneth},
	month = jan,
	year = {2011},
	pmid = {20384625},
	pmcid = {PMC2904826},
	pages = {36--45},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/Z2QY6R8L/Bůžková et al. - 2011 - Permutation and parametric bootstrap tests for gen.pdf:application/pdf}
}

@inproceedings{cortes_ensembles_2011,
	address = {Corvallis, Oregon},
	title = {Ensembles of {Kernel} {Predictors}},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {Conference} {Annual} {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI}-11)},
	publisher = {AUAI Press},
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	year = {2011},
	pages = {145--152}
}

@misc{noauthor_uai_nodate,
	title = {{UAI} - {Uncertainty} in {Artificial} {Intelligence}},
	url = {https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=2188&proceeding_id=27},
	urldate = {2018-03-03},
	file = {UAI - Uncertainty in Artificial Intelligence:/home/jeremiah/Zotero/storage/29X38IX6/displayArticleDetails.html:text/html}
}

@article{cortes_algorithms_2012,
	title = {Algorithms for {Learning} {Kernels} {Based} on {Centered} {Alignment}},
	volume = {13},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v13/cortes12a.html},
	number = {Mar},
	urldate = {2018-03-03},
	journal = {Journal of Machine Learning Research},
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	year = {2012},
	pages = {795--828},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/SHZZ29N6/Cortes et al. - 2012 - Algorithms for Learning Kernels Based on Centered .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/CKUPSB7D/cortes12a.html:text/html}
}

@article{cai_kernel_2011,
	title = {Kernel {Machine} {Approach} to {Testing} the {Significance} of {Multiple} {Genetic} {Markers} for {Risk} {Prediction}},
	volume = {67},
	issn = {1541-0420},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1541-0420.2010.01544.x/abstract},
	doi = {10.1111/j.1541-0420.2010.01544.x},
	abstract = {Summary There is growing evidence that genomic and proteomic research holds great potential for changing irrevocably the practice of medicine. The ability to identify important genomic and biological markers for risk assessment can have a great impact in public health from disease prevention, to detection, to treatment selection. However, the potentially large number of markers and the complexity in the relationship between the markers and the outcome of interest impose a grand challenge in developing accurate risk prediction models. The standard approach to identifying important markers often assesses the marginal effects of individual markers on a phenotype of interest. When multiple markers relate to the phenotype simultaneously via a complex structure, such a type of marginal analysis may not be effective. To overcome such difficulties, we employ a kernel machine Cox regression framework and propose an efficient score test to assess the overall effect of a set of markers, such as genes within a pathway or a network, on survival outcomes. The proposed test has the advantage of capturing the potentially nonlinear effects without explicitly specifying a particular nonlinear functional form. To approximate the null distribution of the score statistic, we propose a simple resampling procedure that can be easily implemented in practice. Numerical studies suggest that the test performs well with respect to both empirical size and power even when the number of variables in a gene set is not small compared to the sample size.},
	language = {en},
	number = {3},
	urldate = {2018-03-02},
	journal = {Biometrics},
	author = {Cai, Tianxi and Tonini, Giulia and Lin, Xihong},
	month = sep,
	year = {2011},
	keywords = {Gene-set analysis, Genetic association, Genetic pathways, Kernel machine, Kernel PCA, Risk prediction, Score test, Survival analysis},
	pages = {975--986},
	file = {Snapshot:/home/jeremiah/Zotero/storage/LZ62WBP5/abstract.html:text/html}
}

@article{james_concise_2014,
	title = {Concise {Probability} {Distributions} of {Eigenvalues} of {Real}-{Valued} {Wishart} {Matrices}},
	url = {http://arxiv.org/abs/1402.6757},
	abstract = {In this paper, we consider the problem of deriving new eigenvalue distributions of real-valued Wishart matrices that arises in many scientific and engineering applications. The distributions are derived using the tools from the theory of skew symmetric matrices. In particular, we relate the multiple integrals of a determinant, which arises while finding the eigenvalue distributions, in terms of the Pfaffian of skew-symmetric matrices. Pfaffians being the square root of skew symmetric matrices are easy to compute than the conventional distributions that involve Zonal polynomials or beta integrals. We show that the plots of the derived distributions are exactly coinciding with the numerically simulated plots.},
	urldate = {2018-02-26},
	journal = {arXiv:1402.6757 [cs, math]},
	author = {James, Oliver and Lee, Heung-No},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.6757},
	keywords = {Computer Science - Information Theory},
	annote = {Comment: Submitted to Math Journal, 7 pages},
	file = {arXiv\:1402.6757 PDF:/home/jeremiah/Zotero/storage/VZH7TQ7N/James and Lee - 2014 - Concise Probability Distributions of Eigenvalues o.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/CLTW2GV8/1402.html:text/html}
}

@book{anderson_introduction_1960,
	edition = {Second Printing edition},
	title = {An {Introduction} to {Multivariate} {Statistical} {Analysis}},
	publisher = {John Wiley and Sons},
	author = {Anderson, T. W.},
	month = jan,
	year = {1960}
}

@article{ning_general_2014,
	title = {A {General} {Theory} of {Hypothesis} {Tests} and {Confidence} {Regions} for {Sparse} {High} {Dimensional} {Models}},
	url = {http://arxiv.org/abs/1412.8765},
	abstract = {We consider the problem of uncertainty assessment for low dimensional components in high dimensional models. Specifically, we propose a decorrelated score function to handle the impact of high dimensional nuisance parameters. We consider both hypothesis tests and confidence regions for generic penalized M-estimators. Unlike most existing inferential methods which are tailored for individual models, our approach provides a general framework for high dimensional inference and is applicable to a wide range of applications. From the testing perspective, we develop general theorems to characterize the limiting distributions of the decorrelated score test statistic under both null hypothesis and local alternatives. These results provide asymptotic guarantees on the type I errors and local powers of the proposed test. Furthermore, we show that the decorrelated score function can be used to construct point and confidence region estimators that are semiparametrically efficient. We also generalize this framework to broaden its applications. First, we extend it to handle high dimensional null hypothesis, where the number of parameters of interest can increase exponentially fast with the sample size. Second, we establish the theory for model misspecification. Third, we go beyond the likelihood framework, by introducing the generalized score test based on general loss functions. Thorough numerical studies are conducted to back up the developed theoretical results.},
	urldate = {2018-02-26},
	journal = {arXiv:1412.8765 [stat]},
	author = {Ning, Yang and Liu, Han},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.8765},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: 80 pages, 1 figure, 3 tables},
	file = {arXiv\:1412.8765 PDF:/home/jeremiah/Zotero/storage/XM38MWEE/Ning and Liu - 2014 - A General Theory of Hypothesis Tests and Confidenc.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/SPRDM68Z/1412.html:text/html}
}

@article{chiani_probability_2017,
	title = {On the {Probability} {That} {All} {Eigenvalues} of {Gaussian}, {Wishart}, and {Double} {Wishart} {Random} {Matrices} {Lie} {Within} an {Interval}},
	volume = {63},
	issn = {0018-9448},
	doi = {10.1109/TIT.2017.2694846},
	abstract = {We derive the probability that all eigenvalues of a random matrix M lie within an arbitrary interval [a, b], ψ(a, b) Pra λmin(M), λmax(M) b, when M is a real or complex finite-dimensional Wishart, double Wishart, or Gaussian symmetric/Hermitian matrix. We give efficient recursive formulas allowing the exact evaluation of ψ(a, b) for Wishart matrices, even with a large number of variates and degrees of freedom. We also prove that the probability that all eigenvalues are within the limiting spectral support (given by the Marčenko-Pastur or the semicircle laws) tends for large dimensions to the universal values 0.6921 and 0.9397 for the real and complex cases, respectively. Applications include improved bounds for the probability that a Gaussian measurement matrix has a given restricted isometry constant in compressed sensing.},
	number = {7},
	journal = {IEEE Transactions on Information Theory},
	author = {Chiani, M.},
	month = jul,
	year = {2017},
	keywords = {complex finite-dimensional Wishart, compressed sensing, Compressed sensing, Covariance matrices, double Wishart matrix, eigenvalues and eigenfunctions, Eigenvalues and eigenfunctions, eigenvalues distribution, Gaussian eigenvalues, Gaussian measurement matrix, Gaussian orthogonal ensemble, Gaussian symmetric-Hermitian matrix, Jacobi ensemble, Jacobian matrices, Limiting, MANOVA, matrix algebra, Physics, principal component analysis, probability, random matrix, Random matrix theory, Symmetric matrices, Tracy-Widom distribution, Wishart matrices},
	pages = {4521--4531},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/A3UKDD4I/7902128.html:text/html}
}

@article{wu_rare-variant_2011,
	title = {Rare-{Variant} {Association} {Testing} for {Sequencing} {Data} with the {Sequence} {Kernel} {Association} {Test}},
	volume = {89},
	issn = {0002-9297},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/},
	doi = {10.1016/j.ajhg.2011.05.029},
	abstract = {Sequencing studies are increasingly being conducted to identify rare variants associated with complex traits. The limited power of classical single-marker association analysis for rare variants poses a central challenge in such studies. We propose the sequence kernel association test (SKAT), a supervised, flexible, computationally efficient regression method to test for association between genetic variants (common and rare) in a region and a continuous or dichotomous trait while easily adjusting for covariates. As a score-based variance-component test, SKAT can quickly calculate p values analytically by fitting the null model containing only the covariates, and so can easily be applied to genome-wide data. Using SKAT to analyze a genome-wide sequencing study of 1000 individuals, by segmenting the whole genome into 30 kb regions, requires only 7 hr on a laptop. Through analysis of simulated data across a wide range of practical scenarios and triglyceride data from the Dallas Heart Study, we show that SKAT can substantially outperform several alternative rare-variant association tests. We also provide analytic power and sample-size calculations to help design candidate-gene, whole-exome, and whole-genome sequence association studies.},
	number = {1},
	urldate = {2018-02-25},
	journal = {American Journal of Human Genetics},
	author = {Wu, Michael C. and Lee, Seunggeun and Cai, Tianxi and Li, Yun and Boehnke, Michael and Lin, Xihong},
	month = jul,
	year = {2011},
	pmid = {21737059},
	pmcid = {PMC3135811},
	pages = {82--93},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/93GFCTSE/Wu et al. - 2011 - Rare-Variant Association Testing for Sequencing Da.pdf:application/pdf}
}

@article{wang_kernel_2015,
	title = {Kernel methods for large-scale genomic data analysis},
	volume = {16},
	issn = {1467-5463},
	url = {https://academic.oup.com/bib/article/16/2/183/246779},
	doi = {10.1093/bib/bbu024},
	abstract = {Machine learning, particularly kernel methods, has been demonstrated as a promising new tool to tackle the challenges imposed by today’s explosive data growth in genomics. They provide a practical and principled approach to learning how a large number of genetic variants are associated with complex phenotypes, to help reveal the complexity in the relationship between the genetic markers and the outcome of interest. In this review, we highlight the potential key role it will have in modern genomic data processing, especially with regard to integration with classical methods for gene prioritizing, prediction and data fusion.},
	language = {en},
	number = {2},
	journal = {Briefings in Bioinformatics},
	author = {Wang, Xuefeng and Xing, Eric P. and Schaid, Daniel J.},
	month = mar,
	year = {2015},
	pages = {183--192},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/3FA4LKHJ/Wang et al. - 2015 - Kernel methods for large-scale genomic data analys.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/2J42S46L/246779.html:text/html}
}

@misc{noauthor_cityscapes_nodate,
	title = {Cityscapes {Dataset}},
	url = {https://www.cityscapes-dataset.com/},
	abstract = {The Cityscapes Dataset is a large-scale dataset for semantic urban scene understanding that contains a diverse set of stereo video recordings from 50 cities},
	language = {en-US},
	journal = {Cityscapes Dataset},
	file = {Snapshot:/home/jeremiah/Zotero/storage/G8UT2N5K/www.cityscapes-dataset.com.html:text/html}
}

@inproceedings{zhou_unsupervised_2017,
	title = {Unsupervised {Learning} of {Depth} and {Ego}-{Motion} from {Video}},
	doi = {10.1109/CVPR.2017.700},
	abstract = {We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work [10, 14, 16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhou, T. and Brown, M. and Snavely, N. and Lowe, D. G.},
	month = jul,
	year = {2017},
	keywords = {camera motion estimation, cameras, Cameras, ego-motion, end-to-end learning approach, Geometry, ground-truth pose, image motion analysis, image sequences, KITTI dataset, learning (artificial intelligence), monocular depth, monocular video sequences, motion estimation, object detection, Pipelines, pose estimation, Pose estimation, supervisory signal, Three-dimensional displays, Training, unstructured video sequences, unsupervised learning, unsupervised learning framework, video signal processing},
	pages = {6612--6619},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/P4GC3QNJ/8100183.html:text/html}
}

@article{zhan_fast_2017-1,
	title = {A fast small-sample kernel independence test for microbiome community-level association analysis},
	volume = {73},
	issn = {1541-0420},
	doi = {10.1111/biom.12684},
	abstract = {To fully understand the role of microbiome in human health and diseases, researchers are increasingly interested in assessing the relationship between microbiome composition and host genomic data. The dimensionality of the data as well as complex relationships between microbiota and host genomics pose considerable challenges for analysis. In this article, we apply a kernel RV coefficient (KRV) test to evaluate the overall association between host gene expression and microbiome composition. The KRV statistic can capture nonlinear correlations and complex relationships among the individual data types and between gene expression and microbiome composition through measuring general dependency. Testing proceeds via a similar route as existing tests of the generalized RV coefficients and allows for rapid p-value calculation. Strategies to allow adjustment for confounding effects, which is crucial for avoiding misleading results, and to alleviate the problem of selecting the most favorable kernel are considered. Simulation studies show that KRV is useful in testing statistical independence with finite samples given the kernels are appropriately chosen, and can powerfully identify existing associations between microbiome composition and host genomic data while protecting type I error. We apply the KRV to a microbiome study examining the relationship between host transcriptome and microbiome composition within the context of inflammatory bowel disease and are able to derive new biological insights and provide formal inference on prior qualitative observations.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Zhan, Xiang and Plantinga, Anna and Zhao, Ni and Wu, Michael C.},
	month = dec,
	year = {2017},
	pmid = {28295177},
	pmcid = {PMC5592124},
	keywords = {Kernel, Microbiome composition, Multivariate association test, Omnibus test, RV coefficient},
	pages = {1453--1463}
}

@article{godard_unsupervised_2016,
	title = {Unsupervised {Monocular} {Depth} {Estimation} with {Left}-{Right} {Consistency}},
	url = {http://arxiv.org/abs/1609.03677},
	abstract = {Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.},
	urldate = {2018-02-23},
	journal = {arXiv:1609.03677 [cs, stat]},
	author = {Godard, Clément and Mac Aodha, Oisin and Brostow, Gabriel J.},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03677},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: CVPR 2017 oral},
	file = {arXiv\:1609.03677 PDF:/home/jeremiah/Zotero/storage/JGNB8NMH/Godard et al. - 2016 - Unsupervised Monocular Depth Estimation with Left-.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/VUAPN5R8/1609.html:text/html}
}

@article{eigen_depth_2014,
	title = {Depth {Map} {Prediction} from a {Single} {Image} using a {Multi}-{Scale} {Deep} {Network}},
	url = {http://arxiv.org/abs/1406.2283},
	abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
	urldate = {2018-02-23},
	journal = {arXiv:1406.2283 [cs]},
	author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2283},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1406.2283 PDF:/home/jeremiah/Zotero/storage/TF6EE3N4/Eigen et al. - 2014 - Depth Map Prediction from a Single Image using a M.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/SUG5EJWV/1406.html:text/html}
}

@misc{noauthor_scannet_nodate,
	title = {{ScanNet} {\textbar} {Richly}-annotated 3D {Reconstructions} of {Indoor} {Scenes}},
	url = {http://www.scan-net.org/},
	urldate = {2018-02-23}
}

@misc{noauthor_robust_nodate,
	title = {Robust {Vision} {Challenge} 2018},
	url = {http://www.robustvision.net/},
	urldate = {2018-02-23},
	file = {Robust Vision Challenge 2018:/home/jeremiah/Zotero/storage/A6KWXAVL/www.robustvision.net.html:text/html}
}

@misc{europe_proxy_nodate,
	title = {Proxy {Virtual} {Worlds}},
	url = {http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds, http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds},
	abstract = {Proxy Virtual Worlds},
	author = {Europe, NAVER LABS},
	file = {Snapshot:/home/jeremiah/Zotero/storage/LZV3EAU7/Proxy-Virtual-Worlds.html:text/html}
}

@misc{noauthor_synthia_nodate,
	title = {{SYNTHIA} {Dataset}},
	url = {http://synthia-dataset.net/},
	language = {en-US}
}

@article{firman_rgbd_2016,
	title = {{RGBD} {Datasets}: {Past}, {Present} and {Future}},
	shorttitle = {{RGBD} {Datasets}},
	url = {http://arxiv.org/abs/1604.00999},
	abstract = {Since the launch of the Microsoft Kinect, scores of RGBD datasets have been released. These have propelled advances in areas from reconstruction to gesture recognition. In this paper we explore the field, reviewing datasets across eight categories: semantics, object pose estimation, camera tracking, scene reconstruction, object tracking, human actions, faces and identification. By extracting relevant information in each category we help researchers to find appropriate data for their needs, and we consider which datasets have succeeded in driving computer vision forward and why. Finally, we examine the future of RGBD datasets. We identify key areas which are currently underexplored, and suggest that future directions may include synthetic data and dense reconstructions of static and dynamic scenes.},
	urldate = {2018-02-23},
	journal = {arXiv:1604.00999 [cs]},
	author = {Firman, Michael},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00999},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: 8 pages excluding references (CVPR style)},
	file = {arXiv\:1604.00999 PDF:/home/jeremiah/Zotero/storage/FQCCJ6SI/Firman - 2016 - RGBD Datasets Past, Present and Future.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/2M97FG4M/1604.html:text/html}
}

@article{chen_single-image_2016,
	title = {Single-{Image} {Depth} {Perception} in the {Wild}},
	url = {http://arxiv.org/abs/1604.03901},
	abstract = {This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset "Depth in the Wild" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.},
	urldate = {2018-02-23},
	journal = {arXiv:1604.03901 [cs]},
	author = {Chen, Weifeng and Fu, Zhao and Yang, Dawei and Deng, Jia},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.03901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv\:1604.03901 PDF:/home/jeremiah/Zotero/storage/ETI8WT6P/Chen et al. - 2016 - Single-Image Depth Perception in the Wild.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/T5CI92Y3/1604.html:text/html}
}

@misc{noauthor_kitti_nodate,
	title = {The {KITTI} {Vision} {Benchmark} {Suite}},
	url = {http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction},
	urldate = {2018-02-23},
	file = {The KITTI Vision Benchmark Suite:/home/jeremiah/Zotero/storage/FG2SSS7A/eval_depth.html:text/html}
}

@misc{noauthor_cornell_nodate,
	title = {Cornell {Personal} {Robotics}: {RGB}-{D} {Human} {Activity} {Datasets} ({CAD}-60, {CAD}-120)},
	url = {http://pr.cs.cornell.edu/humanactivities/data.php},
	urldate = {2018-02-23},
	file = {Cornell Personal Robotics\: RGB-D Human Activity Datasets (CAD-60, CAD-120):/home/jeremiah/Zotero/storage/CYGLFDNE/data.html:text/html}
}

@article{xue_video_2017,
	title = {Video {Enhancement} with {Task}-{Oriented} {Flow}},
	url = {http://arxiv.org/abs/1711.09078},
	abstract = {Many video processing algorithms rely on optical flow to register different frames within a sequence. However, a precise estimation of optical flow is often neither tractable nor optimal for a particular task. In this paper, we propose task-oriented flow (TOFlow), a flow representation tailored for specific video processing tasks. We design a neural network with a motion estimation component and a video processing component. These two parts can be jointly trained in a self-supervised manner to facilitate learning of the proposed TOFlow. We demonstrate that TOFlow outperforms the traditional optical flow on three different video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution. We also introduce Vimeo-90K, a large-scale, high-quality video dataset for video processing to better evaluate the proposed algorithm.},
	urldate = {2018-02-23},
	journal = {arXiv:1711.09078 [cs]},
	author = {Xue, Tianfan and Chen, Baian and Wu, Jiajun and Wei, Donglai and Freeman, William T.},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09078},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project page: http://toflow.csail.mit.edu},
	file = {arXiv\:1711.09078 PDF:/home/jeremiah/Zotero/storage/JB8UBVQ9/Xue et al. - 2017 - Video Enhancement with Task-Oriented Flow.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/Y9B2489Q/1711.html:text/html}
}

@article{manuck_gene-environment_2014,
	title = {Gene-environment interaction},
	volume = {65},
	issn = {1545-2085},
	doi = {10.1146/annurev-psych-010213-115100},
	abstract = {With the advent of increasingly accessible technologies for typing genetic variation, studies of gene-environment (G×E) interactions have proliferated in psychological research. Among the aims of such studies are testing developmental hypotheses and models of the etiology of behavioral disorders, defining boundaries of genetic and environmental influences, and identifying individuals most susceptible to risk exposures or most amenable to preventive and therapeutic interventions. This research also coincides with the emergence of unanticipated difficulties in detecting genetic variants of direct association with behavioral traits and disorders, which may be obscured if genetic effects are expressed only in predisposing environments. In this essay we consider these and other rationales for positing G×E interactions, review conceptual models meant to inform G×E interpretations from a psychological perspective, discuss points of common critique to which G×E research is vulnerable, and address the role of the environment in G×E interactions.},
	language = {eng},
	journal = {Annual Review of Psychology},
	author = {Manuck, Stephen B. and McCaffery, Jeanne M.},
	year = {2014},
	pmid = {24405358},
	keywords = {Humans, Genetic Predisposition to Disease, Phenotype, Environment, Gene-Environment Interaction, Genetic Variation, Mental Disorders, Models, Psychological},
	pages = {41--70}
}

@article{zhang_genome-wide_2014,
	title = {A genome-wide gene–environment interaction analysis for tobacco smoke and lung cancer susceptibility},
	volume = {35},
	issn = {0143-3334},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4076813/},
	doi = {10.1093/carcin/bgu076},
	abstract = {Tobacco smoke is the major environmental risk factor underlying lung carcinogenesis. However, approximately one-tenth smokers develop lung cancer in their lifetime indicating there is significant individual variation in susceptibility to lung cancer. And, the reasons for this are largely unknown. In particular, the genetic variants discovered in genome-wide association studies (GWAS) account for only a small fraction of the phenotypic variations for lung cancer, and gene–environment interactions are thought to explain the missing fraction of disease heritability. The ability to identify smokers at high risk of developing cancer has substantial preventive implications. Thus, we undertook a gene–smoking interaction analysis in a GWAS of lung cancer in Han Chinese population using a two-phase designed case–control study. In the discovery phase, we evaluated all pair-wise (591 370) gene–smoking interactions in 5408 subjects (2331 cases and 3077 controls) using a logistic regression model with covariate adjustment. In the replication phase, promising interactions were validated in an independent population of 3023 subjects (1534 cases and 1489 controls). We identified interactions between two single nucleotide polymorphisms and smoking. The interaction P values are 6.73 × 10−
6 and 3.84 × 10−
6 for rs1316298 and rs4589502, respectively, in the combined dataset from the two phases. An antagonistic interaction (rs1316298–smoking) and a synergetic interaction (rs4589502–smoking) were observed. The two interactions identified in our study may help explain some of the missing heritability in lung cancer susceptibility and present strong evidence for further study of these gene–smoking interactions, which are benefit to intensive screening and smoking cessation interventions.},
	number = {7},
	urldate = {2018-02-22},
	journal = {Carcinogenesis},
	author = {Zhang, Ruyang and Chu, Minjie and Zhao, Yang and Wu, Chen and Guo, Huan and Shi, Yongyong and Dai, Juncheng and Wei, Yongyue and Jin, Guangfu and Ma, Hongxia and Dong, Jing and Yi, Honggang and Bai, Jianling and Gong, Jianhang and Sun, Chongqi and Zhu, Meng and Wu, Tangchun and Hu, Zhibin and Lin, Dongxin and Shen, Hongbing and Chen, Feng},
	month = jul,
	year = {2014},
	pmid = {24658283},
	pmcid = {PMC4076813},
	pages = {1528--1535},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/ZD49MQ7T/Zhang et al. - 2014 - A genome-wide gene–environment interaction analysi.pdf:application/pdf}
}

@article{rezende_variational_2015,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1505.05770},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	urldate = {2018-02-22},
	journal = {arXiv:1505.05770 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05770},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Proceedings of the 32nd International Conference on Machine Learning},
	file = {arXiv\:1505.05770 PDF:/home/jeremiah/Zotero/storage/UPZ7IQBM/Rezende and Mohamed - 2015 - Variational Inference with Normalizing Flows.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/42MEY3SX/1505.html:text/html}
}

@article{zanella_marginal_2009,
	title = {On the marginal distribution of the eigenvalues of wishart matrices},
	volume = {57},
	issn = {0090-6778},
	doi = {10.1109/TCOMM.2009.04.070143},
	abstract = {Random matrices play a crucial role in the design and analysis of multiple-input multiple-output (MIMO) systems. In particular, performance of MIMO systems depends on the statistical properties of a subclass of random matrices known as Wishart when the propagation environment is characterized by Rayleigh or Rician fading. This paper focuses on the stochastic analysis of this class of matrices and proposes a general methodology to evaluate some multiple nested integrals of interest. With this methodology we obtain a closed-form expression for the joint probability density function of k consecutive ordered eigenvalues and, as a special case, the PDF of the lscrth ordered eigenvalue of Wishart matrices. The distribution of the largest eigenvalue can be used to analyze the performance of MIMO maximal ratio combining systems. The PDF of the smallest eigenvalue can be used for MIMO antenna selection techniques. Finally, the PDF the kth largest eigenvalue finds applications in the performance analysis of MIMO singular value decomposition systems.},
	number = {4},
	journal = {IEEE Transactions on Communications},
	author = {Zanella, A. and Chiani, M. and Win, M. Z.},
	month = apr,
	year = {2009},
	keywords = {eigenvalues and eigenfunctions, Eigenvalues and eigenfunctions, matrix algebra, probability, random matrix, antenna arrays, closed-form expression, Closed-form solution, diversity reception, Diversity reception, eigenvalue, Matrix decomposition, maximal ratio combining system, MIMO, MIMO antenna selection technique, MIMO communication, Multiple-input multiple-output (MIMO), Wishart matrices, eigenvalue distribution, marginal distribution, multiple-input multiple-output system, Performance analysis, probability density function, Probability density function, Rayleigh channel, Rayleigh channels, Rician channels, Rician fading channel, Singular value decomposition, statistical property, stochastic analysis, stochastic processes, Stochastic processes, Wishart matrix},
	pages = {1050--1060},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/DLSK9DVW/4814372.html:text/html}
}

@article{chinnappan_knowledge_2012,
	title = {{KNOWLEDGE} {USE} {IN} {THE} {CONSTRUCTION} {OF} {GEOMETRY} {PROOF} {BY} {SRI} {LANKAN} {STUDENTS}},
	volume = {10},
	issn = {1571-0068, 1573-1774},
	url = {https://link.springer.com/article/10.1007/s10763-011-9298-8},
	doi = {10.1007/s10763-011-9298-8},
	abstract = {ABSTRACTWithin the domain of geometry, proof and proof development continues to be a problematic area for students. Battista (2007) suggested that the investigation of knowledge components that students bring to understanding and constructing geometry proofs could provide important insights into the above issue. This issue also features prominently in the deliberations of the 2009 International Commission on Mathematics Instruction Study on the learning and teaching of proofs in mathematics, in general, and geometry, in particular. In the study reported here, we consider knowledge use by a cohort of 166 Sri Lankan students during the construction of geometry proofs. Three knowledge components were hypothesised to influence the students’ attempts at proof development: geometry content knowledge, general problem-solving skills and geometry reasoning skills. Regression analyses supported our conjecture that all 3 knowledge components played important functions in developing proofs. We suggest that whilst students have to acquire a robust body of geometric content knowledge, the activation and the utilisation of this knowledge during the construction of proof need to be guided by general problem-solving and reasoning skills.},
	language = {en},
	number = {4},
	journal = {International Journal of Science and Mathematics Education},
	author = {Chinnappan, Mohan and Ekanayake, Madduma B. and Brown, Christine},
	month = aug,
	year = {2012},
	pages = {865--887},
	file = {Snapshot:/home/jeremiah/Zotero/storage/6BZD26V6/s10763-011-9298-8.html:text/html}
}

@incollection{guin_cognitive_1996,
	series = {{NATO} {ASI} {Series}},
	title = {A {Cognitive} {Analysis} of {Geometry} {Proof} {Focused} on {Intelligent} {Tutoring} {Systems}},
	isbn = {978-3-642-64608-9 978-3-642-60927-5},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-60927-5_6},
	abstract = {The elaboration of an Intelligent Tutoring System in geometry proof requires a model of geometry proof problem solving. Actually, the development of these models is still directed by the system’s processes and not by a study of human behaviour. Here we present elements of a cognitive and didactical analysis of proof in geometry that must be taken in account in such a model. This study was carried out with mathematic teachers (GIA: Artificial Intelligence group, IREM Strasbourg).},
	language = {en},
	booktitle = {Intelligent {Learning} {Environments}: {The} {Case} of {Geometry}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Guin, Dominique},
	year = {1996},
	doi = {10.1007/978-3-642-60927-5_6},
	pages = {82--93},
	file = {Snapshot:/home/jeremiah/Zotero/storage/ZFMKWDAV/978-3-642-60927-5_6.html:text/html}
}

@incollection{tall_cognitive_2011,
	series = {New {ICMI} {Study} {Series}},
	title = {Cognitive {Development} of {Proof}},
	isbn = {978-94-007-2128-9 978-94-007-2129-6},
	url = {https://link.springer.com/chapter/10.1007/978-94-007-2129-6_2},
	abstract = {This chapter traces the long-term cognitive development of mathematical proof from the young child to the frontiers of research. It uses a framework building from perception and action, through proof by embodied actions and classifications, geometric proof and operational proof in arithmetic and algebra, to the formal set-theoretic definition and formal deduction. In each context, proof develops over the long-term from the recognition and description of observed properties and the links between them, the selection of specific properties that can be used as definitions from which other properties may be deduced, to the construction of ‘crystalline concepts’ whose properties are a consequence of the context. These include Platonic objects in geometry, symbols having relationships in arithmetic and algebra and formal axiomatic systems whose properties are determined by their definitions.},
	language = {en},
	booktitle = {Proof and {Proving} in {Mathematics} {Education}},
	publisher = {Springer, Dordrecht},
	author = {Tall, David and Yevdokimov, Oleksiy and Koichu, Boris and Whiteley, Walter and Kondratieva, Margo and Cheng, Ying-Hao},
	year = {2011},
	doi = {10.1007/978-94-007-2129-6_2},
	pages = {13--49},
	file = {Snapshot:/home/jeremiah/Zotero/storage/ZMETADCU/978-94-007-2129-6_2.html:text/html}
}

@incollection{johnson_adaptor_2007,
	title = {Adaptor {Grammars}: {A} {Framework} for {Specifying} {Compositional} {Nonparametric} {Bayesian} {Models}},
	shorttitle = {Adaptor {Grammars}},
	url = {http://papers.nips.cc/paper/3101-adaptor-grammars-a-framework-for-specifying-compositional-nonparametric-bayesian-models.pdf},
	urldate = {2018-02-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Johnson, Mark and Griffiths, Thomas L. and Goldwater, Sharon},
	editor = {Schölkopf, B. and Platt, J. C. and Hoffman, T.},
	year = {2007},
	pages = {641--648},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/KS4YBWH2/Johnson et al. - 2007 - Adaptor Grammars A Framework for Specifying Compo.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/QTKL8TZY/3101-adaptor-grammars-a-framework-for-specifying-compositional-nonparametric-bayesian-models.html:text/html}
}

@incollection{hanna_proofs_2010,
	title = {Proofs as {Bearers} of {Mathematical} {Knowledge}},
	isbn = {978-1-4419-0575-8 978-1-4419-0576-5},
	url = {https://link.springer.com/chapter/10.1007/978-1-4419-0576-5_7},
	abstract = {Yehuda Rav’s inspiring paper “Why do we prove theorems?” published in Philosophia Mathematica (1999; 7: 5–41) has interesting implications for mathematics education. We examine Rav’s central ideas on proof – that proofs convey important elements of mathematics such as strategies and methods, that it is “proofs rather than theorems that are the bearers of mathematical knowledge” and thus that proofs should be the primary focus of mathematical interest – and then discuss their significance for mathematics education in general and for the teaching of proof in particular.},
	language = {en},
	booktitle = {Explanation and {Proof} in {Mathematics}},
	publisher = {Springer, Boston, MA},
	author = {Hanna, Gila and Barbeau, Ed},
	year = {2010},
	doi = {10.1007/978-1-4419-0576-5_7},
	pages = {85--100}
}

@article{hilbert_learning_2008,
	title = {Learning to prove in geometry: {Learning} from heuristic examples and how it can be supported},
	volume = {18},
	issn = {0959-4752},
	shorttitle = {Learning to prove in geometry},
	url = {http://www.sciencedirect.com/science/article/pii/S095947520600096X},
	doi = {10.1016/j.learninstruc.2006.10.008},
	abstract = {This field experiment tested whether a special type of worked-out examples (i.e., heuristic examples) helps learners develop better conceptual knowledge about mathematical proving and proving skills than a control condition focussing on mathematical contents. Additionally, we analysed the benefits of self-explanation prompts and completion requirements in a 2×2-design. The participants' (N=111 student teachers) proving skills and their conceptual knowledge were significantly better when learning with heuristic examples as compared to the control condition. Completion requirements impaired learning especially in combination with self-explanation prompts. The sole provision of self-explanation prompts, in contrast, fostered conceptual knowledge as well as skills.},
	number = {1},
	urldate = {2018-02-12},
	journal = {Learning and Instruction},
	author = {Hilbert, Tatjana S. and Renkl, Alexander and Kessler, Stephan and Reiss, Kristina},
	month = feb,
	year = {2008},
	keywords = {Heuristic examples, Learning to prove, Self-explanations, Worked-out examples},
	pages = {54--65},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/LEUQ4M3E/Hilbert et al. - 2008 - Learning to prove in geometry Learning from heuri.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/9IGV6XVV/S095947520600096X.html:text/html}
}

@article{sriram_cold_2017,
	title = {Cold {Fusion}: {Training} {Seq}2Seq {Models} {Together} with {Language} {Models}},
	shorttitle = {Cold {Fusion}},
	url = {http://arxiv.org/abs/1708.06426},
	abstract = {Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10\% of the labeled training data.},
	urldate = {2018-02-06},
	journal = {arXiv:1708.06426 [cs]},
	author = {Sriram, Anuroop and Jun, Heewoo and Satheesh, Sanjeev and Coates, Adam},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.06426},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1708.06426 PDF:/home/jeremiah/Zotero/storage/DZ5XESN6/Sriram et al. - 2017 - Cold Fusion Training Seq2Seq Models Together with.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/8LDS7V2D/1708.html:text/html}
}

@misc{noauthor_4_nodate,
	title = {(4) {Learning} to {Solve} {Geometry} {Problems} from {Natural} {Language} {Demonstrations} in {Textbooks}},
	url = {https://www.researchgate.net/publication/318738982_Learning_to_Solve_Geometry_Problems_from_Natural_Language_Demonstrations_in_Textbooks},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	journal = {ResearchGate},
	file = {Snapshot:/home/jeremiah/Zotero/storage/R6S4U2DQ/318738982_Learning_to_Solve_Geometry_Problems_from_Natural_Language_Demonstrations_in_Textbooks.pdf:application/pdf}
}

@incollection{jitkrittum_linear-time_2017,
	title = {A {Linear}-{Time} {Kernel} {Goodness}-of-{Fit} {Test}},
	url = {http://papers.nips.cc/paper/6630-a-linear-time-kernel-goodness-of-fit-test.pdf},
	urldate = {2018-02-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Jitkrittum, Wittawat and Xu, Wenkai and Szabo, Zoltan and Fukumizu, Kenji and Gretton, Arthur},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {261--270},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/4T8NNQYB/Jitkrittum et al. - 2017 - A Linear-Time Kernel Goodness-of-Fit Test.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/KVBNATPA/6630-a-linear-time-kernel-goodness-of-fit-test.html:text/html}
}

@article{ye_visually_2010,
	title = {Visually {Dynamic} {Presentation} of {Proofs} in {Plane} {Geometry}},
	volume = {45},
	issn = {0168-7433, 1573-0670},
	url = {https://link.springer.com/article/10.1007/s10817-009-9162-5},
	doi = {10.1007/s10817-009-9162-5},
	abstract = {With dynamic mediums such as computer displays, we propose a new kind of visually dynamic presentation of proofs in plane geometry. In a single diagram for the proof, when the proof text goes on step by step with mouse clicks, the related geometry elements in the diagram are added, animated, or deleted dynamically with various visually dynamic effects. It solves not only the problem of identifying geometry elements in the proof text with those in the diagram, but also makes the proof more vividly visualized and intuitive. Our ongoing developing system “Java Geometry Expert” (JGEX) uses two methods to create such visually dynamic presentations: the manual input method and the automatic method. In this first part of the series of our work, we propose the main features of our visually dynamic presentation of proofs and present the manual input method to create such presentations. The manual input method mainly uses mouse clicks to create the dynamic geometry diagram and the proof text.},
	language = {en},
	number = {3},
	journal = {Journal of Automated Reasoning},
	author = {Ye, Zheng and Chou, Shang-Ching and Gao, Xiao-Shan},
	month = oct,
	year = {2010},
	pages = {213--241},
	file = {Snapshot:/home/jeremiah/Zotero/storage/FL67GTG7/s10817-009-9162-5.html:text/html}
}

@article{chou_deductive_2000,
	title = {A {Deductive} {Database} {Approach} to {Automated} {Geometry} {Theorem} {Proving} and {Discovering}},
	volume = {25},
	issn = {0168-7433, 1573-0670},
	url = {https://link.springer.com/article/10.1023/A:1006171315513},
	doi = {10.1023/A:1006171315513},
	abstract = {We report our effort to build a geometry deductive database, which can be used to find the fixpoint for a geometric configuration. The system can find all the properties of the configuration that can be deduced using a fixed set of geometric rules. To control the size of the database, we propose the idea of a structured deductive database. Our experiments show that this technique could reduce the size of the database by one hundred times. We propose the data-based search strategy to improve the efficiency of forward chaining. We also make clear progress in the problems of how to select good geometric rules, how to add auxiliary points, and how to construct numerical diagrams as models automatically. The program is tested with 160 nontrivial geometry configurations. For these geometric configurations, the program not only finds most of their well-known properties but also often gives unexpected results, some of which are possibly new. Also, the proofs generated by the program are generally short and totally geometric.},
	language = {en},
	number = {3},
	journal = {Journal of Automated Reasoning},
	author = {Chou, Shang-Ching and Gao, Xiao-Shan and Zhang, Jing-Zhong},
	month = oct,
	year = {2000},
	pages = {219--246},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/SUB6NVXU/Chou et al. - 2000 - A Deductive Database Approach to Automated Geometr.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/ZXM69NGC/A1006171315513.html:text/html}
}

@article{coelho_automated_1986,
	title = {Automated reasoning in geometry theorem proving with {Prolog}},
	volume = {2},
	issn = {0168-7433, 1573-0670},
	url = {https://link.springer.com/article/10.1007/BF00248249},
	doi = {10.1007/BF00248249},
	abstract = {This paper describes automated reasoning in a PROLOG Euclidean geometry theorem-prover. It brings into focus general topics in automated reasoning and the ability of Prolog in coping with them.},
	language = {en},
	number = {4},
	journal = {Journal of Automated Reasoning},
	author = {Coelho, Helder and Pereira, Luis Moniz},
	month = dec,
	year = {1986},
	pages = {329--390},
	file = {Snapshot:/home/jeremiah/Zotero/storage/VJZGR88R/BF00248249.html:text/html}
}

@article{bruderlin_using_1993,
	title = {Using geometric rewrite rules for solving geometric problems symbolically},
	volume = {116},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/030439759390324M},
	doi = {10.1016/0304-3975(93)90324-M},
	abstract = {This paper describes the theoretical framework of a geometric problem solver based on rewrite rules which were directly derived from axioms of Euclidean geometry. Completeness and termination of the system were obtained by making the set of rewrite rules locally confluent and by introducing a simple, well-founded ordering. This research presents a first step in finding a geometric alternative to the numerical and computer algebra methods currently used for such problems and, thus, has the potential of supporting geometric applications such as computer-aided geometric design.},
	number = {2},
	urldate = {2018-01-29},
	journal = {Theoretical Computer Science},
	author = {Bru¨derlin, Beat},
	month = aug,
	year = {1993},
	pages = {291--303},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/I3L7VRTH/Bru¨derlin - 1993 - Using geometric rewrite rules for solving geometri.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/CQJT25AK/030439759390324M.html:text/html}
}

@article{coelho_automated_1986-1,
	title = {Automated reasoning in geometry theorem proving with {Prolog}},
	volume = {2},
	issn = {0168-7433, 1573-0670},
	url = {https://link.springer.com/article/10.1007/BF00248249},
	doi = {10.1007/BF00248249},
	abstract = {This paper describes automated reasoning in a PROLOG Euclidean geometry theorem-prover. It brings into focus general topics in automated reasoning and the ability of Prolog in coping with them.},
	language = {en},
	number = {4},
	journal = {Journal of Automated Reasoning},
	author = {Coelho, Helder and Pereira, Luis Moniz},
	month = dec,
	year = {1986},
	pages = {329--390},
	file = {Snapshot:/home/jeremiah/Zotero/storage/INYK98LM/BF00248249.html:text/html}
}

@article{janicic_area_2012,
	title = {The {Area} {Method}},
	volume = {48},
	issn = {0168-7433, 1573-0670},
	url = {https://link.springer.com/article/10.1007/s10817-010-9209-7},
	doi = {10.1007/s10817-010-9209-7},
	abstract = {The area method for Euclidean constructive geometry was proposed by Chou, Gao and Zhang in the early 1990’s. The method can efficiently prove many non-trivial geometry theorems and is one of the most interesting and most successful methods for automated theorem proving in geometry. The method produces proofs that are often very concise and human-readable. In this paper, we provide a first complete presentation of the method. We provide both algorithmic and implementation details that were omitted in the original presentations. We also give a variant of Chou, Gao and Zhang’s axiom system. Based on this axiom system, we proved formally all the lemmas needed by the method and its soundness using the Coq proof assistant. To our knowledge, apart from the original implementation by the authors who first proposed the method, there are only three implementations more. Although the basic idea of the method is simple, implementing it is a very challenging task because of a number of details that has to be dealt with. With the description of the method given in this paper, implementing the method should be still complex, but a straightforward task. In the paper we describe all these implementations and also some of their applications.},
	language = {en},
	number = {4},
	journal = {Journal of Automated Reasoning},
	author = {Janičić, Predrag and Narboux, Julien and Quaresma, Pedro},
	month = apr,
	year = {2012},
	pages = {489--532},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/WY45LDV8/Janičić et al. - 2012 - The Area Method.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/LGSRARG2/s10817-010-9209-7.html:text/html}
}

@incollection{billich_area_2015,
	series = {Trends in {Mathematics}},
	title = {The {Area} {Method} and {Proving} {Plane} {Geometry} {Theorems}},
	isbn = {978-3-319-12576-3 978-3-319-12577-0},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-12577-0_48},
	abstract = {The process of proving, deriving and discovering theorems is important in mathematics investigation. In this paper, we will use the elimination technique which is based on the theory of the area method. The main idea of this method will be illustrated through an example from plane geometry. In addition, we look at the application possibilities of using GCLC geometry system with built-in theorem prover in verification and proving constructive geometric statements.},
	language = {en},
	booktitle = {Current {Trends} in {Analysis} and {Its} {Applications}},
	publisher = {Birkhäuser, Cham},
	author = {Billich, Martin},
	year = {2015},
	doi = {10.1007/978-3-319-12577-0_48},
	pages = {433--439},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/8X3IA5GB/Billich - 2015 - The Area Method and Proving Plane Geometry Theorem.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/JDJMAXGA/978-3-319-12577-0_48.html:text/html}
}

@article{ljungman_modification_2009,
	title = {Modification of the {Interleukin}-6 {Response} to {Air} {Pollution} by {Interleukin}-6 and {Fibrinogen} {Polymorphisms}},
	volume = {117},
	issn = {0091-6765},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2737012/},
	doi = {10.1289/ehp.0800370},
	abstract = {Background
Evidence suggests that cardiovascular effects of air pollution are mediated by inflammation and that air pollution can induce genetic expression of the interleukin-6 gene (IL6).

Objectives
We investigated whether IL6 and fibrinogen gene variants can affect plasma IL-6 responses to air pollution in patients with cardiovascular disease.

Methods
We repeatedly determined plasma IL-6 in 955 myocardial infarction survivors from six European cities (n = 5,539). We conducted city-specific analyses using additive mixed models adjusting for patient characteristics, time trend, and weather to assess the impact of air pollutants on plasma IL-6. We pooled city-specific estimates using meta-analysis methodology. We selected three IL6 single-nucleotide polymorphisms (SNPs) and one SNP each from the fibrinogen α-chain gene (FGA) and β-chain gene (FGB) for gene–environment analyses.

Results
We found the most consistent modifications for variants in IL6 rs2069832 and FBG rs1800790 after exposure to carbon monoxide (CO; 24-hr average; p-values for interaction, 0.034 and 0.019, respectively). Nitrogen dioxide effects were consistently modified, but p-values for interaction were larger (0.09 and 0.19, respectively). The strongest effects were seen 6–11 hr after exposure, when, for example, the overall effect of a 2.2\% increase in IL-6 per 0.64 mg/m3 CO was modified to a 10\% (95\% confidence interval, 4.6–16\%) increase in IL-6 (p-value for interaction = 0.002) for minor homozygotes of FGB rs1800790.

Conclusions
The effect of gaseous traffic-related air pollution on inflammation may be stronger in genetic subpopulations with ischemic heart disease. This information could offer an opportunity to identify postinfarction patients who would benefit more than others from a cleaner environment and antiinflammatory treatment.},
	number = {9},
	urldate = {2018-01-18},
	journal = {Environmental Health Perspectives},
	author = {Ljungman, Petter and Bellander, Tom and Schneider, Alexandra and Breitner, Susanne and Forastiere, Francesco and Hampel, Regina and Illig, Thomas and Jacquemin, Bénédicte and Katsouyanni, Klea and von Klot, Stephanie and Koenig, Wolfgang and Lanki, Timo and Nyberg, Fredrik and Pekkanen, Juha and Pistelli, Riccardo and Pitsavos, Christos and Rosenqvist, Mårten and Sunyer, Jordi and Peters, Annette},
	month = sep,
	year = {2009},
	pmid = {19750100},
	pmcid = {PMC2737012},
	pages = {1373--1379},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/VA4RR4V3/Ljungman et al. - 2009 - Modification of the Interleukin-6 Response to Air .pdf:application/pdf}
}

@article{zanobetti_gene-air_2011,
	title = {Gene-air pollution interaction and cardiovascular disease: a review},
	volume = {53},
	issn = {1873-1740},
	shorttitle = {Gene-air pollution interaction and cardiovascular disease},
	doi = {10.1016/j.pcad.2011.01.001},
	abstract = {Genetic susceptibility is likely to play a role in response to air pollution. Hence, gene-environment interaction studies can be a tool for exploring the mechanisms and the importance of the pathway in the association between air pollution and a cardiovascular outcome. In this article, we present a systematic review of the studies that have examined gene-environment interactions in relation to the cardiovascular health effects of air pollutants. We identified 16 articles meeting our search criteria. Of these studies, most have focused on individual functional polymorphisms or individual candidate genes. Moreover, they were all based on 3 study populations that have been extensively investigated in relation to air pollution effects: the Normative Aging Study, Air Pollution and Inflammatory Response in Myocardial Infarction Survivors: Gene-Environment Interaction in a High Risk Group, and Multiethnic Study of Atherosclerosis. In conclusions, the studies differed substantially in both the cardiovascular outcomes examined and the polymorphisms examined, so there is little confirmation of results across cohorts. Gene-environment interaction studies can help explore the mechanisms and the potential pathway in the association between air pollution and a cardiovascular outcome; replication of findings and studies involving multiple cohorts would be needed to draw stronger conclusions.},
	language = {eng},
	number = {5},
	journal = {Progress in Cardiovascular Diseases},
	author = {Zanobetti, Antonella and Baccarelli, Andrea and Schwartz, Joel},
	month = apr,
	year = {2011},
	pmid = {21414469},
	pmcid = {PMC3073822},
	keywords = {Humans, Genetic Association Studies, Genetic Predisposition to Disease, Phenotype, Cardiovascular Diseases, Evidence-Based Medicine, Inhalation Exposure, Mutation, Particulate Matter, Polymorphism, Genetic, Risk Assessment, Risk Factors},
	pages = {344--352}
}

@article{poggio_general_2004,
	title = {General conditions for predictivity in learning theory},
	volume = {428},
	copyright = {2004 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature02341},
	doi = {10.1038/nature02341},
	abstract = {General conditions for predictivity in learning theory},
	language = {En},
	number = {6981},
	journal = {Nature},
	author = {Poggio, Tomaso and Rifkin, Ryan and Mukherjee, Sayan and Niyogi, Partha},
	month = mar,
	year = {2004},
	pages = {419},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/6UMNKW6I/Poggio et al. - 2004 - General conditions for predictivity in learning th.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/8IJMGSS5/nature02341.html:text/html}
}

@incollection{nong_conditions_2012,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Conditions for {RBF} {Neural} {Networks} to {Universal} {Approximation} and {Numerical} {Experiments}},
	isbn = {978-3-642-31967-9 978-3-642-31968-6},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-31968-6_36},
	abstract = {In this paper, we investigate the universal approximation property of Radial Basis Function (RBF) networks. We show that RBFs are not required to be integrable for the REF networks to be universal approximators. Instead, RBF networks can uniformly approximate any continuous function on a compact set provided that the radial basis activation function is continuous almost everywhere, locally essentially bounded, and not a polynomial. The approximation is also discussed. Some experimental results are reported to illustrate our findings.},
	language = {en},
	booktitle = {Communications and {Information} {Processing}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Nong, Jifu},
	year = {2012},
	doi = {10.1007/978-3-642-31968-6_36},
	pages = {299--308},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/AUQ3T5DN/Nong - 2012 - Conditions for RBF Neural Networks to Universal Ap.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/4P92G39M/978-3-642-31968-6_36.html:text/html}
}

@inproceedings{seo_diagram_2014,
	address = {Québec City, Québec, Canada},
	series = {{AAAI}'14},
	title = {Diagram {Understanding} in {Geometry} {Questions}},
	url = {http://dl.acm.org/citation.cfm?id=2892753.2892944},
	abstract = {Automatically solving geometry questions is a long-standing AI problem. A geometry question typically includes a textual description accompanied by a diagram. The first step in solving geometry questions is diagram understanding, which consists of identifying visual elements in the diagram, their locations, their geometric properties, and aligning them to corresponding textual descriptions. In this paper, we present a method for diagram understanding that identifies visual elements in a diagram while maximizing agreement between textual and visual data. We show that the method's objective function is submodular; thus we are able to introduce an efficient method for diagram understanding that is close to optimal. To empirically evaluate our method, we compile a new dataset of geometry questions (textual descriptions and diagrams) and compare with baselines that utilize standard vision techniques. Our experimental evaluation shows an F1 boost of more than 17\% in identifying visual elements and 25\% in aligning visual elements with their textual descriptions.},
	urldate = {2018-01-16},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Seo, Min Joon and Hajishirzi, Hannaneh and Farhadi, Ali and Etzioni, Oren},
	year = {2014},
	pages = {2831--2838}
}

@article{beltrametti_hough_2013,
	title = {Hough {Transform} of {Special} {Classes} of {Curves}},
	volume = {6},
	url = {http://epubs.siam.org/doi/abs/10.1137/120863794},
	doi = {10.1137/120863794},
	abstract = {The Hough transform is a standard pattern recognition technique introduced between the 1960s and the 1970s for the detection of straight lines, circles, and ellipses. Here we offer a mathematical foundation, based on algebraic-geometry arguments, of an extension of this approach to the automated recognition of rational cubic, quartic, and elliptic curves. The accuracy of this approach is tested against synthetic data and in the case of experimental observations provided by the NASA Solar Dynamics Observatory mission.},
	number = {1},
	urldate = {2018-01-16},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Beltrametti, M. and Massone, A. and Piana, M.},
	month = jan,
	year = {2013},
	pages = {391--412},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/QME5YEGH/Beltrametti et al. - 2013 - Hough Transform of Special Classes of Curves.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/TVX3INQH/120863794.html:text/html}
}

@article{reddy_universal_2017,
	title = {Universal {Semantic} {Parsing}},
	url = {http://arxiv.org/abs/1702.03196},
	abstract = {Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions. Our code and data can be downloaded at https://github.com/sivareddyg/udeplambda.},
	urldate = {2018-01-15},
	journal = {arXiv:1702.03196 [cs]},
	author = {Reddy, Siva and Täckström, Oscar and Petrov, Slav and Steedman, Mark and Lapata, Mirella},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.03196},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2017},
	file = {arXiv\:1702.03196 PDF:/home/jeremiah/Zotero/storage/AQ9BF3K6/Reddy et al. - 2017 - Universal Semantic Parsing.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MWU2GHME/1702.html:text/html}
}

@phdthesis{clark_estimation_2013,
	title = {Estimation and hypothesis testing with additive kernel machines for high-dimensional data},
	url = {https://cdr.lib.unc.edu/record/uuid:0ab1aa23-e8cd-4e73-93d4-0819f67f8a8a},
	abstract = {Advances in high throughput biotechnology have culminated in the development of large scale, population based studies for identifying genomic features (e.g. genes, SNPs, CpGs, etc.) associated with complex diseases and traits. Understanding an individual's genetic disposition for particular traits and diseases can provide information toward the development of individualized risk profiles and treatment regimes and simultaneously provides clues as to the biological mechanisms underlying complex traits. However, the high-dimensionality of the feature space, the limited availability of samples, and our incomplete understanding of how features biologically influence various diseases impose a grand challenge for statisticians. To mitigate some of these challenges, we propose several new methods. First, we develop the additive least square kernel machine (ALSKM) approach for nonparametrically modeling and testing the cumulative effect of a group of features (such as multiple biologically related CpGs) while nonparametrically adjusting for complex, nonlinear covariates. Our proposed methods model both the genomic features and the complex covariates using the kernel machine framework. Second, building on the ALSKM, we develop a novel approach for testing for interactions between two different groups of (biologically related) features. Specifically, we develop a multi-marker test which can test for epistasis, or gene-gene interactions, between two different groups of genomic features. Finally, we again use on the machinery developed under Topics 1 and 2 to develop an approach for testing the association between rare variants and a phenotype in the presence of common variants while accommodating potential interactions between the common and rare variants. By focusing on multi-feature testing, these approaches reduce the dimensionality of the data. Using the kernel machine framework allows for flexible, possibly nonparametric, analysis which is important given our incomplete understanding of how features influence various traits and diseases.},
	urldate = {2018-01-14},
	school = {University of North Carolina at Chapel Hill},
	author = {Clark, Jennifer},
	year = {2013},
	file = {Carolina Digital Repository - Estimation and hypothesis testing with additive kernel machines for high-dimensional data:/home/jeremiah/Zotero/storage/AMMHM4FJ/uuid0ab1aa23-e8cd-4e73-93d4-0819f67f8a8a.html:text/html}
}

@book{windle_statistical_2016,
	title = {Statistical {Approaches} to {Gene} x {Environment} {Interactions} for {Complex} {Phenotypes}},
	isbn = {978-0-262-33551-5},
	abstract = {Findings from the Human Genome Project and from Genome-Wide Association (GWA) studies indicate that many diseases and traits manifest a more complex genomic pattern than previously assumed. These findings, and advances in high-throughput sequencing, suggest that there are many sources of influence -- genetic, epigenetic, and environmental. This volume investigates the role of the interactions of genes and environment (G × E) in diseases and traits (referred to by the contributors as complex phenotypes) including depression, diabetes, obesity, and substance use. The contributors first present different statistical approaches or strategies to address G × E and G × G interactions with high-throughput sequenced data, including two-stage procedures to identify G × E and G × G interactions, marker-set approaches to assessing interactions at the gene level, and the use of a partial-least square (PLS) approach. The contributors then turn to specific complex phenotypes, research designs, or combined methods that may advance the study of G × E interactions, considering such topics as randomized clinical trials in obesity research, longitudinal research designs and statistical models, and the development of polygenic scores to investigate G × E interactions.ContributorsFatima Umber Ahmed, Yin-Hsiu Chen, James Y. Dai, Caroline Y. Doyle, Zihuai He, Li Hsu, Shuo Jiao, Erin Loraine Kinnally, Yi-An Ko, Charles Kooperberg, Seunggeun Lee, Arnab Maity, Jeanne M. McCaffery, Bhramar Mukherjee, Sung Kyun Park, Duncan C. Thomas, Alexandre Todorov, Jung-Ying Tzeng, Tao Wang, Michael Windle, Min Zhang},
	language = {en},
	publisher = {MIT Press},
	author = {Windle, Michael},
	month = jul,
	year = {2016},
	note = {Google-Books-ID: geipDAAAQBAJ},
	keywords = {Computers / Bioinformatics, Science / Life Sciences / Genetics \& Genomics}
}

@article{lin_test_2013,
	title = {Test for interactions between a genetic marker set and environment in generalized linear models},
	volume = {14},
	issn = {1468-4357},
	doi = {10.1093/biostatistics/kxt006},
	abstract = {We consider in this paper testing for interactions between a genetic marker set and an environmental variable. A common practice in studying gene-environment (GE) interactions is to analyze one single-nucleotide polymorphism (SNP) at a time. It is of significant interest to analyze SNPs in a biologically defined set simultaneously, e.g. gene or pathway. In this paper, we first show that if the main effects of multiple SNPs in a set are associated with a disease/trait, the classical single SNP-GE interaction analysis can be biased. We derive the asymptotic bias and study the conditions under which the classical single SNP-GE interaction analysis is unbiased. We further show that, the simple minimum p-value-based SNP-set GE analysis, can be biased and have an inflated Type 1 error rate. To overcome these difficulties, we propose a computationally efficient and powerful gene-environment set association test (GESAT) in generalized linear models. Our method tests for SNP-set by environment interactions using a variance component test, and estimates the main SNP effects under the null hypothesis using ridge regression. We evaluate the performance of GESAT using simulation studies, and apply GESAT to data from the Harvard lung cancer genetic study to investigate GE interactions between the SNPs in the 15q24-25.1 region and smoking on lung cancer risk.},
	language = {eng},
	number = {4},
	journal = {Biostatistics (Oxford, England)},
	author = {Lin, Xinyi and Lee, Seunggeun and Christiani, David C. and Lin, Xihong},
	month = sep,
	year = {2013},
	pmid = {23462021},
	pmcid = {PMC3769996},
	keywords = {Computer Simulation, Humans, Polymorphism, Single Nucleotide, Female, Genetic Predisposition to Disease, Models, Genetic, Gene-Environment Interaction, Asymptotic bias analysis, Gene–environment interactions, Genetic Markers, Genome-wide association studies, Linear Models, Lung Neoplasms, Male, Score statistic, Single-nucleotide polymorphism, Smoking, Variance component test},
	pages = {667--681}
}

@article{tzeng_studying_2011,
	title = {Studying gene and gene-environment effects of uncommon and common variants on continuous traits: a marker-set approach using gene-trait similarity regression},
	volume = {89},
	issn = {1537-6605},
	shorttitle = {Studying gene and gene-environment effects of uncommon and common variants on continuous traits},
	doi = {10.1016/j.ajhg.2011.07.007},
	abstract = {Genomic association analyses of complex traits demand statistical tools that are capable of detecting small effects of common and rare variants and modeling complex interaction effects and yet are computationally feasible. In this work, we introduce a similarity-based regression method for assessing the main genetic and interaction effects of a group of markers on quantitative traits. The method uses genetic similarity to aggregate information from multiple polymorphic sites and integrates adaptive weights that depend on allele frequencies to accomodate common and uncommon variants. Collapsing information at the similarity level instead of the genotype level avoids canceling signals that have the opposite etiological effects and is applicable to any class of genetic variants without the need for dichotomizing the allele types. To assess gene-trait associations, we regress trait similarities for pairs of unrelated individuals on their genetic similarities and assess association by using a score test whose limiting distribution is derived in this work. The proposed regression framework allows for covariates, has the capacity to model both main and interaction effects, can be applied to a mixture of different polymorphism types, and is computationally efficient. These features make it an ideal tool for evaluating associations between phenotype and marker sets defined by linkage disequilibrium (LD) blocks, genes, or pathways in whole-genome analysis.},
	language = {eng},
	number = {2},
	journal = {American Journal of Human Genetics},
	author = {Tzeng, Jung-Ying and Zhang, Daowen and Pongpanich, Monnat and Smith, Chris and McCarthy, Mark I. and Sale, Michèle M. and Worrall, Bradford B. and Hsu, Fang-Chi and Thomas, Duncan C. and Sullivan, Patrick F.},
	month = aug,
	year = {2011},
	pmid = {21835306},
	pmcid = {PMC3155192},
	keywords = {Computer Simulation, Humans, Models, Genetic, Environment, Mutation, Genetic Markers, Chromosomes, Human, Pair 21, Databases, Genetic, Genes, Quantitative Trait, Heritable, Regression Analysis},
	pages = {277--288}
}

@article{ge_kernel_2015,
	title = {A kernel machine method for detecting effects of interaction between multidimensional variable sets: {An} imaging genetics application},
	volume = {109},
	issn = {1053-8119},
	shorttitle = {A kernel machine method for detecting effects of interaction between multidimensional variable sets},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811915000440},
	doi = {10.1016/j.neuroimage.2015.01.029},
	abstract = {Measurements derived from neuroimaging data can serve as markers of disease and/or healthy development, are largely heritable, and have been increasingly utilized as (intermediate) phenotypes in genetic association studies. To date, imaging genetic studies have mostly focused on discovering isolated genetic effects, typically ignoring potential interactions with non-genetic variables such as disease risk factors, environmental exposures, and epigenetic markers. However, identifying significant interaction effects is critical for revealing the true relationship between genetic and phenotypic variables, and shedding light on disease mechanisms. In this paper, we present a general kernel machine based method for detecting effects of the interaction between multidimensional variable sets. This method can model the joint and epistatic effect of a collection of single nucleotide polymorphisms (SNPs), accommodate multiple factors that potentially moderate genetic influences, and test for nonlinear interactions between sets of variables in a flexible framework. As a demonstration of application, we applied the method to the data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) to detect the effects of the interactions between candidate Alzheimer's disease (AD) risk genes and a collection of cardiovascular disease (CVD) risk factors, on hippocampal volume measurements derived from structural brain magnetic resonance imaging (MRI) scans. Our method identified that two genes, CR1 and EPHA1, demonstrate significant interactions with CVD risk factors on hippocampal volume, suggesting that CR1 and EPHA1 may play a role in influencing AD-related neurodegeneration in the presence of CVD risks.},
	urldate = {2018-01-14},
	journal = {NeuroImage},
	author = {Ge, Tian and Nichols, Thomas E. and Ghosh, Debashis and Mormino, Elizabeth C. and Smoller, Jordan W. and Sabuncu, Mert R.},
	month = apr,
	year = {2015},
	keywords = {Alzheimer's disease, Cardiovascular disease, Imaging genetics, Interaction, Kernel machines},
	pages = {505--514},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/T8EFZ3CP/Ge et al. - 2015 - A kernel machine method for detecting effects of i.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/V5FNNU7V/S1053811915000440.html:text/html}
}

@article{larson_regularized_2014,
	title = {Regularized {Rare} {Variant} {Enrichment} {Analysis} for {Case}-{Control} {Exome} {Sequencing} {Data}},
	volume = {38},
	issn = {1098-2272},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/gepi.21783/abstract},
	doi = {10.1002/gepi.21783},
	abstract = {Rare variants have recently garnered an immense amount of attention in genetic association analysis. However, unlike methods traditionally used for single marker analysis in GWAS, rare variant analysis often requires some method of aggregation, since single marker approaches are poorly powered for typical sequencing study sample sizes. Advancements in sequencing technologies have rendered next-generation sequencing platforms a realistic alternative to traditional genotyping arrays. Exome sequencing in particular not only provides base-level resolution of genetic coding regions, but also a natural paradigm for aggregation via genes and exons. Here, we propose the use of penalized regression in combination with variant aggregation measures to identify rare variant enrichment in exome sequencing data. In contrast to marginal gene-level testing, we simultaneously evaluate the effects of rare variants in multiple genes, focusing on gene-based least absolute shrinkage and selection operator (LASSO) and exon-based sparse group LASSO models. By using gene membership as a grouping variable, the sparse group LASSO can be used as a gene-centric analysis of rare variants while also providing a penalized approach toward identifying specific regions of interest. We apply extensive simulations to evaluate the performance of these approaches with respect to specificity and sensitivity, comparing these results to multiple competing marginal testing methods. Finally, we discuss our findings and outline future research.},
	language = {en},
	number = {2},
	urldate = {2018-01-14},
	journal = {Genetic Epidemiology},
	author = {Larson, Nicholas B. and Schaid, Daniel J.},
	month = feb,
	year = {2014},
	keywords = {association analysis, exome sequencing, LASSO, rare variants, regularization},
	pages = {104--113},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/WVETJKG7/Larson and Schaid - 2014 - Regularized Rare Variant Enrichment Analysis for C.pdf:application/pdf}
}

@article{boonstra_tests_2016,
	title = {Tests for {Gene}-{Environment} {Interactions} and {Joint} {Effects} {With} {Exposure} {Misclassification}},
	volume = {183},
	issn = {1476-6256},
	doi = {10.1093/aje/kwv198},
	abstract = {The number of methods for genome-wide testing of gene-environment (G-E) interactions continues to increase, with the aim of discovering new genetic risk factors and obtaining insight into the disease-gene-environment relationship. The relative performance of these methods, assessed on the basis of family-wise type I error rate and power, depends on underlying disease-gene-environment associations, estimates of which may be biased in the presence of exposure misclassification. This simulation study expands on a previously published simulation study of methods for detecting G-E interactions by evaluating the impact of exposure misclassification. We consider 7 single-step and modular screening methods for identifying G-E interaction at a genome-wide level and 7 joint tests for genetic association and G-E interaction, for which the goal is to discover new genetic susceptibility loci by leveraging G-E interaction when present. In terms of statistical power, modular methods that screen on the basis of the marginal disease-gene relationship are more robust to exposure misclassification. Joint tests that include main/marginal effects of a gene display a similar robustness, which confirms results from earlier studies. Our results offer an increased understanding of the strengths and limitations of methods for genome-wide searches for G-E interaction and joint tests in the presence of exposure misclassification.},
	language = {eng},
	number = {3},
	journal = {American Journal of Epidemiology},
	author = {Boonstra, Philip S. and Mukherjee, Bhramar and Gruber, Stephen B. and Ahn, Jaeil and Schmit, Stephanie L. and Chatterjee, Nilanjan},
	month = feb,
	year = {2016},
	pmid = {26755675},
	pmcid = {PMC4724093},
	keywords = {Computer Simulation, Gene-Environment Interaction, case-control, gene discovery, gene-environment independence, genome-wide association, Genomics, modular methods, multiple testing, screening test, weighted hypothesis test},
	pages = {237--247}
}

@article{mukherjee_testing_2012,
	title = {Testing gene-environment interaction in large-scale case-control association studies: possible choices and comparisons},
	volume = {175},
	issn = {1476-6256},
	shorttitle = {Testing gene-environment interaction in large-scale case-control association studies},
	doi = {10.1093/aje/kwr367},
	abstract = {Several methods for screening gene-environment interaction have recently been proposed that address the issue of using gene-environment independence in a data-adaptive way. In this report, the authors present a comparative simulation study of power and type I error properties of 3 classes of procedures: 1) the standard 1-step case-control method; 2) the case-only method that requires an assumption of gene-environment independence for the underlying population; and 3) a variety of hybrid methods, including empirical-Bayes, 2-step, and model averaging, that aim at gaining power by exploiting the assumption of gene-environment independence and yet can protect against false positives when the independence assumption is violated. These studies suggest that, although the case-only method generally has maximum power, it has the potential to create substantial false positives in large-scale studies even when a small fraction of markers are associated with the exposure under study in the underlying population. All the hybrid methods perform well in protecting against such false positives and yet can retain substantial power advantages over standard case-control tests. The authors conclude that, for future genome-wide scans for gene-environment interactions, major power gain is possible by using alternatives to standard case-control analysis. Whether a case-only type scan or one of the hybrid methods should be used depends on the strength and direction of gene-environment interaction and association, the level of tolerance for false positives, and the nature of replication strategies.},
	language = {eng},
	number = {3},
	journal = {American Journal of Epidemiology},
	author = {Mukherjee, Bhramar and Ahn, Jaeil and Gruber, Stephen B. and Chatterjee, Nilanjan},
	month = feb,
	year = {2012},
	pmid = {22199027},
	pmcid = {PMC3286201},
	keywords = {Computer Simulation, Models, Statistical, Gene-Environment Interaction, Bayes Theorem, Case-Control Studies},
	pages = {177--190}
}

@article{cornelis_gene-environment_2012,
	title = {Gene-environment interactions in genome-wide association studies: a comparative study of tests applied to empirical studies of type 2 diabetes},
	volume = {175},
	issn = {1476-6256},
	shorttitle = {Gene-environment interactions in genome-wide association studies},
	doi = {10.1093/aje/kwr368},
	abstract = {The question of which statistical approach is the most effective for investigating gene-environment (G-E) interactions in the context of genome-wide association studies (GWAS) remains unresolved. By using 2 case-control GWAS (the Nurses' Health Study, 1976-2006, and the Health Professionals Follow-up Study, 1986-2006) of type 2 diabetes, the authors compared 5 tests for interactions: standard logistic regression-based case-control; case-only; semiparametric maximum-likelihood estimation of an empirical-Bayes shrinkage estimator; and 2-stage tests. The authors also compared 2 joint tests of genetic main effects and G-E interaction. Elevated body mass index was the exposure of interest and was modeled as a binary trait to avoid an inflated type I error rate that the authors observed when the main effect of continuous body mass index was misspecified. Although both the case-only and the semiparametric maximum-likelihood estimation approaches assume that the tested markers are independent of exposure in the general population, the authors did not observe any evidence of inflated type I error for these tests in their studies with 2,199 cases and 3,044 controls. Both joint tests detected markers with known marginal effects. Loci with the most significant G-E interactions using the standard, empirical-Bayes, and 2-stage tests were strongly correlated with the exposure among controls. Study findings suggest that methods exploiting G-E independence can be efficient and valid options for investigating G-E interactions in GWAS.},
	language = {eng},
	number = {3},
	journal = {American Journal of Epidemiology},
	author = {Cornelis, Marilyn C. and Tchetgen, Eric J. Tchetgen and Liang, Liming and Qi, Lu and Chatterjee, Nilanjan and Hu, Frank B. and Kraft, Peter},
	month = feb,
	year = {2012},
	pmid = {22199026},
	pmcid = {PMC3261439},
	keywords = {Humans, Female, Gene-Environment Interaction, Male, Genomics, Body Mass Index, Diabetes Mellitus, Type 2, Genome-Wide Association Study},
	pages = {191--202}
}

@article{voorman_behavior_2011,
	title = {Behavior of {QQ}-{Plots} and {Genomic} {Control} in {Studies} of {Gene}-{Environment} {Interaction}},
	volume = {6},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0019416},
	doi = {10.1371/journal.pone.0019416},
	abstract = {Genome-wide association studies of gene-environment interaction (GxE GWAS) are becoming popular. As with main effects GWAS, quantile-quantile plots (QQ-plots) and Genomic Control are being used to assess and correct for population substructure. However, in GE work these approaches can be seriously misleading, as we illustrate; QQ-plots may give strong indications of substructure when absolutely none is present. Using simulation and theory, we show how and why spurious QQ-plot inflation occurs in GE GWAS, and how this differs from main-effects analyses. We also explain how simple adjustments to standard regression-based methods used in GE GWAS can alleviate this problem.},
	language = {en},
	number = {5},
	journal = {PLOS ONE},
	author = {Voorman, Arend and Lumley, Thomas and McKnight, Barbara and Rice, Kenneth},
	month = may,
	year = {2011},
	keywords = {Genome-wide association studies, Genetic drift, Linear regression analysis, Population genetics, Regression analysis, Simulation and modeling, Test statistics, Variant genotypes},
	pages = {e19416},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/VCZYDRXH/Voorman et al. - 2011 - Behavior of QQ-Plots and Genomic Control in Studie.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/4PWAFBYK/article.html:text/html}
}

@article{tchetgen_tchetgen_robustness_2011,
	title = {On the robustness of tests of genetic associations incorporating gene-environment interaction when the environmental exposure is misspecified},
	volume = {22},
	issn = {1531-5487},
	doi = {10.1097/EDE.0b013e31820877c5},
	abstract = {We consider the robustness of tests of genetic associations that incorporate gene-environment interactions when the environmental exposure is misspecified, which is likely the case when the exposure is continuous. We formally prove that, under the null hypothesis of no genetic association, misspecified ordinary logistic regression and profile likelihood (Chatterjee and Carroll, Biometrika. 2005;92:399-418) analyses of case-control data both consistently estimate the null parameters of no genetic main effect and interaction, provided that genetic and environmental factors are unrelated in the underlying population. However, we argue that the associated likelihood ratio test, score test, and Wald test statistics obtained using the estimated information matrix have incorrect type-1 error rates due to model mis-specification. Based on these observations, we propose the use of the sandwich estimator of variance in conjunction with the consistent maximum (profile) likelihood estimates to construct Wald-type test statistics with correct type-1 error rate for the null of no genetic association.},
	language = {eng},
	number = {2},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Tchetgen Tchetgen, Eric J. and Kraft, Peter},
	month = mar,
	year = {2011},
	pmid = {21228699},
	keywords = {Humans, Genetic Predisposition to Disease, Algorithms, Environmental Exposure, Likelihood Functions, Logistic Models, Sensitivity and Specificity},
	pages = {257--261}
}

@article{thomas_gene--environment-wide_2010,
	title = {Gene--environment-wide association studies: emerging approaches},
	volume = {11},
	issn = {1471-0064},
	shorttitle = {Gene--environment-wide association studies},
	doi = {10.1038/nrg2764},
	abstract = {Despite the yield of recent genome-wide association (GWA) studies, the identified variants explain only a small proportion of the heritability of most complex diseases. This unexplained heritability could be partly due to gene--environment (G×E) interactions or more complex pathways involving multiple genes and exposures. This Review provides a tutorial on the available epidemiological designs and statistical analysis approaches for studying specific G×E interactions and choosing the most appropriate methods. I discuss the approaches that are being developed for studying entire pathways and available techniques for mining interactions in GWA data. I also explore methods for marrying hypothesis-driven pathway-based approaches with 'agnostic' GWA studies.},
	language = {eng},
	number = {4},
	journal = {Nature Reviews. Genetics},
	author = {Thomas, Duncan},
	month = apr,
	year = {2010},
	pmid = {20212493},
	pmcid = {PMC2891422},
	keywords = {Humans, Genetic Predisposition to Disease, Models, Genetic, Environment, Risk Factors, Genome-Wide Association Study, Data Mining, Epigenesis, Genetic},
	pages = {259--272}
}

@article{jia_data_2016,
	title = {Data {Recombination} for {Neural} {Semantic} {Parsing}},
	url = {http://arxiv.org/abs/1606.03622},
	abstract = {Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.},
	urldate = {2018-01-13},
	journal = {arXiv:1606.03622 [cs]},
	author = {Jia, Robin and Liang, Percy},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03622},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2016},
	file = {arXiv\:1606.03622 PDF:/home/jeremiah/Zotero/storage/RQ4E4T7X/Jia and Liang - 2016 - Data Recombination for Neural Semantic Parsing.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WJQ692QI/1606.html:text/html}
}

@book{matsuzaki_semantic_2017,
	title = {Semantic {Parsing} of {Pre}-university {Math} {Problems}},
	author = {Matsuzaki, Takuya and Ito, Takumi and Iwane, Hidenao and Anai, Hirokazu and H. Arai, Noriko},
	month = jan,
	year = {2017},
	doi = {10.18653/v1/P17-1195}
}

@book{liang_bringing_2015,
	title = {Bringing {Machine} {Learning} and {Compositional} {Semantics} {Together}},
	volume = {1},
	abstract = {Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases. This review presents a simple discriminative learning framework for defining such models and relating them to logical theories. Within this framework, we discuss the task of learning to map utterances to logical forms (semantic parsing) and the task of learning from denotations with logical forms as latent variables. We also consider models that use distributed (e.g., vector) representations rather than logical ones, showing that these can be considered part of the same overall framework for understanding meaning and structural complexity.},
	author = {Liang, Percy and Potts, Christopher},
	month = jan,
	year = {2015},
	doi = {10.1146/annurev-linguist-030514-125312}
}

@misc{noauthor_semantic_nodate,
	title = {Semantic {Parsing} with {CCGs}},
	url = {http://yoavartzi.com/tutorial/},
	urldate = {2018-01-07},
	file = {Semantic Parsing with CCGs:/home/jeremiah/Zotero/storage/NU44TNH6/tutorial.html:text/html}
}

@book{duan_parsing_2015,
	title = {Parsing {Chinese} with a {Generalized} {Categorial} {Grammar}},
	abstract = {Categorial grammars are attractive because they have a clear account of unbounded dependencies. This accounting is especially important in Mandarin Chinese which makes extensive usage of unbounded dependencies. However, parsers trained on existing categorial grammar annotations (Tse and Curran, 2010) extracted from the Penn Chinese Treebank (Xue et al., 2005) are not as accurate as those trained on the original treebank, possibly because enforcing a small set of inference rules in these grammars leads to large sets of categories, which cause sparse data problems. This work reannotates the Penn Chinese Treebank into a generalized categorial grammar which uses a larger rule set and a substantially smaller category set while retaining the capacity to model unbounded dependencies. Experimental results show a statistically significant improvement in parsing accuracy with this categorial grammar.},
	author = {Duan, Manjuan and Schuler, William},
	month = jan,
	year = {2015},
	doi = {10.18653/v1/W15-3304}
}

@article{roy_mapping_2017,
	title = {Mapping to {Declarative} {Knowledge} for {Word} {Problem} {Solving}},
	url = {http://arxiv.org/abs/1712.09391},
	abstract = {Math word problems form a natural abstraction to a range of quantitative reasoning problems, such as understanding financial news, sports results, and casualties of war. Solving such problems requires the understanding of several mathematical concepts such as dimensional analysis, subset relationships, etc. In this paper, we develop declarative rules which govern the translation of natural language description of these concepts to math expressions. We then present a framework for incorporating such declarative knowledge into word problem solving. Our method learns to map arithmetic word problem text to math expressions, by learning to select the relevant declarative knowledge for each operation of the solution expression. This provides a way to handle multiple concepts in the same problem while, at the same time, support interpretability of the answer expression. Our method models the mapping to declarative knowledge as a latent variable, thus removing the need for expensive annotations. Experimental evaluation suggests that our domain knowledge based solver outperforms all other systems, and that it generalizes better in the realistic case where the training data it is exposed to is biased in a different way than the test data.},
	urldate = {2018-01-06},
	journal = {arXiv:1712.09391 [cs]},
	author = {Roy, Subhro and Roth, Dan},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.09391},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at TACL 2018},
	file = {arXiv\:1712.09391 PDF:/home/jeremiah/Zotero/storage/YSHDQMGX/Roy and Roth - 2017 - Mapping to Declarative Knowledge for Word Problem .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MQKPQEN6/1712.html:text/html}
}

@inproceedings{wang_building_2015,
	title = {Building a {Semantic} {Parser} {Overnight}},
	abstract = {How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.},
	booktitle = {{ACL}},
	author = {Wang, Yushi and Berant, Jonathan and Liang, Percy},
	year = {2015},
	keywords = {Grammar, Logical Form, MRL, Semantic Parser, Semantic Parsing},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/3HEP63LZ/Wang et al. - 2015 - Building a Semantic Parser Overnight.pdf:application/pdf}
}

@article{alvin_synthesis_2016,
	title = {Synthesis of {Geometry} {Proof} {Problems}},
	url = {https://www.microsoft.com/en-us/research/publication/synthesis-geometry-proof-problems/},
	abstract = {This paper presents a semi-automated methodology for generating geometric proof problems of the kind found in a high-school curriculum. We formalize the notion of a geometry proof problem and describe an algorithm for generating such problems over a user-provided figure. Our experimental results indicate that our problem generation algorithm can effectively generate proof problems in …},
	journal = {Microsoft Research},
	author = {Alvin, Chris and Gulwani, Sumit and Majumdar, Rupak and Mukhopadhyay, Supratik},
	month = dec,
	year = {2016},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/NGKY4V5D/Alvin et al. - 2016 - Synthesis of Geometry Proof Problems.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/M8RG27LX/synthesis-geometry-proof-problems.html:text/html}
}

@article{kembhavi_diagram_2016,
	title = {A {Diagram} {Is} {Worth} {A} {Dozen} {Images}},
	url = {http://arxiv.org/abs/1603.07396},
	abstract = {Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation and reasoning, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.},
	urldate = {2017-12-24},
	journal = {arXiv:1603.07396 [cs]},
	author = {Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.07396},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv\:1603.07396 PDF:/home/jeremiah/Zotero/storage/RN8AIVZ3/Kembhavi et al. - 2016 - A Diagram Is Worth A Dozen Images.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/AF9F6XSW/1603.html:text/html}
}

@article{koncel-kedziorski_parsing_2015,
	title = {Parsing {Algebraic} {Word} {Problems} into {Equations}},
	volume = {3},
	copyright = {Copyright (c) 2015 Association for Computational Linguistics},
	issn = {2307-387X},
	url = {https://transacl.org/ojs/index.php/tacl/article/view/692},
	abstract = {This paper formalizes the problem of solving multi-sentence algebraic word problems as that of generating and scoring equation trees. We use integer linear programming to generate equation trees and score their likelihood by learning local and global discriminative models. These models are trained on a small set of word problems and their answers, without any manual annotation, in order to choose the equation that best matches the problem text. We refer to the overall system as ALGES. We compare ALGES with previous work and show that it covers the full gamut of arithmetic operations whereas Hosseini et al. (2014) only handle addition and subtraction. In addition, ALGES overcomes the brittleness of the Kushman et al. (2014) approach on single-equation problems, yielding a 15\% to 50\% reduction in error.},
	language = {en},
	number = {0},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Koncel-Kedziorski, Rik and Hajishirzi, Hannaneh and Sabharwal, Ashish and Etzioni, Oren and Ang, Siena Dumas},
	month = dec,
	year = {2015},
	pages = {585--597},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/IRB6RJCN/Koncel-Kedziorski et al. - 2015 - Parsing Algebraic Word Problems into Equations.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/VYWS6E5V/692.html:text/html}
}

@misc{noauthor_abstract_nodate,
	title = {Abstract {Meaning} {Representation} ({AMR}) {Annotation} {Release} 2.0 - {Linguistic} {Data} {Consortium}},
	url = {https://catalog.ldc.upenn.edu/LDC2017T10},
	urldate = {2017-12-21},
	file = {Abstract Meaning Representation (AMR) Annotation Release 2.0 - Linguistic Data Consortium:/home/jeremiah/Zotero/storage/7F3DLK96/LDC2017T10.html:text/html}
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to {CAMR}!},
	url = {http://www.cs.brandeis.edu/~clp/camr/camr.html},
	urldate = {2017-12-21},
	file = {Welcome to CAMR!:/home/jeremiah/Zotero/storage/AAU83CYJ/camr.html:text/html}
}

@book{banarescu_abstract_2013,
	title = {Abstract {Meaning} {Representation} for {Sembanking}},
	abstract = {We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.},
	author = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
	year = {2013},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/N3RR6R6U/Banarescu et al. - 2013 - Abstract Meaning Representation for Sembanking.pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/8H64B5ZW/summary.html:text/html}
}

@article{flanigan_discriminative_2014,
	title = {A {Discriminative} {Graph}-{Based} {Parser} for the {Abstract} {Meaning} {Representation}},
	url = {http://repository.cmu.edu/lti/153},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	author = {Flanigan, Jeffrey and Thomson, Sam and Carbonell, Jaime and Dyer, Chris and Smith, Noah},
	month = jun,
	year = {2014},
	pages = {1426--1436},
	file = {"A Discriminative Graph-Based Parser for the Abstract Meaning Represent" by Jeffrey Flanigan, Sam Thomson et al.:/home/jeremiah/Zotero/storage/K5MAZTZX/153.html:text/html}
}

@misc{noauthor_cross-lingual_nodate,
	title = {Cross-lingual {Abstract} {Meaning} {Representation} {Parsing} - {Semantic} {Scholar}},
	url = {/paper/Cross-lingual-Abstract-Meaning-Representation-Pars-Damonte-Cohen/0d13180af2bbe4f58dcd5130fa67aca728a0f184},
	abstract = {Abstract Meaning Representation (AMR) annotation efforts have mostly focused on English. In order to train parsers on other languages, we propose a method based on annotation projection, which involves exploiting annotations in a source language and a parallel corpus of the source language and a target language. Using English as the source language, we show promising results for Italian, Spanish, German and Chinese as target languages. Besides evaluating the target parsers on nongold datasets, we further propose an evaluation method that exploits the English gold annotations and does not require access to gold annotations for the target languages. This is achieved by inverting the projection process: a new English parser is learned from the target language parser and evaluated on the existing English gold standard.Meaning Representation (AMR) annotation efforts have mostly focused on English. In order to train parsers on other languages, we propose a method based on annotation projection, which involves exploiting annotations in a source language and a parallel corpus of the source language and a target language. Using English as the source language, we show promising results for Italian, Spanish, German and Chinese as target languages. Besides evaluating the target parsers on nongold datasets, we further propose an evaluation method that exploits the English gold annotations and does not require access to gold annotations for the target languages. This is achieved by inverting the projection process: a new English parser is learned from the target language parser and evaluated on the existing English gold standard.},
	file = {Snapshot:/home/jeremiah/Zotero/storage/2IVZXRTV/0d13180af2bbe4f58dcd5130fa67aca728a0f184.html:text/html}
}

@article{damonte_cross-lingual_2017,
	title = {Cross-lingual {Abstract} {Meaning} {Representation} {Parsing}},
	url = {http://arxiv.org/abs/1704.04539},
	abstract = {Abstract Meaning Representation (AMR) annotation efforts have mostly focused on English. In order to train parsers on other languages, we propose a method based on annotation projection, which involves exploiting annotations in a source language and a parallel corpus of the source language and a target language. Using English as the source language, we show promising results for Italian, Spanish, German and Chinese as target languages. Besides evaluating the target parsers on non-gold datasets, we further propose an evaluation method that exploits the English gold annotations and does not require access to gold annotations for the target languages. This is achieved by inverting the projection process: a new English parser is learned from the target language parser and evaluated on the existing English gold standard.},
	urldate = {2017-12-21},
	journal = {arXiv:1704.04539 [cs]},
	author = {Damonte, Marco and Cohen, Shay B.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04539},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1704.04539 PDF:/home/jeremiah/Zotero/storage/FUZK2T8T/Damonte and Cohen - 2017 - Cross-lingual Abstract Meaning Representation Pars.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/ZNN7RI3W/1704.html:text/html}
}

@article{ramdas_high-dimensional_2014,
	title = {On the {High}-dimensional {Power} of {Linear}-time {Kernel} {Two}-{Sample} {Testing} under {Mean}-difference {Alternatives}},
	url = {http://arxiv.org/abs/1411.6314},
	abstract = {Nonparametric two sample testing deals with the question of consistently deciding if two distributions are different, given samples from both, without making any parametric assumptions about the form of the distributions. The current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ ({\textbackslash}textit\{general\} alternatives), and those which are designed to specifically test easier alternatives, like a difference in means ({\textbackslash}textit\{mean-shift\} alternatives). The main contribution of this paper is to explicitly characterize the power of a popular nonparametric two sample test, designed for general alternatives, under a mean-shift alternative in the high-dimensional setting. Specifically, we explicitly derive the power of the linear-time Maximum Mean Discrepancy statistic using the Gaussian kernel, where the dimension and sample size can both tend to infinity at any rate, and the two distributions differ in their means. As a corollary, we find that if the signal-to-noise ratio is held constant, then the test's power goes to one if the number of samples increases faster than the dimension increases. This is the first explicit power derivation for a general nonparametric test in the high-dimensional setting, and also the first analysis of how tests designed for general alternatives perform when faced with easier ones.},
	urldate = {2017-12-20},
	journal = {arXiv:1411.6314 [cs, math, stat]},
	author = {Ramdas, Aaditya and Reddi, Sashank J. and Poczos, Barnabas and Singh, Aarti and Wasserman, Larry},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.6314},
	keywords = {Computer Science - Information Theory, Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence, Mathematics - Statistics Theory},
	annote = {Comment: 25 pages, 5 figures},
	file = {arXiv\:1411.6314 PDF:/home/jeremiah/Zotero/storage/EAK9UDQC/Ramdas et al. - 2014 - On the High-dimensional Power of Linear-time Kerne.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/ISVSCTEK/1411.html:text/html}
}

@book{fasshauer_positive_nodate,
	title = {Positive {Definite} {Kernels}: {Past}, {Present} and {Future}},
	shorttitle = {Positive {Definite} {Kernels}},
	abstract = {Positive definite kernels play an increasingly prominent role in many applications such as scattered data fitting, numerical solution of PDEs, computer experiments, machine learning, rapid prototyping and computer graphics. We discuss some of the historical and current developments of the theory and applications of positive definite kernels – always with an eye toward the mathematics of Göttingen in general and Robert Schaback in particular. A few comments concerning the future of the field are also provided.},
	author = {Fasshauer, Gregory E.},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/T74EB96G/Fasshauer - Positive Definite Kernels Past, Present and Futur.pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/B9BANPMD/summary.html:text/html}
}

@article{alemi_deepmath_2016,
	title = {{DeepMath} - {Deep} {Sequence} {Models} for {Premise} {Selection}},
	url = {http://arxiv.org/abs/1606.04442},
	abstract = {We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving on a large scale.},
	urldate = {2017-12-14},
	journal = {arXiv:1606.04442 [cs]},
	author = {Alemi, Alex A. and Chollet, Francois and Een, Niklas and Irving, Geoffrey and Szegedy, Christian and Urban, Josef},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04442},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
	file = {arXiv\:1606.04442 PDF:/home/jeremiah/Zotero/storage/ISYZZKFE/Alemi et al. - 2016 - DeepMath - Deep Sequence Models for Premise Select.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/R8CBEN8T/1606.html:text/html}
}

@article{loos_deep_2017,
	title = {Deep {Network} {Guided} {Proof} {Search}},
	url = {http://arxiv.org/abs/1701.06972},
	abstract = {Deep learning techniques lie at the heart of several significant AI advances in recent years including object recognition and detection, image captioning, machine translation, speech recognition and synthesis, and playing the game of Go. Automated first-order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program analysis, theory reasoning, security, interpolation, and system verification. Here we suggest deep learning based guidance in the proof search of the theorem prover E. We train and compare several deep neural network models on the traces of existing ATP proofs of Mizar statements and use them to select processed clauses during proof search. We give experimental evidence that with a hybrid, two-phase approach, deep learning based guidance can significantly reduce the average number of proof search steps while increasing the number of theorems proved. Using a few proof guidance strategies that leverage deep neural networks, we have found first-order proofs of 7.36\% of the first-order logic translations of the Mizar Mathematical Library theorems that did not previously have ATP generated proofs. This increases the ratio of statements in the corpus with ATP generated proofs from 56\% to 59\%.},
	urldate = {2017-12-14},
	journal = {arXiv:1701.06972 [cs]},
	author = {Loos, Sarah and Irving, Geoffrey and Szegedy, Christian and Kaliszyk, Cezary},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.06972},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
	file = {arXiv\:1701.06972 PDF:/home/jeremiah/Zotero/storage/PQN78REP/Loos et al. - 2017 - Deep Network Guided Proof Search.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/ZSYZXW2K/1701.html:text/html}
}

@article{loos_deep_2017-1,
	title = {Deep {Network} {Guided} {Proof} {Search}},
	url = {http://arxiv.org/abs/1701.06972},
	abstract = {Deep learning techniques lie at the heart of several significant AI advances in recent years including object recognition and detection, image captioning, machine translation, speech recognition and synthesis, and playing the game of Go. Automated first-order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program analysis, theory reasoning, security, interpolation, and system verification. Here we suggest deep learning based guidance in the proof search of the theorem prover E. We train and compare several deep neural network models on the traces of existing ATP proofs of Mizar statements and use them to select processed clauses during proof search. We give experimental evidence that with a hybrid, two-phase approach, deep learning based guidance can significantly reduce the average number of proof search steps while increasing the number of theorems proved. Using a few proof guidance strategies that leverage deep neural networks, we have found first-order proofs of 7.36\% of the first-order logic translations of the Mizar Mathematical Library theorems that did not previously have ATP generated proofs. This increases the ratio of statements in the corpus with ATP generated proofs from 56\% to 59\%.},
	urldate = {2017-12-14},
	journal = {arXiv:1701.06972 [cs]},
	author = {Loos, Sarah and Irving, Geoffrey and Szegedy, Christian and Kaliszyk, Cezary},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.06972},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
	file = {arXiv\:1701.06972 PDF:/home/jeremiah/Zotero/storage/YVGLXFRJ/Loos et al. - 2017 - Deep Network Guided Proof Search.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/3E452VYF/1701.html:text/html}
}

@article{loos_deep_2017-2,
	title = {Deep {Network} {Guided} {Proof} {Search}},
	url = {http://arxiv.org/abs/1701.06972},
	abstract = {Deep learning techniques lie at the heart of several significant AI advances in recent years including object recognition and detection, image captioning, machine translation, speech recognition and synthesis, and playing the game of Go. Automated first-order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program analysis, theory reasoning, security, interpolation, and system verification. Here we suggest deep learning based guidance in the proof search of the theorem prover E. We train and compare several deep neural network models on the traces of existing ATP proofs of Mizar statements and use them to select processed clauses during proof search. We give experimental evidence that with a hybrid, two-phase approach, deep learning based guidance can significantly reduce the average number of proof search steps while increasing the number of theorems proved. Using a few proof guidance strategies that leverage deep neural networks, we have found first-order proofs of 7.36\% of the first-order logic translations of the Mizar Mathematical Library theorems that did not previously have ATP generated proofs. This increases the ratio of statements in the corpus with ATP generated proofs from 56\% to 59\%.},
	urldate = {2017-12-14},
	journal = {arXiv:1701.06972 [cs]},
	author = {Loos, Sarah and Irving, Geoffrey and Szegedy, Christian and Kaliszyk, Cezary},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.06972},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
	file = {arXiv\:1701.06972 PDF:/home/jeremiah/Zotero/storage/TCWQFFHR/Loos et al. - 2017 - Deep Network Guided Proof Search.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/ZC6QCWJK/1701.html:text/html}
}

@article{rocktaschel_end--end_2017,
	title = {End-to-{End} {Differentiable} {Proving}},
	url = {http://arxiv.org/abs/1705.11040},
	abstract = {We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.},
	urldate = {2017-12-14},
	journal = {arXiv:1705.11040 [cs]},
	author = {Rocktäschel, Tim and Riedel, Sebastian},
	month = may,
	year = {2017},
	note = {arXiv: 1705.11040},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: NIPS 2017 camera-ready, NIPS 2017},
	file = {arXiv\:1705.11040 PDF:/home/jeremiah/Zotero/storage/LR5FHNXM/Rocktäschel and Riedel - 2017 - End-to-End Differentiable Proving.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MK2W387K/1705.html:text/html}
}

@article{xu_semantic_2017,
	title = {A {Semantic} {Loss} {Function} for {Deep} {Learning} with {Symbolic} {Knowledge}},
	url = {http://arxiv.org/abs/1711.11157},
	abstract = {This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that our semantic loss function effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.},
	urldate = {2017-12-13},
	journal = {arXiv:1711.11157 [cs, stat]},
	author = {Xu, Jingyi and Zhang, Zilu and Friedman, Tal and Liang, Yitao and Broeck, Guy Van den},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11157},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
	file = {arXiv\:1711.11157 PDF:/home/jeremiah/Zotero/storage/826I33BF/Xu et al. - 2017 - A Semantic Loss Function for Deep Learning with Sy.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/W3K5YS7W/1711.html:text/html}
}

@article{huskova_consistency_1993,
	title = {Consistency of the {Generalized} {Bootstrap} for {Degenerate} \${U}\$-{Statistics}},
	volume = {21},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176349399},
	doi = {10.1214/aos/1176349399},
	abstract = {A generalized bootstrap version is defined for degenerate UUU-statistics. Our main result shows that the (conditional) distribution function of the bootstrapped degenerate UUU-statistic provides a consistent estimator for the unknown distribution function of the degenerate UUU-statistic under consideration. For the proof we rely on a rank statistic approach.},
	language = {EN},
	number = {4},
	journal = {The Annals of Statistics},
	author = {Huskova, Marie and Janssen, Paul},
	month = dec,
	year = {1993},
	mrnumber = {MR1245770},
	zmnumber = {0797.62036},
	keywords = {Bootstrap consistency, degenerate \$U\$-statistics, weighted bootstrapping},
	pages = {1811--1823},
	file = {Snapshot:/home/jeremiah/Zotero/storage/6MRLJPAL/1176349399.html:text/html}
}

@incollection{bernstein_manifold_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Manifold {Learning} in {Regression} {Tasks}},
	copyright = {©2015 Springer International Publishing Switzerland},
	isbn = {978-3-319-17090-9 978-3-319-17091-6},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-17091-6_36},
	abstract = {The paper presents a new geometrically motivated method for non-linear regression based on Manifold learning technique. The regression problem is to construct a predictive function which estimates an unknown smooth mapping f from q-dimensional inputs to m-dimensional outputs based on a training data set consisting of given ‘input-output’ pairs. The unknown mapping f determines q-dimensional manifold M(f) consisting of all the ‘input-output’ vectors which is embedded in (q+m)-dimensional space and covered by a single chart; the training data set determines a sample from this manifold. Modern Manifold Learning methods allow constructing the certain estimator M* from the manifold-valued sample which accurately approximates the manifold. The proposed method called Manifold Learning Regression (MLR) finds the predictive function fMLR to ensure an equality M(fMLR) = M*. The MLR simultaneously estimates the m×q Jacobian matrix of the mapping f.},
	language = {en},
	number = {9047},
	urldate = {2016-10-17},
	booktitle = {Statistical {Learning} and {Data} {Sciences}},
	publisher = {Springer International Publishing},
	author = {Bernstein, Alexander and Kuleshov, Alexander and Yanovich, Yury},
	editor = {Gammerman, Alexander and Vovk, Vladimir and Papadopoulos, Harris},
	month = apr,
	year = {2015},
	doi = {10.1007/978-3-319-17091-6_36},
	keywords = {Algorithm Analysis and Problem Complexity, Artificial Intelligence (incl. Robotics), Computation by Abstract Devices, Database Management, Dimensionality reduction, Information Storage and Retrieval, Information Systems Applications (incl. Internet), Manifold learning, Manifold learning regression, Nonlinear regression, Tangent bundle manifold learning},
	pages = {414--423},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/QI7IAAAH/Bernstein et al. - 2015 - Manifold Learning in Regression Tasks.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/9X3GDCAM/Bernstein et al. - 2015 - Manifold Learning in Regression Tasks.html:text/html}
}

@article{brosch_manifold_2013,
	title = {Manifold learning of brain {MRIs} by deep learning},
	volume = {16},
	abstract = {Manifold learning of medical images plays a potentially important role for modeling anatomical variability within a population with pplications that include segmentation, registration, and prediction of clinical parameters. This paper describes a novel method for learning the manifold of 3D brain images that, unlike most existing manifold learning methods, does not require the manifold space to be locally linear, and does not require a predefined similarity measure or a prebuilt proximity graph. Our manifold learning method is based on deep learning, a machine learning approach that uses layered networks (called deep belief networks, or DBNs) and has received much attention recently in the computer vision field due to their success in object recognition tasks. DBNs have traditionally been too computationally expensive for application to 3D images due to the large number of trainable parameters. Our primary contributions are (1) a much more computationally efficient training method for DBNs that makes training on 3D medical images with a resolution of up to 128 x 128 x 128 practical, and (2) the demonstration that DBNs can learn a low-dimensional manifold of brain volumes that detects modes of variations that correlate to demographic and disease parameters.},
	language = {ENG},
	number = {Pt 2},
	journal = {Medical image computing and computer-assisted intervention: MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
	author = {Brosch, Tom and Tam, Roger and {Initiative for the Alzheimers Disease Neuroimaging}},
	year = {2013},
	pmid = {24579194},
	keywords = {Humans, Algorithms, Sensitivity and Specificity, Information Storage and Retrieval, Artificial Intelligence, Brain, Brain Diseases, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Magnetic Resonance Imaging, Pattern Recognition, Automated, Reproducibility of Results},
	pages = {633--640}
}

@incollection{bengio_non-local_2005,
	title = {Non-{Local} {Manifold} {Tangent} {Learning}},
	url = {http://papers.nips.cc/paper/2647-non-local-manifold-tangent-learning.pdf},
	urldate = {2016-10-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 17},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Monperrus, Martin},
	editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
	year = {2005},
	pages = {129--136},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/PF4DPKJJ/Bengio and Monperrus - 2005 - Non-Local Manifold Tangent Learning.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/DR68DZ95/2647-non-local-manifold-tangent-learning.html:text/html}
}

@incollection{steinke_non-parametric_2009,
	title = {Non-parametric {Regression} {Between} {Manifolds}},
	url = {http://papers.nips.cc/paper/3561-non-parametric-regression-between-manifolds.pdf},
	urldate = {2016-10-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 21},
	publisher = {Curran Associates, Inc.},
	author = {Steinke, Florian and Hein, Matthias},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
	pages = {1561--1568},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/7CCWFAQS/Steinke and Hein - 2009 - Non-parametric Regression Between Manifolds.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/CS7D7HTB/3561-non-parametric-regression-between-manifolds.html:text/html}
}

@article{jayasumana_kernel_2015,
	title = {Kernel {Methods} on {Riemannian} {Manifolds} with {Gaussian} {RBF} {Kernels}},
	volume = {37},
	issn = {0162-8828, 2160-9292},
	url = {http://arxiv.org/abs/1412.0265},
	doi = {10.1109/TPAMI.2015.2414422},
	abstract = {In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this paper, we define Gaussian radial basis function (RBF)-based positive definite kernels on manifolds that permit us to embed a given manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with any given metric is not always positive definite, we present a unified framework for analyzing the positive definiteness of the Gaussian RBF on a generic metric space. We then use the proposed framework to identify positive definite kernels on two specific manifolds commonly encountered in computer vision: the Riemannian manifold of symmetric positive definite matrices and the Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the help of such positive definite Gaussian kernels.},
	number = {12},
	urldate = {2016-10-15},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jayasumana, Sadeep and Hartley, Richard and Salzmann, Mathieu and Li, Hongdong and Harandi, Mehrtash},
	month = dec,
	year = {2015},
	note = {arXiv: 1412.0265},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {2464--2477},
	file = {arXiv\:1412.0265 PDF:/home/jeremiah/Zotero/storage/U6XRBNKE/Jayasumana et al. - 2015 - Kernel Methods on Riemannian Manifolds with Gaussi.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/BAS2MI3Z/1412.html:text/html}
}

@article{lustig_sparse_2007,
	title = {Sparse {MRI}: {The} application of compressed sensing for rapid {MR} imaging},
	volume = {58},
	issn = {0740-3194},
	shorttitle = {Sparse {MRI}},
	doi = {10.1002/mrm.21391},
	abstract = {The sparsity which is implicit in MR images is exploited to significantly undersample k-space. Some MR images such as angiograms are already sparse in the pixel representation; other, more complicated images have a sparse representation in some transform domain-for example, in terms of spatial finite-differences or their wavelet coefficients. According to the recently developed mathematical theory of compressed-sensing, images with a sparse representation can be recovered from randomly undersampled k-space data, provided an appropriate nonlinear recovery scheme is used. Intuitively, artifacts due to random undersampling add as noise-like interference. In the sparse transform domain the significant coefficients stand out above the interference. A nonlinear thresholding scheme can recover the sparse coefficients, effectively recovering the image itself. In this article, practical incoherent undersampling schemes are developed and analyzed by means of their aliasing interference. Incoherence is introduced by pseudo-random variable-density undersampling of phase-encodes. The reconstruction is performed by minimizing the l(1) norm of a transformed image, subject to data fidelity constraints. Examples demonstrate improved spatial resolution and accelerated acquisition for multislice fast spin-echo brain imaging and 3D contrast enhanced angiography.},
	language = {eng},
	number = {6},
	journal = {Magnetic Resonance in Medicine},
	author = {Lustig, Michael and Donoho, David and Pauly, John M.},
	month = dec,
	year = {2007},
	pmid = {17969013},
	keywords = {Humans, Algorithms, Sensitivity and Specificity, Artificial Intelligence, Brain, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Magnetic Resonance Imaging, Pattern Recognition, Automated, Reproducibility of Results, Data Compression, Phantoms, Imaging},
	pages = {1182--1195}
}

@article{lustig_sparse_2007-1,
	title = {Sparse {MRI}: {The} application of compressed sensing for rapid {MR} imaging},
	volume = {58},
	issn = {0740-3194},
	shorttitle = {Sparse {MRI}},
	doi = {10.1002/mrm.21391},
	abstract = {The sparsity which is implicit in MR images is exploited to significantly undersample k-space. Some MR images such as angiograms are already sparse in the pixel representation; other, more complicated images have a sparse representation in some transform domain-for example, in terms of spatial finite-differences or their wavelet coefficients. According to the recently developed mathematical theory of compressed-sensing, images with a sparse representation can be recovered from randomly undersampled k-space data, provided an appropriate nonlinear recovery scheme is used. Intuitively, artifacts due to random undersampling add as noise-like interference. In the sparse transform domain the significant coefficients stand out above the interference. A nonlinear thresholding scheme can recover the sparse coefficients, effectively recovering the image itself. In this article, practical incoherent undersampling schemes are developed and analyzed by means of their aliasing interference. Incoherence is introduced by pseudo-random variable-density undersampling of phase-encodes. The reconstruction is performed by minimizing the l(1) norm of a transformed image, subject to data fidelity constraints. Examples demonstrate improved spatial resolution and accelerated acquisition for multislice fast spin-echo brain imaging and 3D contrast enhanced angiography.},
	language = {eng},
	number = {6},
	journal = {Magnetic Resonance in Medicine},
	author = {Lustig, Michael and Donoho, David and Pauly, John M.},
	month = dec,
	year = {2007},
	pmid = {17969013},
	keywords = {Humans, Algorithms, Sensitivity and Specificity, Artificial Intelligence, Brain, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Magnetic Resonance Imaging, Pattern Recognition, Automated, Reproducibility of Results, Data Compression, Phantoms, Imaging},
	pages = {1182--1195}
}

@article{vincent_stacked_2010,
	title = {Stacked {Denoising} {Autoencoders}: {Learning} {Useful} {Representations} in a {Deep} {Network} with a {Local} {Denoising} {Criterion}},
	volume = {11},
	number = {Dec},
	journal = {Journal of Machine Learning Research},
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	year = {2010},
	pages = {3371--3408}
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	number = {Nov},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605}
}

@article{usman_compressive_2014,
	title = {Compressive manifold learning: {Estimating} one-dimensional respiratory motion directly from undersampled k-space data},
	volume = {72},
	number = {4},
	journal = {Magnetic Resonance in Medicine},
	author = {Usman, Muhammad and Vaillant, Ghislain and Atkinson, David and Schaeffter, Tobias and Prieto, Claudia},
	month = oct,
	year = {2014},
	pages = {1130--1140}
}

@book{vincent_extracting_2008,
	title = {Extracting and composing robust features with denoising autoencoders},
	publisher = {ACM},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	month = jul,
	year = {2008}
}

@article{bengio_implicit_2012,
	title = {Implicit {Density} {Estimation} by {Local} {Moment} {Matching} to {Sample} from {Auto}-{Encoders}},
	journal = {arXiv.org},
	author = {Bengio, Yoshua and Alain, Guillaume and Rifai, Salah},
	month = jun,
	year = {2012},
	note = {arXiv: 1207.0057}
}

@article{grandvalet_noise_2006,
	title = {Noise {Injection}: {Theoretical} {Prospects}},
	journal = {dx.doi.org.ezp-prod1.hul.harvard.edu},
	author = {Grandvalet, Yves and Canu, Stéphane and Boucheron, Stéphane},
	month = mar,
	year = {2006}
}

@inproceedings{rifai_contractive_2011,
	title = {Contractive auto-encoders: {Explicit} invariance during feature extraction},
	booktitle = {Proceedings of the {\textbackslash}ldots},
	author = {Rifai, S and Vincent, P and Muller, X},
	year = {2011}
}

@article{sietsma_creating_1991,
	title = {Creating artificial neural networks that generalize},
	volume = {4},
	number = {1},
	journal = {Neural Networks},
	author = {Sietsma, Jocelyn and Dow, Robert J F},
	month = jan,
	year = {1991},
	pages = {67--79}
}

@article{bengio_unsupervised_2012,
	title = {Unsupervised feature learning and deep learning: {A} review and new perspectives},
	journal = {CoRR},
	author = {Bengio, Y and Courville, A C},
	year = {2012},
	annote = {Review Article}
}

@article{holmstrom_using_1992,
	title = {Using additive noise in back-propagation training},
	volume = {3},
	number = {1},
	journal = {IEEE Transactions on Neural Networks},
	author = {Holmstrom, L and Koistinen, P},
	year = {1992},
	pages = {24--38}
}

@article{hoffman_no-u-turn_2011,
	title = {The {No}-{U}-{Turn} {Sampler}: {Adaptively} {Setting} {Path} {Lengths} in {Hamiltonian} {Monte} {Carlo}},
	shorttitle = {The {No}-{U}-{Turn} {Sampler}},
	url = {http://arxiv.org/abs/1111.4246},
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{{\textbackslash}epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{{\textbackslash}epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
	urldate = {2016-10-15},
	journal = {arXiv:1111.4246 [cs, stat]},
	author = {Hoffman, Matthew D. and Gelman, Andrew},
	month = nov,
	year = {2011},
	note = {arXiv: 1111.4246},
	keywords = {Computer Science - Learning, Statistics - Computation},
	annote = {Comment: 30 pages, 7 figures},
	file = {arXiv\:1111.4246 PDF:/home/jeremiah/Zotero/storage/VMUUVASN/Hoffman and Gelman - 2011 - The No-U-Turn Sampler Adaptively Setting Path Len.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/2SEEGDZM/1111.html:text/html}
}

@article{kim_analytic_2006,
	title = {Analytic, {Computational}, and {Approximate} {Forms} for {Ratios} of {Noncentral} and {Central} {Gaussian} {Quadratic} {Forms}},
	volume = {15},
	issn = {1061-8600},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3704188/},
	doi = {10.1198/106186006X112954},
	abstract = {Many useful statistics equal the ratio of a possibly noncentral chi-square to a quadratic form in Gaussian variables with all positive weights. Expressing the density and distribution function as positively weighted sums of corresponding F functions has many advantages. The mixture forms have analytic value when embedded within a more complex problem. The mixture forms also have computational value. The expansions work well with quadratic forms having few components and small degrees of freedom. A more general algorithm from earlier literature can take longer or fail to converge in the same setting. Many approximations have been suggested for the problem. a positively weighted noncentral quadratic form can always have two moments matched to a noncentral chi-square. For a single quadratic form, the noncentral form performs neither uniformly more or less accurately than older approximations. The approach also gives a noncentral F approximation for any ratio of a positively weighted noncentral form to a positively weighted central quadratic form. The method provides better accuracy for noncentral ratios than approximations based on a single chi-square. The accuracy suffices for many practical applications, such as power analysis, even with few degrees of freedom. Naturally the approximation proves much faster and simpler to compute than any exact method. Embedding the approximation in analytic expressions provides simple forms which correctly guarantee only positive values have nonzero probabilities, and also automatically reduce to partially or fully exact results when either quadratic form has only one term.},
	number = {2},
	urldate = {2016-10-06},
	journal = {Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America},
	author = {Kim, Hae-Young and Gribbin, Matthew J. and Muller, Keith E. and Taylor, Douglas J.},
	month = jun,
	year = {2006},
	pmid = {23843686},
	pmcid = {PMC3704188},
	pages = {443--459},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/EM4NEFJ9/Kim et al. - 2006 - Analytic, Computational, and Approximate Forms for.pdf:application/pdf}
}

@article{rockova_fast_2015,
	title = {Fast {Bayesian} {Factor} {Analysis} via {Automatic} {Rotations} to {Sparsity}},
	volume = {0},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2015.1100620},
	doi = {10.1080/01621459.2015.1100620},
	abstract = {Rotational post-hoc transformations have traditionally played a key role in enhancing the interpretability of factor analysis. Regularization methods also serve to achieve this goal by prioritizing sparse loading matrices. In this work, we bridge these two paradigms with a unifying Bayesian framework. Our approach deploys intermediate factor rotations throughout the learning process, greatly enhancing the effectiveness of sparsity inducing priors. These automatic rotations to sparsity are embedded within a PXL-EM algorithm, a Bayesian variant of parameter-expanded EM for posterior mode detection. By iterating between soft-thresholding of small factor loadings and transformations of the factor basis, we obtain (a) dramatic accelerations, (b) robustness against poor initializations and (c) better oriented sparse solutions. To avoid the pre-specification of the factor cardinality, we extend the loading matrix to have infinitely many columns with the Indian Buffet Process (IBP) prior. The factor dimensionality is learned from the posterior, which is shown to concentrate on sparse matrices. Our deployment of PXL-EM performs a dynamic posterior exploration, outputting a solution path indexed by a sequence of spike-and-slab priors. For accurate recovery of the factor loadings, we deploy the Spike-and-Slab LASSO prior, a two-component refinement of the Laplace prior (Ročková, 2015). A companion criterion, motivated as an integral lower bound, is provided to effectively select the best recovery. The potential of the proposed procedure is demonstrated on both simulated and real high-dimensional data, which would render posterior simulation impractical.},
	number = {ja},
	urldate = {2016-10-05},
	journal = {Journal of the American Statistical Association},
	author = {Ročková, Veronika and George, Edward I.},
	month = oct,
	year = {2015},
	pages = {00--00},
	file = {Snapshot:/home/jeremiah/Zotero/storage/4I84JZPJ/01621459.2015.html:text/html}
}

@article{brodersen_inferring_2015,
	title = {Inferring causal impact using {Bayesian} structural time-series models},
	url = {http://research.google.com/pubs/pub41854.html},
	urldate = {2016-10-04},
	author = {Brodersen, Kay H. and Gallusser, Fabian and Koehler, Jim and Remy, Nicolas and Scott, Steven L.},
	year = {2015},
	file = {Snapshot:/home/jeremiah/Zotero/storage/5MRH3WG6/pub41854.html:text/html}
}

@incollection{moakher_symmetric_2006,
	series = {Mathematics and {Visualization}},
	title = {Symmetric {Positive}-{Definite} {Matrices}: {From} {Geometry} to {Applications} and {Visualization}},
	copyright = {©2006 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-25032-6 978-3-540-31272-7},
	shorttitle = {Symmetric {Positive}-{Definite} {Matrices}},
	url = {http://link.springer.com/chapter/10.1007/3-540-31272-2_17},
	abstract = {In many engineering applications that use tensor analysis, such as tensor imaging, the underlying tensors have the characteristic of being positive definite. It might therefore be more appropriate to use techniques specially adapted to such tensors. We will describe the geometry and calculus on the Riemannian symmetric space of positive-definite tensors. First, we will explain why the geometry, constructed by Emile Cartan, is a natural geometry on that space. Then, we will use this framework to present formulas for means and interpolations specific to positive-definite tensors.},
	language = {en},
	urldate = {2016-10-03},
	booktitle = {Visualization and {Processing} of {Tensor} {Fields}},
	publisher = {Springer Berlin Heidelberg},
	author = {Moakher, Maher and Batchelor, Philipp G.},
	editor = {Weickert, Professor Joachim and Hagen, Professor Hans},
	year = {2006},
	doi = {10.1007/3-540-31272-2_17},
	keywords = {Analysis, Computer Imaging, Vision, Pattern Recognition and Graphics, Differential Geometry, Image Processing and Computer Vision, Imaging / Radiology, Visualization},
	pages = {285--298},
	file = {Snapshot:/home/jeremiah/Zotero/storage/VWEGUK44/10.html:text/html}
}

@incollection{burges_advances_1999,
	address = {Cambridge, MA, USA},
	title = {Advances in {Kernel} {Methods}},
	isbn = {978-0-262-19416-7},
	url = {http://dl.acm.org/citation.cfm?id=299094.299100},
	urldate = {2016-10-02},
	publisher = {MIT Press},
	author = {Burges, Christopher J. C.},
	editor = {Schölkopf, Bernhard and Burges, Christopher J. C. and Smola, Alexander J.},
	year = {1999},
	pages = {89--116}
}

@article{lim_estimation_2013,
	title = {Estimation {Stability} with {Cross} {Validation} ({ESCV})},
	url = {http://arxiv.org/abs/1303.3128},
	abstract = {Cross-validation (CV) is often used to select the regularization parameter in high dimensional problems. However, when applied to the sparse modeling method Lasso, CV leads to models that are unstable in high-dimensions, and consequently not suited for reliable interpretation. In this paper, we propose a model-free criterion ESCV based on a new estimation stability (ES) metric and CV. Our proposed ESCV finds a locally ES-optimal model smaller than the CV choice so that the it fits the data and also enjoys estimation stability property. We demonstrate that ESCV is an effective alternative to CV at a similar easily parallelizable computational cost. In particular, we compare the two approaches with respect to several performance measures when applied to the Lasso on both simulated and real data sets. For dependent predictors common in practice, our main finding is that, ESCV cuts down false positive rates often by a large margin, while sacrificing little of true positive rates. ESCV usually outperforms CV in terms of parameter estimation while giving similar performance as CV in terms of prediction. For the two real data sets from neuroscience and cell biology, the models found by ESCV are less than half of the model sizes by CV. Judged based on subject knowledge, they are more plausible than those by CV as well. We also discuss some regularization parameter alignment issues that come up in both approaches.},
	urldate = {2016-09-29},
	journal = {arXiv:1303.3128 [stat]},
	author = {Lim, Chinghway and Yu, Bin},
	month = mar,
	year = {2013},
	note = {arXiv: 1303.3128},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv\:1303.3128 PDF:/home/jeremiah/Zotero/storage/E869CJ4W/Lim and Yu - 2013 - Estimation Stability with Cross Validation (ESCV).pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/5HBWB5ST/1303.html:text/html}
}

@article{vasile_meta-prod2vec_2016,
	title = {Meta-{Prod}2Vec - {Product} {Embeddings} {Using} {Side}-{Information} for {Recommendation}},
	url = {http://arxiv.org/abs/1607.07326},
	doi = {10.1145/2959100.2959160},
	abstract = {We propose Meta-Prod2vec, a novel method to compute item similarities for recommendation that leverages existing item metadata. Such scenarios are frequently encountered in applications such as content recommendation, ad targeting and web search. Our method leverages past user interactions with items and their attributes to compute low-dimensional embeddings of items. Specifically, the item metadata is in- jected into the model as side information to regularize the item embeddings. We show that the new item representa- tions lead to better performance on recommendation tasks on an open music dataset.},
	urldate = {2016-09-29},
	journal = {arXiv:1607.07326 [cs]},
	author = {Vasile, Flavian and Smirnova, Elena and Conneau, Alexis},
	year = {2016},
	note = {arXiv: 1607.07326},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	pages = {225--232},
	file = {arXiv\:1607.07326 PDF:/home/jeremiah/Zotero/storage/NZ4A3AUA/Vasile et al. - 2016 - Meta-Prod2Vec - Product Embeddings Using Side-Info.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/X73QNTEN/1607.html:text/html}
}

@article{marinari_simulated_1992,
	title = {Simulated {Tempering}: {A} {New} {Monte} {Carlo} {Scheme}},
	volume = {19},
	issn = {0295-5075, 1286-4854},
	shorttitle = {Simulated {Tempering}},
	url = {http://arxiv.org/abs/hep-lat/9205018},
	doi = {10.1209/0295-5075/19/6/002},
	abstract = {We propose a new global optimization method (\{{\textbackslash}em Simulated Tempering\}) for simulating effectively a system with a rough free energy landscape (i.e. many coexisting states) at finite non-zero temperature. This method is related to simulated annealing, but here the temperature becomes a dynamic variable, and the system is always kept at equilibrium. We analyze the method on the Random Field Ising Model, and we find a dramatic improvement over conventional Metropolis and cluster methods. We analyze and discuss the conditions under which the method has optimal performances.},
	number = {6},
	urldate = {2016-09-29},
	journal = {Europhysics Letters (EPL)},
	author = {Marinari, Enzo and Parisi, Giorgio},
	month = jul,
	year = {1992},
	note = {arXiv: hep-lat/9205018},
	keywords = {Condensed Matter, High Energy Physics - Lattice, High Energy Physics - Phenomenology, High Energy Physics - Theory},
	pages = {451--458},
	annote = {Comment: 12 pages, very simple LaTeX file, figures are not included, sorry},
	file = {arXiv\:hep-lat/9205018 PDF:/home/jeremiah/Zotero/storage/2CATB3IF/Marinari and Parisi - 1992 - Simulated Tempering A New Monte Carlo Scheme.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WCH2SJUI/9205018.html:text/html}
}

@article{calderhead_general_2014,
	title = {A general construction for parallelizing {Metropolis}−{Hastings} algorithms},
	volume = {111},
	issn = {0027-8424},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4267367/},
	doi = {10.1073/pnas.1408184111},
	abstract = {Many computational problems in modern-day statistics are heavily dependent on Markov chain Monte Carlo (MCMC) methods. These algorithms allow us to evaluate arbitrary probability distributions; however, they are inherently sequential in nature due to the Markov property, which severely limits their computational speed. We propose a general approach that allows scalable parallelization of existing MCMC methods. We do so by defining a finite-state Markov chain on multiple proposals in a way that ensures asymptotic convergence to the correct stationary distribution. In example simulations, we demonstrate up to two orders of magnitude improvement in overall computational performance., Markov chain Monte Carlo methods (MCMC) are essential tools for solving many modern-day statistical and computational problems; however, a major limitation is the inherently sequential nature of these algorithms. In this paper, we propose a natural generalization of the Metropolis−Hastings algorithm that allows for parallelizing a single chain using existing MCMC methods. We do so by proposing multiple points in parallel, then constructing and sampling from a finite-state Markov chain on the proposed points such that the overall procedure has the correct target density as its stationary distribution. Our approach is generally applicable and straightforward to implement. We demonstrate how this construction may be used to greatly increase the computational speed and statistical efficiency of a variety of existing MCMC methods, including Metropolis-Adjusted Langevin Algorithms and Adaptive MCMC. Furthermore, we show how it allows for a principled way of using every integration step within Hamiltonian Monte Carlo methods; our approach increases robustness to the choice of algorithmic parameters and results in increased accuracy of Monte Carlo estimates with little extra computational cost.},
	number = {49},
	urldate = {2016-09-29},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Calderhead, Ben},
	month = dec,
	year = {2014},
	pmid = {25422442},
	pmcid = {PMC4267367},
	pages = {17408--17413},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/BCPKJ5XE/Calderhead - 2014 - A general construction for parallelizing Metropoli.pdf:application/pdf}
}

@article{yang_computational_2015,
	title = {On the {Computational} {Complexity} of {High}-{Dimensional} {Bayesian} {Variable} {Selection}},
	url = {http://arxiv.org/abs/1505.07925},
	abstract = {We study the computational complexity of Markov chain Monte Carlo (MCMC) methods for high-dimensional Bayesian linear regression under sparsity constraints. We first show that a Bayesian approach can achieve variable-selection consistency under relatively mild conditions on the design matrix. We then demonstrate that the statistical criterion of posterior concentration need not imply the computational desideratum of rapid mixing of the MCMC algorithm. By introducing a truncated sparsity prior for variable selection, we provide a set of conditions that guarantee both variable-selection consistency and rapid mixing of a particular Metropolis-Hastings algorithm. The mixing time is linear in the number of covariates up to a logarithmic factor. Our proof controls the spectral gap of the Markov chain by constructing a canonical path ensemble that is inspired by the steps taken by greedy algorithms for variable selection.},
	urldate = {2016-09-29},
	journal = {arXiv:1505.07925 [cs, math, stat]},
	author = {Yang, Yun and Wainwright, Martin J. and Jordan, Michael I.},
	month = may,
	year = {2015},
	note = {arXiv: 1505.07925},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, Statistics - Methodology, Mathematics - Statistics Theory},
	annote = {Comment: 42 pages, 3 figures},
	file = {arXiv\:1505.07925 PDF:/home/jeremiah/Zotero/storage/HDFEF2J7/Yang et al. - 2015 - On the Computational Complexity of High-Dimensiona.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/7NT5PWSD/1505.html:text/html}
}

@article{gorham_measuring_2015,
	title = {Measuring {Sample} {Quality} with {Stein}'s {Method}},
	url = {http://arxiv.org/abs/1506.03039},
	abstract = {To improve the efficiency of Monte Carlo estimation, practitioners are turning to biased Markov chain Monte Carlo procedures that trade off asymptotic exactness for computational speed. The reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced. However, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias. To address these challenges, we introduce a new computable quality measure based on Stein's method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions. We use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyperparameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference.},
	urldate = {2016-09-29},
	journal = {arXiv:1506.03039 [cs, math, stat]},
	author = {Gorham, Jackson and Mackey, Lester},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03039},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology, Mathematics - Probability},
	annote = {Comment: 17 pages, 6 figures},
	file = {arXiv\:1506.03039 PDF:/home/jeremiah/Zotero/storage/DEF63JRW/Gorham and Mackey - 2015 - Measuring Sample Quality with Stein's Method.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/22IHB5FP/1506.html:text/html}
}

@incollection{meeds_optimization_2015,
	title = {Optimization {Monte} {Carlo}: {Efficient} and {Embarrassingly} {Parallel} {Likelihood}-{Free} {Inference}},
	shorttitle = {Optimization {Monte} {Carlo}},
	url = {http://papers.nips.cc/paper/5881-optimization-monte-carlo-efficient-and-embarrassingly-parallel-likelihood-free-inference.pdf},
	urldate = {2016-09-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Meeds, Ted and Welling, Max},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2080--2088},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/G7AXW69J/Meeds and Welling - 2015 - Optimization Monte Carlo Efficient and Embarrassi.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/X4T8AK6C/5881-optimization-monte-carlo-efficient-and-embarrassingly-parallel-likelihood-free-inference.html:text/html}
}

@article{korattikara_austerity_2013,
	title = {Austerity in {MCMC} {Land}: {Cutting} the {Metropolis}-{Hastings} {Budget}},
	shorttitle = {Austerity in {MCMC} {Land}},
	url = {http://arxiv.org/abs/1304.5299},
	abstract = {Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.},
	urldate = {2016-09-17},
	journal = {arXiv:1304.5299 [cs, stat]},
	author = {Korattikara, Anoop and Chen, Yutian and Welling, Max},
	month = apr,
	year = {2013},
	note = {arXiv: 1304.5299},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: v4 - version accepted by ICML2014},
	file = {arXiv\:1304.5299 PDF:/home/jeremiah/Zotero/storage/SBR4P3DV/Korattikara et al. - 2013 - Austerity in MCMC Land Cutting the Metropolis-Has.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/8HUKP4BJ/1304.html:text/html}
}

@article{parikh_proximal_2014,
	title = {Proximal {Algorithms}},
	volume = {1},
	issn = {2167-3888},
	url = {http://dx.doi.org/10.1561/2400000003},
	doi = {10.1561/2400000003},
	abstract = {This monograph is about a class of optimization algorithms called proximal algorithms. Much like Newton's method is a standard tool for solving unconstrained smooth optimization problems of modest size, proximal algorithms can be viewed as an analogous tool for nonsmooth, constrained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical algorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed-form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpretations of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.},
	number = {3},
	urldate = {2016-09-17},
	journal = {Found. Trends Optim.},
	author = {Parikh, Neal and Boyd, Stephen},
	month = jan,
	year = {2014},
	pages = {127--239}
}

@article{rosasco_convergence_2014,
	title = {Convergence of {Stochastic} {Proximal} {Gradient} {Algorithm}},
	url = {http://arxiv.org/abs/1403.5074},
	abstract = {We prove novel convergence results for a stochastic proximal gradient algorithm suitable for solving a large class of convex optimization problems, where a convex objective function is given by the sum of a smooth and a possibly non-smooth component. We consider the iterates convergence and derive \$O(1/n)\$ non asymptotic bounds in expectation in the strongly convex case, as well as almost sure convergence results under weaker assumptions. Our approach allows to avoid averaging and weaken boundedness assumptions which are often considered in theoretical studies and might not be satisfied in practice.},
	urldate = {2016-09-17},
	journal = {arXiv:1403.5074 [math]},
	author = {Rosasco, Lorenzo and Villa, Silvia and Vũ, Bang Công},
	month = mar,
	year = {2014},
	note = {arXiv: 1403.5074},
	keywords = {65K10, 90C25, 90C15, 62G08, Mathematics - Optimization and Control},
	annote = {Comment: 24 pages},
	file = {arXiv\:1403.5074 PDF:/home/jeremiah/Zotero/storage/QQ4K3WHQ/Rosasco et al. - 2014 - Convergence of Stochastic Proximal Gradient Algori.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/F78VDFIK/1403.html:text/html}
}

@article{atchade_perturbed_2014,
	title = {On perturbed proximal gradient algorithms},
	url = {http://arxiv.org/abs/1402.2365},
	abstract = {We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non-asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models.},
	urldate = {2016-09-17},
	journal = {arXiv:1402.2365 [math, stat]},
	author = {Atchade, Yves F. and Fort, Gersende and Moulines, Eric},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.2365},
	keywords = {Mathematics - Statistics Theory, Mathematics - Optimization and Control, 60F15, 60G42},
	annote = {Comment: 29 pages, 5 figures},
	file = {arXiv\:1402.2365 PDF:/home/jeremiah/Zotero/storage/FFNZ4CSP/Atchade et al. - 2014 - On perturbed proximal gradient algorithms.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/4VC8DG8V/1402.html:text/html}
}

@article{vollmer_non-_2015,
	title = {({Non}-) asymptotic properties of {Stochastic} {Gradient} {Langevin} {Dynamics}},
	url = {http://arxiv.org/abs/1501.00438},
	abstract = {Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally infeasible. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem in three ways: it generates proposed moves using only a subset of the data, it skips the Metropolis-Hastings accept-reject step, and it uses sequences of decreasing step sizes. In {\textbackslash}cite\{TehThierryVollmerSGLD2014\}, we provided the mathematical foundations for the decreasing step size SGLD, including consistency and a central limit theorem. However, in practice the SGLD is run for a relatively small number of iterations, and its step size is not decreased to zero. The present article investigates the behaviour of the SGLD with fixed step size. In particular we characterise the asymptotic bias explicitly, along with its dependence on the step size and the variance of the stochastic gradient. On that basis a modified SGLD which removes the asymptotic bias due to the variance of the stochastic gradients up to first order in the step size is derived. Moreover, we are able to obtain bounds on the finite-time bias, variance and mean squared error (MSE). The theory is illustrated with a Gaussian toy model for which the bias and the MSE for the estimation of moments can be obtained explicitly. For this toy model we study the gain of the SGLD over the standard Euler method in the limit of large data sets.},
	urldate = {2016-09-16},
	journal = {arXiv:1501.00438 [math, stat]},
	author = {Vollmer, Sebastian J. and Zygalakis, Konstantinos C. and Teh, {and} Yee Whye},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.00438},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Mathematics - Statistics Theory, 60J05, 65C05},
	annote = {Comment: 42 pages, 7 figures},
	file = {arXiv\:1501.00438 PDF:/home/jeremiah/Zotero/storage/FKDDBDTQ/Vollmer et al. - 2015 - (Non-) asymptotic properties of Stochastic Gradien.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/R6U8XJW8/1501.html:text/html}
}

@article{ma_complete_2015,
	title = {A {Complete} {Recipe} for {Stochastic} {Gradient} {MCMC}},
	url = {http://arxiv.org/abs/1506.04696},
	abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.},
	urldate = {2016-09-16},
	journal = {arXiv:1506.04696 [math, stat]},
	author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04696},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Mathematics - Statistics Theory},
	file = {arXiv\:1506.04696 PDF:/home/jeremiah/Zotero/storage/X53NTX36/Ma et al. - 2015 - A Complete Recipe for Stochastic Gradient MCMC.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WH3A562B/1506.html:text/html}
}

@article{hu_fast_2015,
	title = {Fast {Sampling} for {Bayesian} {Max}-{Margin} {Models}},
	url = {http://arxiv.org/abs/1504.07107},
	abstract = {Bayesian max-margin models have shown great superiority in various machine learning tasks with a likelihood regularization, while the probabilistic Monte Carlo sampling for these models still remains challenging, especially for large-scale settings. In analogy to the data augmentation technique to tackle with non-smoothness of the hinge loss, we present a stochastic subgradient MCMC method which is easy to implement and computationally efficient. We investigate the variants that use adaptive stepsizes and thermostats to improve mixing speeds for Bayesian linear SVM. Furthermore, we design a stochastic subgradient HMC within Gibbs method and a doubly stochastic HMC algorithm for mixture of SVMs, a popular extension of linear classifiers. Experimental results on a wide range of problems demonstrate the effectiveness of our approach.},
	urldate = {2016-09-16},
	journal = {arXiv:1504.07107 [cs, stat]},
	author = {Hu, Wenbo and Zhu, Jun and Xu, Minjie and Zhang, Bo},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.07107},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1504.07107 PDF:/home/jeremiah/Zotero/storage/B745968G/Hu et al. - 2015 - Fast Sampling for Bayesian Max-Margin Models.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/HM3TCZBU/1504.html:text/html}
}

@article{chen_stochastic_2014,
	title = {Stochastic {Gradient} {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1402.4102},
	abstract = {Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.},
	urldate = {2016-09-15},
	journal = {arXiv:1402.4102 [cs, stat]},
	author = {Chen, Tianqi and Fox, Emily B. and Guestrin, Carlos},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.4102},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology},
	annote = {Comment: ICML 2014 version},
	file = {arXiv\:1402.4102 PDF:/home/jeremiah/Zotero/storage/VMB6XB2Q/Chen et al. - 2014 - Stochastic Gradient Hamiltonian Monte Carlo.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/J3TJ86G4/1402.html:text/html}
}

@article{mukherjee_parallel_2013,
	title = {Parallel {Boosting} with {Momentum}},
	url = {http://research.google.com/pubs/pub41341.html},
	urldate = {2016-09-15},
	author = {Mukherjee, Indraneel and Canini, Kevin and Frongillo, Rafael and Singer, Yoram},
	year = {2013},
	file = {Snapshot:/home/jeremiah/Zotero/storage/EIA9ADXC/pub41341.html:text/html}
}

@incollection{nitanda_stochastic_2014,
	title = {Stochastic {Proximal} {Gradient} {Descent} with {Acceleration} {Techniques}},
	url = {http://papers.nips.cc/paper/5610-stochastic-proximal-gradient-descent-with-acceleration-techniques.pdf},
	urldate = {2016-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Nitanda, Atsushi},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {1574--1582},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/7ZRPW29R/Nitanda - 2014 - Stochastic Proximal Gradient Descent with Accelera.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/X7VBTJTA/5610-stochastic-proximal-gradient-descent-with-acceleration-techniques.html:text/html}
}

@article{shalev-shwartz_stochastic_2012,
	title = {Stochastic {Dual} {Coordinate} {Ascent} {Methods} for {Regularized} {Loss} {Minimization}},
	url = {http://arxiv.org/abs/1209.1873},
	abstract = {Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications.},
	urldate = {2016-09-15},
	journal = {arXiv:1209.1873 [cs, math, stat]},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	month = sep,
	year = {2012},
	note = {arXiv: 1209.1873},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Mathematics - Optimization and Control},
	file = {arXiv\:1209.1873 PDF:/home/jeremiah/Zotero/storage/2PVJWXWB/Shalev-Shwartz and Zhang - 2012 - Stochastic Dual Coordinate Ascent Methods for Regu.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/RUG9GKCD/1209.html:text/html}
}

@article{ohzeki_stochastic_2016,
	title = {Stochastic gradient method with accelerated stochastic dynamics},
	volume = {699},
	issn = {1742-6588, 1742-6596},
	url = {http://arxiv.org/abs/1511.06036},
	doi = {10.1088/1742-6596/699/1/012019},
	abstract = {In this paper, we propose a novel technique to implement stochastic gradient methods, which are beneficial for learning from large datasets, through accelerated stochastic dynamics. A stochastic gradient method is based on mini-batch learning for reducing the computational cost when the amount of data is large. The stochasticity of the gradient can be mitigated by the injection of Gaussian noise, which yields the stochastic Langevin gradient method; this method can be used for Bayesian posterior sampling. However, the performance of the stochastic Langevin gradient method depends on the mixing rate of the stochastic dynamics. In this study, we propose violating the detailed balance condition to enhance the mixing rate. Recent studies have revealed that violating the detailed balance condition accelerates the convergence to a stationary state and reduces the correlation time between the samplings. We implement this violation of the detailed balance condition in the stochastic gradient Langevin method and test our method for a simple model to demonstrate its performance.},
	urldate = {2016-09-15},
	journal = {Journal of Physics: Conference Series},
	author = {Ohzeki, Masayuki},
	month = mar,
	year = {2016},
	note = {arXiv: 1511.06036},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
	pages = {012019},
	annote = {Comment: 12 pages, proceedings for International Meeting on High-Dimensional Data Driven Science (HD3-2015) (http://www.sparse-modeling.jp/HD3-2015/index\_e.html)},
	file = {arXiv\:1511.06036 PDF:/home/jeremiah/Zotero/storage/NKVN3H3Z/Ohzeki - 2016 - Stochastic gradient method with accelerated stocha.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/2VK65XSM/1511.html:text/html}
}

@article{andor_globally_2016,
	title = {Globally {Normalized} {Transition}-{Based} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1603.06042},
	abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.},
	urldate = {2016-09-15},
	journal = {arXiv:1603.06042 [cs]},
	author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.06042},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1603.06042 PDF:/home/jeremiah/Zotero/storage/CDCSCP6S/Andor et al. - 2016 - Globally Normalized Transition-Based Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/3XS4BPBN/1603.html:text/html}
}

@misc{bootstrap_grayscale_nodate,
	title = {Grayscale - {Free} {One} {Page} {Bootstrap} {Theme}},
	url = {https://startbootstrap.com/template-overviews/grayscale/},
	abstract = {Grayscale is a free Bootstrap theme with a dark color scheme, smooth scrolling page animations, and a collapsing top navigation bar. It works great for portfolios, businesses, and more!},
	urldate = {2016-09-15},
	journal = {Start Bootstrap},
	author = {Bootstrap, Start},
	file = {Snapshot:/home/jeremiah/Zotero/storage/8AIRVFFQ/grayscale.html:text/html}
}

@article{gomez-uribe_netflix_2015,
	title = {The {Netflix} {Recommender} {System}: {Algorithms}, {Business} {Value}, and {Innovation}},
	volume = {6},
	issn = {2158-656X},
	shorttitle = {The {Netflix} {Recommender} {System}},
	url = {http://doi.acm.org/10.1145/2843948},
	doi = {10.1145/2843948},
	abstract = {This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a recommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention and medium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.},
	number = {4},
	urldate = {2016-09-15},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Gomez-Uribe, Carlos A. and Hunt, Neil},
	month = dec,
	year = {2015},
	keywords = {Recommender, systems},
	pages = {13:1--13:19},
	file = {ACM Full Text PDF:/home/jeremiah/Zotero/storage/27DIMKX6/Gomez-Uribe and Hunt - 2015 - The Netflix Recommender System Algorithms, Busine.pdf:application/pdf}
}

@inproceedings{ahn_distributed_2014,
	title = {Distributed {Stochastic} {Gradient} {MCMC}},
	url = {http://jmlr.org/proceedings/papers/v32/ahn14.html},
	urldate = {2016-09-15},
	author = {Ahn, Sungjin and Shahbaba, Babak and Welling, Max},
	year = {2014},
	pages = {1044--1052},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/K3P2G5QC/Ahn et al. - 2014 - Distributed Stochastic Gradient MCMC.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/H8FITRSU/Ahn et al. - 2014 - Distributed Stochastic Gradient MCMC.html:text/html}
}

@article{korattikara_sequential_2015,
	title = {Sequential {Tests} for {Large}-{Scale} {Learning}},
	volume = {28},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/NECO_a_00796},
	doi = {10.1162/NECO_a_00796},
	abstract = {We argue that when faced with big data sets, learning and inference algorithms should compute updates using only subsets of data items. We introduce algorithms that use sequential hypothesis tests to adaptively select such a subset of data points. The statistical properties of this subsampling process can be used to control the efficiency and accuracy of learning or inference. In the context of learning by optimization, we test for the probability that the update direction is no more than 90 degrees in the wrong direction. In the context of posterior inference using Markov chain Monte Carlo, we test for the probability that our decision to accept or reject a sample is wrong. We experimentally evaluate our algorithms on a number of models and data sets.},
	number = {1},
	urldate = {2016-09-15},
	journal = {Neural Computation},
	author = {Korattikara, Anoop and Chen, Yutian and Welling, Max},
	month = nov,
	year = {2015},
	pages = {45--70},
	file = {Neural Computation Snapshot:/home/jeremiah/Zotero/storage/RICPQ3KF/NECO_a_00796.html:text/html}
}

@article{angelino_patterns_2016,
	title = {Patterns of {Scalable} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/1602.05221},
	abstract = {Datasets are growing not just in size but in complexity, creating a demand for rich models and quantification of uncertainty. Bayesian methods are an excellent fit for this demand, but scaling Bayesian inference is a challenge. In response to this challenge, there has been considerable recent work based on varying assumptions about model structure, underlying computational resources, and the importance of asymptotic correctness. As a result, there is a zoo of ideas with few clear overarching principles. In this paper, we seek to identify unifying principles, patterns, and intuitions for scaling Bayesian inference. We review existing work on utilizing modern computing resources with both MCMC and variational approximation techniques. From this taxonomy of ideas, we characterize the general principles that have proven successful for designing scalable inference procedures and comment on the path forward.},
	urldate = {2016-09-15},
	journal = {arXiv:1602.05221 [stat]},
	author = {Angelino, Elaine and Johnson, Matthew James and Adams, Ryan P.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.05221},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1602.05221 PDF:/home/jeremiah/Zotero/storage/8IDNVDIB/Angelino et al. - 2016 - Patterns of Scalable Bayesian Inference.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WRNJ3JQ6/1602.html:text/html}
}

@article{wang_towards_2016,
	title = {Towards {Bayesian} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Towards {Bayesian} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1604.01662},
	abstract = {While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a general introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this survey, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks.},
	urldate = {2016-09-15},
	journal = {arXiv:1604.01662 [cs, stat]},
	author = {Wang, Hao and Yeung, Dit-Yan},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.01662},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1604.01662 PDF:/home/jeremiah/Zotero/storage/DW5SIAZJ/Wang and Yeung - 2016 - Towards Bayesian Deep Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/5TANJV9R/1604.html:text/html}
}

@article{louizos_structured_2016,
	title = {Structured and {Efficient} {Variational} {Deep} {Learning} with {Matrix} {Gaussian} {Posteriors}},
	url = {http://arxiv.org/abs/1603.04733},
	abstract = {We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian {\textbackslash}cite\{gupta1999matrix\} parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the "local reprarametrization trick" {\textbackslash}cite\{kingma2015variational\} on this posterior distribution we arrive at a Gaussian Process {\textbackslash}cite\{rasmussen2006gaussian\} interpretation of the hidden units in each layer and we, similarly with {\textbackslash}cite\{gal2015dropout\}, provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate "pseudo-data" {\textbackslash}cite\{snelson2005sparse\} in our model, which in turn allows for more efficient sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments.},
	urldate = {2016-09-15},
	journal = {arXiv:1603.04733 [cs, stat]},
	author = {Louizos, Christos and Welling, Max},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04733},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: Updated results with the original folds in the regression experiments. Appearing in the International Conference on Machine Learning (ICML) 2016},
	file = {arXiv\:1603.04733 PDF:/home/jeremiah/Zotero/storage/53H8VIAW/Louizos and Welling - 2016 - Structured and Efficient Variational Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/SEFBQGK6/1603.html:text/html}
}

@article{maaten_visualizing_2008-1,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
	number = {Nov},
	urldate = {2016-09-14},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/XQXEUBHB/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/VFJNTMQK/vandermaaten08a.html:text/html}
}

@article{girolami_riemann_2011,
	title = {Riemann manifold {Langevin} and {Hamiltonian} {Monte} {Carlo} methods},
	volume = {73},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2010.00765.x/abstract},
	doi = {10.1111/j.1467-9868.2010.00765.x},
	abstract = {Summary.  The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis–Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from http://www.ucl.ac.uk/statistics/research/rmhmc allows replication of all the results reported.},
	language = {en},
	number = {2},
	urldate = {2016-09-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Girolami, Mark and Calderhead, Ben},
	month = mar,
	year = {2011},
	keywords = {Bayesian inference, Geometry in statistics, Hamiltonian Monte Carlo methods, Langevin diffusion, Markov chain Monte Carlo methods, Riemann manifolds},
	pages = {123--214},
	file = {Snapshot:/home/jeremiah/Zotero/storage/UGAP635R/abstract.html:text/html}
}

@article{genton_classes_2002,
	title = {Classes of {Kernels} for {Machine} {Learning}: {A} {Statistics} {Perspective}},
	volume = {2},
	issn = {1532-4435},
	shorttitle = {Classes of {Kernels} for {Machine} {Learning}},
	url = {http://dl.acm.org/citation.cfm?id=944790.944815},
	abstract = {In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.},
	urldate = {2016-09-14},
	journal = {J. Mach. Learn. Res.},
	author = {Genton, Marc G.},
	month = mar,
	year = {2002},
	pages = {299--312},
	file = {ACM Full Text PDF:/home/jeremiah/Zotero/storage/9ARWECXJ/Genton - 2002 - Classes of Kernels for Machine Learning A Statist.pdf:application/pdf}
}

@article{mackey_matrix_2014,
	title = {Matrix concentration inequalities via the method of exchangeable pairs},
	volume = {42},
	issn = {0091-1798},
	url = {http://arxiv.org/abs/1201.6002},
	doi = {10.1214/13-AOP892},
	abstract = {This paper derives exponential concentration inequalities and polynomial moment inequalities for the spectral norm of a random matrix. The analysis requires a matrix extension of the scalar concentration theory developed by Sourav Chatterjee using Stein's method of exchangeable pairs. When applied to a sum of independent random matrices, this approach yields matrix generalizations of the classical inequalities due to Hoeffding, Bernstein, Khintchine and Rosenthal. The same technique delivers bounds for sums of dependent random matrices and more general matrix-valued functions of dependent random variables.},
	number = {3},
	urldate = {2016-09-13},
	journal = {The Annals of Probability},
	author = {Mackey, Lester and Jordan, Michael I. and Chen, Richard Y. and Farrell, Brendan and Tropp, Joel A.},
	month = may,
	year = {2014},
	note = {arXiv: 1201.6002},
	keywords = {Mathematics - Probability},
	pages = {906--945},
	annote = {Comment: Published in at http://dx.doi.org/10.1214/13-AOP892 the Annals of Probability (http://www.imstat.org/aop/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv\:1201.6002 PDF:/home/jeremiah/Zotero/storage/QGDNNHWR/Mackey et al. - 2014 - Matrix concentration inequalities via the method o.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/BN3XJMSN/1201.html:text/html}
}

@article{mackey_distributed_2015,
	title = {Distributed {Matrix} {Completion} and {Robust} {Factorization}},
	volume = {16},
	url = {http://jmlr.org/papers/v16/mackey15a.html},
	urldate = {2016-09-13},
	journal = {Journal of Machine Learning Research},
	author = {Mackey, Lester and Talwalkar, Ameet and Jordan, Michael I.},
	year = {2015},
	pages = {913--960},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/9S74RHMF/Mackey et al. - 2015 - Distributed Matrix Completion and Robust Factoriza.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/9W78UA4M/mackey15a.html:text/html}
}

@incollection{maddison_ast_2014,
	title = {A{\textbackslash}ast {Sampling}},
	url = {http://papers.nips.cc/paper/5449-a-sampling.pdf},
	urldate = {2016-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Maddison, Chris J and Tarlow, Daniel and Minka, Tom},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3086--3094},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/K693KHE6/Maddison et al. - 2014 - Aast Sampling.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/WQN4BG2R/5449-a-sampling.html:text/html}
}

@article{neal_mcmc_2012,
	title = {{MCMC} using {Hamiltonian} dynamics},
	url = {http://arxiv.org/abs/1206.1901},
	abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
	urldate = {2016-09-06},
	journal = {arXiv:1206.1901 [physics, stat]},
	author = {Neal, Radford M.},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.1901},
	keywords = {Statistics - Computation, Physics - Computational Physics},
	file = {arXiv\:1206.1901 PDF:/home/jeremiah/Zotero/storage/G96Q65TM/Neal - 2012 - MCMC using Hamiltonian dynamics.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/NM9J8VXI/1206.html:text/html}
}

@article{bagnoli_log-concave_2005,
	title = {Log-{Concave} {Probability} and {Its} {Applications}},
	volume = {26},
	issn = {0938-2259},
	url = {http://www.jstor.org/stable/25055959},
	abstract = {In many applications, assumptions about the log-concavity of a probability distribution allow just enough special structure to yield a workable theory. This paper catalogs a series of theorems relating log-concavity and/or log-convexity of probability density functions, distribution functions, reliability functions, and their integrals. We list a large number of commonly-used probability distributions and report the log-concavity or log-convexity of their density functions and their integrals. We also discuss a variety of applications of log-concavity that have appeared in the literature.},
	number = {2},
	urldate = {2016-09-01},
	journal = {Economic Theory},
	author = {Bagnoli, Mark and Bergstrom, Ted},
	year = {2005},
	pages = {445--469}
}

@article{yang_deep_2014,
	title = {Deep {Fried} {Convnets}},
	url = {http://arxiv.org/abs/1412.7149},
	abstract = {The fully connected layers of a deep convolutional neural network typically contain over 90\% of the network parameters, and consume the majority of the memory required to store the network parameters. Reducing the number of parameters while preserving essentially the same predictive performance is critically important for operating deep neural networks in memory constrained environments such as GPUs or embedded devices. In this paper we show how kernel methods, in particular a single Fastfood layer, can be used to replace all fully connected layers in a deep convolutional neural network. This novel Fastfood layer is also end-to-end trainable in conjunction with convolutional layers, allowing us to combine them into a new architecture, named deep fried convolutional networks, which substantially reduces the memory footprint of convolutional networks trained on MNIST and ImageNet with no drop in predictive performance.},
	urldate = {2016-08-31},
	journal = {arXiv:1412.7149 [cs, stat]},
	author = {Yang, Zichao and Moczulski, Marcin and Denil, Misha and de Freitas, Nando and Smola, Alex and Song, Le and Wang, Ziyu},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.7149},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: svd experiments included}
}

@article{arora_provable_2013,
	title = {Provable {Bounds} for {Learning} {Some} {Deep} {Representations}},
	url = {http://arxiv.org/abs/1310.6343},
	abstract = {We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an \$n\$ node multilayer neural net that has degree at most \$n{\textasciicircum}\{{\textbackslash}gamma\}\$ for some \${\textbackslash}gamma {\textless}1\$ and each edge has a random edge weight in \$[-1,1]\$. Our algorithm learns \{{\textbackslash}em almost all\} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.},
	urldate = {2016-08-31},
	journal = {arXiv:1310.6343 [cs, stat]},
	author = {Arora, Sanjeev and Bhaskara, Aditya and Ge, Rong and Ma, Tengyu},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.6343},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: The first 18 pages serve as an extended abstract and a 36 pages long technical appendix follows},
	file = {arXiv\:1310.6343 PDF:/home/jeremiah/Zotero/storage/EHQQJRA8/Arora et al. - 2013 - Provable Bounds for Learning Some Deep Representat.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/32TQX5S2/1310.html:text/html}
}

@article{wilson_deep_2015,
	title = {Deep {Kernel} {Learning}},
	url = {http://arxiv.org/abs/1511.02222},
	abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost \$O(n)\$ for \$n\$ training points, and predictions cost \$O(1)\$ per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
	urldate = {2016-08-31},
	journal = {arXiv:1511.02222 [cs, stat]},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02222},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Methodology},
	annote = {Comment: 19 pages, 6 figures},
	file = {arXiv\:1511.02222 PDF:/home/jeremiah/Zotero/storage/4WDXZ6VN/Wilson et al. - 2015 - Deep Kernel Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/HDVZRGCS/1511.html:text/html}
}

@incollection{welling_deterministic_2008,
	series = {Proceedings},
	title = {Deterministic {Latent} {Variable} {Models} and their {Pitfalls}},
	isbn = {978-0-89871-654-2},
	url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972788.18},
	abstract = {We derive a number of well known deterministic latent variable models such as PCA, ICA, EPCA, NMF and PLSA as variational EM approximations with point posteriors. We show that the often practiced heuristic of “folding-in” can lead to overly optimistic estimates of the test-set log-likelihood and we verify this result experimentally. We trace this problem back to an infinitely negative entropy term that is ignored in the variational approximation.},
	urldate = {2016-08-29},
	booktitle = {Proceedings of the 2008 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Welling, M. and Chemudugunta, C. and Sutter, N.},
	month = apr,
	year = {2008},
	pages = {196--207},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/KUJ62JFD/Welling et al. - 2008 - Deterministic Latent Variable Models and their Pit.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/EMV4IVIG/1.9781611972788.html:text/html}
}

@article{adams_incorporating_2010,
	title = {Incorporating {Side} {Information} in {Probabilistic} {Matrix} {Factorization} with {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1003.4944},
	abstract = {Probabilistic matrix factorization (PMF) is a powerful method for modeling data associated with pairwise relationships, finding use in collaborative filtering, computational biology, and document analysis, among other areas. In many domains, there is additional information that can assist in prediction. For example, when modeling movie ratings, we might know when the rating occurred, where the user lives, or what actors appear in the movie. It is difficult, however, to incorporate this side information into the PMF model. We propose a framework for incorporating side information by coupling together multiple PMF problems via Gaussian process priors. We replace scalar latent features with functions that vary over the space of side information. The GP priors on these functions require them to vary smoothly and share information. We successfully use this new method to predict the scores of professional basketball games, where side information about the venue and date of the game are relevant for the outcome.},
	urldate = {2016-08-29},
	journal = {arXiv:1003.4944 [cs, stat]},
	author = {Adams, Ryan Prescott and Dahl, George E. and Murray, Iain},
	month = mar,
	year = {2010},
	note = {arXiv: 1003.4944},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: 18 pages, 4 figures, Submitted to UAI 2010},
	file = {arXiv\:1003.4944 PDF:/home/jeremiah/Zotero/storage/M73M969V/Adams et al. - 2010 - Incorporating Side Information in Probabilistic Ma.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/9U5MEKT9/1003.html:text/html}
}

@inproceedings{porteous_bayesian_2010,
	title = {Bayesian {Matrix} {Factorization} with {Side} {Information} and {Dirichlet} {Process} {Mixtures}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1871},
	abstract = {Matrix factorization is a fundamental technique in machine learning that is applicable to collaborative filtering, information retrieval and many other areas. In collaborative filtering and many other tasks, the objective is to fill in missing elements of a sparse data matrix. One of the biggest challenges in this case is filling in a column or row of the matrix with very few observations. In this paper we introduce a Bayesian matrix factorization model that performs regression against side information known about the data in addition to the observations. The side information helps by adding observed entries to the factored matrices.  We also introduce a nonparametric mixture model for the prior of the rows and columns of the factored matrices that gives a different regularization for each latent class. Besides providing a richer prior, the posterior distribution of mixture assignments reveals the latent classes. Using Gibbs sampling for inference, we apply our model to the Netflix Prize problem of predicting movie ratings given an incomplete user-movie ratings matrix.  Incorporating rating information with gathered metadata information, our Bayesian approach outperforms other matrix factorization techniques even when using fewer dimensions.},
	language = {en},
	urldate = {2016-08-29},
	booktitle = {Twenty-{Fourth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Porteous, Ian and Asuncion, Arthur and Welling, Max},
	month = jul,
	year = {2010},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/D7VWR9J9/Porteous et al. - 2010 - Bayesian Matrix Factorization with Side Informatio.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/K8ADZUD9/1871.html:text/html}
}

@article{koltchinskii_nuclear_2010,
	title = {Nuclear norm penalization and optimal rates for noisy low rank matrix completion},
	url = {http://arxiv.org/abs/1011.6256},
	abstract = {This paper deals with the trace regression model where \$n\$ entries or linear combinations of entries of an unknown \$m\_1{\textbackslash}times m\_2\$ matrix \$A\_0\$ corrupted by noise are observed. We propose a new nuclear norm penalized estimator of \$A\_0\$ and establish a general sharp oracle inequality for this estimator for arbitrary values of \$n,m\_1,m\_2\$ under the condition of isometry in expectation. Then this method is applied to the matrix completion problem. In this case, the estimator admits a simple explicit form and we prove that it satisfies oracle inequalities with faster rates of convergence than in the previous works. They are valid, in particular, in the high-dimensional setting \$m\_1m\_2{\textbackslash}gg n\$. We show that the obtained rates are optimal up to logarithmic factors in a minimax sense and also derive, for any fixed matrix \$A\_0\$, a non-minimax lower bound on the rate of convergence of our estimator, which coincides with the upper bound up to a constant factor. Finally, we show that our procedure provides an exact recovery of the rank of \$A\_0\$ with probability close to 1. We also discuss the statistical learning setting where there is no underlying model determined by \$A\_0\$ and the aim is to find the best trace regression model approximating the data.},
	urldate = {2016-08-29},
	journal = {arXiv:1011.6256 [math, stat]},
	author = {Koltchinskii, Vladimir and Tsybakov, Alexandre B. and Lounici, Karim},
	month = nov,
	year = {2010},
	note = {arXiv: 1011.6256},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, 62J99, 62H12, 60B20, 60G15},
	file = {arXiv\:1011.6256 PDF:/home/jeremiah/Zotero/storage/HWUHH2QB/Koltchinskii et al. - 2010 - Nuclear norm penalization and optimal rates for no.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/A5WK9TS4/1011.html:text/html}
}

@inproceedings{salakhutdinov_bayesian_2008,
	address = {New York, NY, USA},
	series = {{ICML} '08},
	title = {Bayesian {Probabilistic} {Matrix} {Factorization} {Using} {Markov} {Chain} {Monte} {Carlo}},
	isbn = {978-1-60558-205-4},
	url = {http://doi.acm.org/10.1145/1390156.1390267},
	doi = {10.1145/1390156.1390267},
	abstract = {Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction accuracy than PMF models trained using MAP estimation.},
	urldate = {2016-08-29},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Salakhutdinov, Ruslan and Mnih, Andriy},
	year = {2008},
	pages = {880--887},
	file = {ACM Full Text PDF:/home/jeremiah/Zotero/storage/WJA8A7Q9/Salakhutdinov and Mnih - 2008 - Bayesian Probabilistic Matrix Factorization Using .pdf:application/pdf}
}

@incollection{mohamed_bayesian_2009,
	title = {Bayesian {Exponential} {Family} {PCA}},
	url = {http://papers.nips.cc/paper/3532-bayesian-exponential-family-pca.pdf},
	urldate = {2016-08-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 21},
	publisher = {Curran Associates, Inc.},
	author = {Mohamed, Shakir and Ghahramani, Zoubin and Heller, Katherine A},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
	pages = {1089--1096},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/KW8DJWIU/Mohamed et al. - 2009 - Bayesian Exponential Family PCA.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/AX42QQFR/3532-bayesian-exponential-family-pca.html:text/html}
}

@inproceedings{haeffele_structured_2014,
	title = {Structured {Low}-{Rank} {Matrix} {Factorization}: {Optimality}, {Algorithm}, and {Applications} to {Image} {Processing}},
	shorttitle = {Structured {Low}-{Rank} {Matrix} {Factorization}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2014c2_haeffele14},
	urldate = {2016-08-26},
	author = {Haeffele, Benjamin and Young, Eric and Vidal, Rene},
	year = {2014},
	pages = {2007--2015},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/BP594EW2/Haeffele et al. - 2014 - Structured Low-Rank Matrix Factorization Optimali.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/GNAZV8VQ/icml2014c2_haeffele14.html:text/html}
}

@article{udell_generalized_2014,
	title = {Generalized {Low} {Rank} {Models}},
	url = {http://arxiv.org/abs/1410.0342},
	abstract = {Principal components analysis (PCA) is a well-known technique for approximating a tabular data set by a low rank matrix. Here, we extend the idea of PCA to handle arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other data types. This framework encompasses many well known techniques in data analysis, such as nonnegative matrix factorization, matrix completion, sparse and robust PCA, \$k\$-means, \$k\$-SVD, and maximum margin matrix factorization. The method handles heterogeneous data sets, and leads to coherent schemes for compressing, denoising, and imputing missing entries across all data types simultaneously. It also admits a number of interesting interpretations of the low rank factors, which allow clustering of examples or of features. We propose several parallel algorithms for fitting generalized low rank models, and describe implementations and numerical results.},
	urldate = {2016-08-26},
	journal = {arXiv:1410.0342 [cs, math, stat]},
	author = {Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.0342},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Mathematics - Optimization and Control},
	annote = {Comment: 84 pages, 19 figures},
	file = {arXiv\:1410.0342 PDF:/home/jeremiah/Zotero/storage/J6W37A4P/Udell et al. - 2014 - Generalized Low Rank Models.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/T3T8FH6A/1410.html:text/html}
}

@article{ahn_large-scale_2015,
	title = {Large-{Scale} {Distributed} {Bayesian} {Matrix} {Factorization} using {Stochastic} {Gradient} {MCMC}},
	url = {http://arxiv.org/abs/1503.01596},
	abstract = {Despite having various attractive qualities such as high prediction accuracy and the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix Factorization has not been widely adopted because of the prohibitive cost of inference. In this paper, we propose a scalable distributed Bayesian matrix factorization algorithm using stochastic gradient MCMC. Our algorithm, based on Distributed Stochastic Gradient Langevin Dynamics, can not only match the prediction accuracy of standard MCMC methods like Gibbs sampling, but at the same time is as fast and simple as stochastic gradient descent. In our experiments, we show that our algorithm can achieve the same level of prediction accuracy as Gibbs sampling an order of magnitude faster. We also show that our method reduces the prediction error as fast as distributed stochastic gradient descent, achieving a 4.1\% improvement in RMSE for the Netflix dataset and an 1.8\% for the Yahoo music dataset.},
	urldate = {2016-08-26},
	journal = {arXiv:1503.01596 [cs, stat]},
	author = {Ahn, Sungjin and Korattikara, Anoop and Liu, Nathan and Rajan, Suju and Welling, Max},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.01596},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1503.01596 PDF:/home/jeremiah/Zotero/storage/MEEGBXEQ/Ahn et al. - 2015 - Large-Scale Distributed Bayesian Matrix Factorizat.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/476MQW45/1503.html:text/html}
}

@article{zhou_beta-negative_2011,
	title = {Beta-{Negative} {Binomial} {Process} and {Poisson} {Factor} {Analysis}},
	url = {http://arxiv.org/abs/1112.3605},
	abstract = {A beta-negative binomial (BNB) process is proposed, leading to a beta-gamma-Poisson process, which may be viewed as a "multi-scoop" generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure, and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Levy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization.},
	urldate = {2016-08-26},
	journal = {arXiv:1112.3605 [stat]},
	author = {Zhou, Mingyuan and Hannah, Lauren and Dunson, David and Carin, Lawrence},
	month = dec,
	year = {2011},
	note = {arXiv: 1112.3605},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: Appearing in AISTATS 2012 (submitted on Oct. 2011)},
	file = {arXiv\:1112.3605 PDF:/home/jeremiah/Zotero/storage/ZH5TSK4F/Zhou et al. - 2011 - Beta-Negative Binomial Process and Poisson Factor .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MNE2B7G3/1112.html:text/html}
}

@inproceedings{kim_scalable_2014,
	title = {\{{Scalable} {Variational} {Bayesian} {Matrix} {Factorization} with {Side} {Information}\}},
	url = {http://jmlr.org/proceedings/papers/v33/kim14b.html},
	urldate = {2016-08-26},
	author = {Kim, Yong-Deok and Choi, Seungjin},
	year = {2014},
	pages = {493--502},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/KI72ZDAK/Kim and Choi - 2014 - Scalable Variational Bayesian Matrix Factorizatio.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/83EIIECC/kim14b.html:text/html}
}

@incollection{gordon_generalized^2_2003,
	title = {Generalized{\textasciicircum}2 {Linear}{\textasciicircum}2 {Models}},
	url = {http://papers.nips.cc/paper/2144-generalized2-linear2-models.pdf},
	urldate = {2016-08-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 15},
	publisher = {MIT Press},
	author = {Gordon, Geoffrey J},
	editor = {Becker, S. and Thrun, S. and Obermayer, K.},
	year = {2003},
	pages = {593--600},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/NBRGIP4X/Gordon - 2003 - Generalized^2 Linear^2 Models.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/334BNZ8I/2144-generalized2-linear2-models.html:text/html}
}

@incollection{singh_unified_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Unified} {View} of {Matrix} {Factorization} {Models}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-87480-5 978-3-540-87481-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-87481-2_24},
	abstract = {We present a unified view of matrix factorization that frames the differences among popular methods, such as NMF, Weighted SVD, E-PCA, MMMF, pLSI, pLSI-pHITS, Bregman co-clustering, and many others, in terms of a small number of modeling choices. Many of these approaches can be viewed as minimizing a generalized Bregman divergence, and we show that (i) a straightforward alternating projection algorithm can be applied to almost any model in our unified view; (ii) the Hessian for each projection has special structure that makes a Newton projection feasible, even when there are equality constraints on the factors, which allows for matrix co-clustering; and (iii) alternating projections can be generalized to simultaneously factor a set of matrices that share dimensions. These observations immediately yield new optimization algorithms for the above factorization methods, and suggest novel generalizations of these methods such as incorporating row and column biases, and adding or relaxing clustering constraints.},
	language = {en},
	number = {5212},
	urldate = {2016-08-26},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Singh, Ajit P. and Gordon, Geoffrey J.},
	editor = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
	month = sep,
	year = {2008},
	doi = {10.1007/978-3-540-87481-2_24},
	keywords = {Algorithm Analysis and Problem Complexity, Artificial Intelligence (incl. Robotics), Database Management, Information Storage and Retrieval, Mathematical Logic and Formal Languages, Probability and Statistics in Computer Science},
	pages = {358--373},
	file = {Snapshot:/home/jeremiah/Zotero/storage/WNIX75G9/10.html:text/html}
}

@inproceedings{collins_generalization_2001,
	title = {A generalization of principal component analysis to the exponential family},
	abstract = {Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data. 1},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Collins, Michael and Dasgupta, Sanjoy and Schapire, Robert E.},
	year = {2001},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/HF7WS6KK/Collins et al. - 2001 - A generalization of principal component analysis t.pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/5KCEDHXX/summary.html:text/html}
}

@article{gopalan_scalable_2013,
	title = {Scalable {Recommendation} with {Poisson} {Factorization}},
	url = {http://arxiv.org/abs/1311.1704},
	abstract = {We develop a Bayesian Poisson matrix factorization model for forming recommendations from sparse user behavior data. These data are large user/item matrices where each user has provided feedback on only a small subset of items, either explicitly (e.g., through star ratings) or implicitly (e.g., through views or purchases). In contrast to traditional matrix factorization approaches, Poisson factorization implicitly models each user's limited attention to consume items. Moreover, because of the mathematical form of the Poisson likelihood, the model needs only to explicitly consider the observed entries in the matrix, leading to both scalable computation and good predictive performance. We develop a variational inference algorithm for approximate posterior inference that scales up to massive data sets. This is an efficient algorithm that iterates over the observed entries and adjusts an approximate posterior over the user/item representations. We apply our method to large real-world user data containing users rating movies, users listening to songs, and users reading scientific papers. In all these settings, Bayesian Poisson factorization outperforms state-of-the-art matrix factorization methods.},
	urldate = {2016-08-26},
	journal = {arXiv:1311.1704 [cs, stat]},
	author = {Gopalan, Prem and Hofman, Jake M. and Blei, David M.},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.1704},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv\:1311.1704 PDF:/home/jeremiah/Zotero/storage/CE4E8AQH/Gopalan et al. - 2013 - Scalable Recommendation with Poisson Factorization.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/TFIPDDS2/1311.html:text/html}
}

@article{pereyra_proximal_2013,
	title = {Proximal {Markov} chain {Monte} {Carlo} algorithms},
	url = {http://arxiv.org/abs/1306.0187},
	abstract = {This paper presents a new Metropolis-adjusted Langevin algorithm (MALA) that uses convex analysis to simulate efficiently from high-dimensional densities that are log-concave, a class of probability distributions that is widely used in modern high-dimensional statistics and data analysis. The method is based on a new first-order approximation for Langevin diffusions that exploits log-concavity to construct Markov chains with favourable convergence properties. This approximation is closely related to Moreau-Yoshida regularisations for convex functions and uses proximity mappings instead of gradient mappings to approximate the continuous-time process. The proposed method complements existing MALA methods in two ways. First, the method is shown to have very robust stability properties and to converge geometrically for many target densities for which other MALA are not geometric, or only if the step size is sufficiently small. Second, the method can be applied to high-dimensional target densities that are not continuously differentiable, a class of distributions that is increasingly used in image processing and machine learning and that is beyond the scope of existing MALA and HMC algorithms. To use this method it is necessary to compute or to approximate efficiently the proximity mappings of the logarithm of the target density. For several popular models, including many Bayesian models used in modern signal and image processing and machine learning, this can be achieved with convex optimisation algorithms and with approximations based on proximal splitting techniques, which can be implemented in parallel. The proposed method is demonstrated on two challenging high-dimensional and non-differentiable models related to image resolution enhancement and low-rank matrix estimation that are not well addressed by existing MCMC methodology.},
	urldate = {2016-08-26},
	journal = {arXiv:1306.0187 [stat]},
	author = {Pereyra, Marcelo},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.0187},
	keywords = {Statistics - Computation, Statistics - Methodology}
}

@article{pimentel-alarcon_deterministic_2014,
	title = {Deterministic {Conditions} for {Subspace} {Identifiability} from {Incomplete} {Sampling}},
	url = {http://arxiv.org/abs/1410.0633},
	abstract = {Consider a generic \$r\$-dimensional subspace of \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$, \$r{\textless}d\$, and suppose that we are only given projections of this subspace onto small subsets of the canonical coordinates. The paper establishes necessary and sufficient deterministic conditions on the subsets for subspace identifiability.},
	urldate = {2016-08-26},
	journal = {arXiv:1410.0633 [cs, math, stat]},
	author = {Pimentel-Alarcón, Daniel L. and Nowak, Robert D. and Boston, Nigel},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.0633},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Mathematics - Combinatorics},
	annote = {Comment: To appear in Proc. of IEEE ISIT, 2015}
}

@article{chandrasekaran_rank-sparsity_2011,
	title = {Rank-{Sparsity} {Incoherence} for {Matrix} {Decomposition}},
	volume = {21},
	issn = {1052-6234, 1095-7189},
	url = {http://arxiv.org/abs/0906.2220},
	doi = {10.1137/090761793},
	abstract = {Suppose we are given a matrix that is formed by adding an unknown sparse matrix to an unknown low-rank matrix. Our goal is to decompose the given matrix into its sparse and low-rank components. Such a problem arises in a number of applications in model and system identification, and is NP-hard in general. In this paper we consider a convex optimization formulation to splitting the specified matrix into its components, by minimizing a linear combination of the \${\textbackslash}ell\_1\$ norm and the nuclear norm of the components. We develop a notion of {\textbackslash}emph\{rank-sparsity incoherence\}, expressed as an uncertainty principle between the sparsity pattern of a matrix and its row and column spaces, and use it to characterize both fundamental identifiability as well as (deterministic) sufficient conditions for exact recovery. Our analysis is geometric in nature, with the tangent spaces to the algebraic varieties of sparse and low-rank matrices playing a prominent role. When the sparse and low-rank matrices are drawn from certain natural random ensembles, we show that the sufficient conditions for exact recovery are satisfied with high probability. We conclude with simulation results on synthetic matrix decomposition problems.},
	number = {2},
	urldate = {2016-08-26},
	journal = {SIAM Journal on Optimization},
	author = {Chandrasekaran, Venkat and Sanghavi, Sujay and Parrilo, Pablo A. and Willsky, Alan S.},
	month = apr,
	year = {2011},
	note = {arXiv: 0906.2220},
	keywords = {Mathematics - Statistics Theory, Mathematics - Optimization and Control},
	pages = {572--596},
	file = {arXiv\:0906.2220 PDF:/home/jeremiah/Zotero/storage/XZIRWK3B/Chandrasekaran et al. - 2011 - Rank-Sparsity Incoherence for Matrix Decomposition.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/RNPGZNV2/Chandrasekaran et al. - 2011 - Rank-Sparsity Incoherence for Matrix Decomposition.html:text/html}
}

@inproceedings{hernandez-lobato_probabilistic_2014,
	title = {Probabilistic {Matrix} {Factorization} with {Non}-random {Missing} {Data}},
	url = {http://jmlr.org/proceedings/papers/v32/hernandez-lobatob14.html},
	urldate = {2016-08-26},
	author = {Hernandez-Lobato, Jose Miguel and Houlsby, Neil and Ghahramani, Zoubin},
	year = {2014},
	pages = {1512--1520},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/GRBS3U3R/Hernandez-Lobato et al. - 2014 - Probabilistic Matrix Factorization with Non-random.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/3GRNEIHH/hernandez-lobatob14.html:text/html}
}

@misc{noauthor_ole_nodate,
	title = {Ole {Winther} · {Ordinal} matrix factorization},
	url = {http://cogsys.imm.dtu.dk/ordinalmatrixfactorization/},
	urldate = {2016-08-26},
	file = {Ole Winther · Ordinal matrix factorization:/home/jeremiah/Zotero/storage/TD5WWI2J/ordinalmatrixfactorization.html:text/html}
}

@article{burer_local_2004,
	title = {Local {Minima} and {Convergence} in {Low}-{Rank} {Semidefinite} {Programming}},
	volume = {103},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/article/10.1007/s10107-004-0564-1},
	doi = {10.1007/s10107-004-0564-1},
	abstract = {.The low-rank semidefinite programming problem LRSDPr is a restriction of the semidefinite programming problem SDP in which a bound r is imposed on the rank of X, and it is well known that LRSDPr is equivalent to SDP if r is not too small. In this paper, we classify the local minima of LRSDPr and prove the optimal convergence of a slight variant of the successful, yet experimental, algorithm of Burer and Monteiro [5], which handles LRSDPr via the nonconvex change of variables X=RRT. In addition, for particular problem classes, we describe a practical technique for obtaining lower bounds on the optimal solution value during the execution of the algorithm. Computational results are presented on a set of combinatorial optimization relaxations, including some of the largest quadratic assignment SDPs solved to date.},
	language = {en},
	number = {3},
	urldate = {2016-08-26},
	journal = {Mathematical Programming},
	author = {Burer, Samuel and Monteiro, Renato D. C.},
	month = dec,
	year = {2004},
	pages = {427--444},
	file = {Snapshot:/home/jeremiah/Zotero/storage/MGSC8KMS/s10107-004-0564-1.html:text/html}
}

@article{davenport_overview_2016,
	title = {An overview of low-rank matrix recovery from incomplete observations},
	volume = {10},
	issn = {1932-4553, 1941-0484},
	url = {http://arxiv.org/abs/1601.06422},
	doi = {10.1109/JSTSP.2016.2539100},
	abstract = {Low-rank matrices play a fundamental role in modeling and computational methods for signal processing and machine learning. In many applications where low-rank matrices arise, these matrices cannot be fully sampled or directly observed, and one encounters the problem of recovering the matrix given only incomplete and indirect observations. This paper provides an overview of modern techniques for exploiting low-rank structure to perform matrix recovery in these settings, providing a survey of recent advances in this rapidly-developing field. Specific attention is paid to the algorithms most commonly used in practice, the existing theoretical guarantees for these algorithms, and representative practical applications of these techniques.},
	number = {4},
	urldate = {2016-08-26},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Davenport, Mark A. and Romberg, Justin},
	month = jun,
	year = {2016},
	note = {arXiv: 1601.06422},
	keywords = {Computer Science - Information Theory},
	pages = {608--622},
	file = {arXiv\:1601.06422 PDF:/home/jeremiah/Zotero/storage/N92FD9MG/Davenport and Romberg - 2016 - An overview of low-rank matrix recovery from incom.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/9FSHUZGH/Davenport and Romberg - 2016 - An overview of low-rank matrix recovery from incom.html:text/html}
}

@article{lafond_low_2015,
	title = {Low {Rank} {Matrix} {Completion} with {Exponential} {Family} {Noise}},
	url = {http://arxiv.org/abs/1502.06919},
	abstract = {The matrix completion problem consists in reconstructing a matrix from a sample of entries, possibly observed with noise. A popular class of estimator, known as nuclear norm penalized estimators, are based on minimizing the sum of a data fitting term and a nuclear norm penalization. Here, we investigate the case where the noise distribution belongs to the exponential family and is sub-exponential. Our framework alllows for a general sampling scheme. We first consider an estimator defined as the minimizer of the sum of a log-likelihood term and a nuclear norm penalization and prove an upper bound on the Frobenius prediction risk. The rate obtained improves on previous works on matrix completion for exponential family. When the sampling distribution is known, we propose another estimator and prove an oracle inequality w.r.t. the Kullback-Leibler prediction risk, which translates immediatly into an upper bound on the Frobenius prediction risk. Finally, we show that all the rates obtained are minimax optimal up to a logarithmic factor.},
	urldate = {2016-08-26},
	journal = {arXiv:1502.06919 [math, stat]},
	author = {Lafond, Jean},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.06919},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {arXiv\:1502.06919 PDF:/home/jeremiah/Zotero/storage/2N9A67BE/Lafond - 2015 - Low Rank Matrix Completion with Exponential Family.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/64C67ICE/1502.html:text/html}
}

@inproceedings{cao_categorical_2015,
	title = {Categorical matrix completion},
	doi = {10.1109/CAMSAP.2015.7383813},
	abstract = {We consider the problem of completing a matrix with categorical-valued entries from partial observations. This is achieved by extending the formulation and theory of one-bit matrix completion [1]. We recover a low-rank matrix M by maximizing the likelihood ratio with a constraint on the nuclear norm of M, and the observations are mapped from entries of M through multiple link functions. We establish theoretical upper and lower bounds on the recovery error, which meet up to a constant factor O(K3/2) where K is the fixed number of categories. The upper bound in our case depends on the number of categories implicitly through a maximization of terms that involve the smoothness of the link functions. In contrast to one-bit matrix completion, our bounds for categorical matrix completion are optimal up to a factor on the order of the square root of the number of categories, which is consistent with an intuition that the problem becomes harder when the number of categories increases. By comparing the performance of our method with the conventional matrix completion method on the MovieLens dataset, we demonstrate the advantage of our method.},
	booktitle = {2015 {IEEE} 6th {International} {Workshop} on {Computational} {Advances} in {Multi}-{Sensor} {Adaptive} {Processing} ({CAMSAP})},
	author = {Cao, Y. and Xie, Y.},
	month = dec,
	year = {2015},
	keywords = {matrix algebra, categorical matrix completion, Conferences, likelihood ratio, Logistics, low-rank matrix, maximum likelihood estimation, Maximum likelihood estimation, Mood, one-bit matrix completion, Recommender systems, Upper bound, Yttrium},
	pages = {369--372},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/MBQI57B7/Cao and Xie - 2015 - Categorical matrix completion.html:text/html}
}

@incollection{banerjee_clustering_2004,
	series = {Proceedings},
	title = {Clustering with {Bregman} {Divergences}},
	isbn = {978-0-89871-568-2},
	url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972740.22},
	abstract = {A wide variety of distortion functions are used for clustering, e.g., squared Euclidean distance, Mahalanobis distance and relative entropy. In this paper, we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences. The proposed algorithms unify centroid-based parametric clustering approaches, such as classical kmeans and information-theoretic clustering, which arise by special choices of the Bregman divergence. The algorithms maintain the simplicity and scalability of the classical kmeans algorithm, while generalizing the basic idea to a very large class of clustering loss functions. There are two main contributions in this paper. First, we pose the hard clustering problem in terms of minimizing the loss in Bregman information, a quantity motivated by rate-distortion theory, and present an algorithm to minimize this loss. Secondly, we show an explicit bijection between Bregman divergences and exponential families. The bijection enables the development of an alternative interpretation of an efficient EM scheme for learning models involving mixtures of exponential distributions. This leads to a simple soft clustering algorithm for all Bregman divergences.},
	urldate = {2016-08-25},
	booktitle = {Proceedings of the 2004 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Banerjee, A. and Merugu, S. and Dhillon, I. and Ghosh, J.},
	month = apr,
	year = {2004},
	pages = {234--245},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/KM7C95T3/Banerjee et al. - 2004 - Clustering with Bregman Divergences.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/VDXQ629T/1.9781611972740.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2016-08-25},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report}
}

@article{hastie_matrix_2014,
	title = {Matrix {Completion} and {Low}-{Rank} {SVD} via {Fast} {Alternating} {Least} {Squares}},
	url = {http://arxiv.org/abs/1410.2596},
	abstract = {The matrix-completion problem has attracted a lot of attention, largely as a result of the celebrated Netflix competition. Two popular approaches for solving the problem are nuclear-norm-regularized matrix approximation (Candes and Tao, 2009, Mazumder, Hastie and Tibshirani, 2010), and maximum-margin matrix factorization (Srebro, Rennie and Jaakkola, 2005). These two procedures are in some cases solving equivalent problems, but with quite different algorithms. In this article we bring the two approaches together, leading to an efficient algorithm for large matrix factorization and completion that outperforms both of these. We develop a software package "softImpute" in R for implementing our approaches, and a distributed version for very large matrices using the "Spark" cluster programming environment.},
	urldate = {2016-08-19},
	journal = {arXiv:1410.2596 [stat]},
	author = {Hastie, Trevor and Mazumder, Rahul and Lee, Jason and Zadeh, Reza},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.2596},
	keywords = {Statistics - Machine Learning, Statistics - Methodology}
}

@incollection{cortes_learning_2009,
	title = {Learning {Non}-{Linear} {Combinations} of {Kernels}},
	url = {http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf},
	urldate = {2016-08-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {396--404},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/IS8B9B4M/Cortes et al. - 2009 - Learning Non-Linear Combinations of Kernels.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/AJ46ZEBH/3692-learning-non-linear-combinations-of-kernels.html:text/html}
}

@article{bach_high-dimensional_2009,
	title = {High-{Dimensional} {Non}-{Linear} {Variable} {Selection} through {Hierarchical} {Kernel} {Learning}},
	url = {http://arxiv.org/abs/0909.0844},
	abstract = {We consider the problem of high-dimensional non-linear variable selection for supervised learning. Our approach is based on performing linear selection among exponentially many appropriately defined positive definite kernels that characterize non-linear interactions between the original variables. To select efficiently from these many kernels, we use the natural hierarchical structure of the problem to extend the multiple kernel learning framework to kernels that can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a graph-adapted sparsity-inducing norm, in polynomial time in the number of selected kernels. Moreover, we study the consistency of variable selection in high-dimensional settings, showing that under certain assumptions, our regularization framework allows a number of irrelevant variables which is exponential in the number of observations. Our simulations on synthetic datasets and datasets from the UCI repository show state-of-the-art predictive performance for non-linear regression problems.},
	urldate = {2016-08-12},
	journal = {arXiv:0909.0844 [cs, math, stat]},
	author = {Bach, Francis},
	month = sep,
	year = {2009},
	note = {arXiv: 0909.0844},
	keywords = {Computer Science - Learning, Mathematics - Statistics Theory},
	file = {arXiv\:0909.0844 PDF:/home/jeremiah/Zotero/storage/EIMZAJH5/Bach - 2009 - High-Dimensional Non-Linear Variable Selection thr.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/NHKDSF4F/0909.html:text/html}
}

@book{jaggi_simple_nodate,
	title = {A {Simple} {Algorithm} for {Nuclear} {Norm} {Regularized} {Problems}},
	abstract = {Optimization problems with a nuclear norm regularization, such as e.g. low norm matrix factorizations, have seen many applications recently. We propose a new approximation algorithm building upon the recent sparse approximate SDP solver of (Hazan, 2008). The experimental efficiency of our method is demonstrated on large matrix completion problems such as the Netflix dataset. The algorithm comes with strong convergence guarantees, and can be interpreted as a first theoretically justified variant of Simon-Funk-type SVD heuristics. The method is free of tuning parameters, and very easy to parallelize. 1.},
	author = {Jaggi, Martin}
}

@article{boyd_distributed_2011,
	title = {Distributed {Optimization} and {Statistical} {Learning} via the {Alternating} {Direction} {Method} of {Multipliers}},
	volume = {3},
	issn = {1935-8237},
	url = {http://dx.doi.org/10.1561/2200000016},
	doi = {10.1561/2200000016},
	abstract = {Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for ℓ1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.},
	number = {1},
	urldate = {2016-07-30},
	journal = {Found. Trends Mach. Learn.},
	author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
	month = jan,
	year = {2011},
	pages = {1--122}
}

@misc{noauthor_distributed_nodate,
	title = {Distributed {Optimization} and {Statistical} {Learning} via the {Alternating} {Direction} {Method} of {Multipliers}},
	url = {http://web.stanford.edu/~boyd/papers/admm_distr_stats.html},
	urldate = {2016-07-30},
	file = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers:/home/jeremiah/Zotero/storage/QFFXE5ER/admm_distr_stats.html:text/html}
}

@article{korattikara_bayesian_2015,
	title = {Bayesian {Dark} {Knowledge}},
	url = {http://arxiv.org/abs/1506.04416},
	abstract = {We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities, e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time). We describe a method for "distilling" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [Hernandez-Lobato and Adams, 2015] and an approach based on variational Bayes [Blundell et al., 2015]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.},
	urldate = {2016-07-29},
	journal = {arXiv:1506.04416 [cs, stat]},
	author = {Korattikara, Anoop and Rathod, Vivek and Murphy, Kevin and Welling, Max},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04416},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: final version submitted to NIPS 2015},
	file = {arXiv\:1506.04416 PDF:/home/jeremiah/Zotero/storage/GF6FTIRK/Korattikara et al. - 2015 - Bayesian Dark Knowledge.pdf:application/pdf}
}

@article{watson_characterization_1992,
	title = {Characterization of the subdifferential of some matrix norms},
	volume = {170},
	issn = {0024-3795},
	url = {http://www.sciencedirect.com/science/article/pii/0024379592904072},
	doi = {10.1016/0024-3795(92)90407-2},
	abstract = {A characterization is given of the subdifferential of matrix norms from two classes, orthogonally invariant norms and operator (or subordinate) norms. Specific results are derived for some special cases.},
	urldate = {2016-07-16},
	journal = {Linear Algebra and its Applications},
	author = {Watson, G. A.},
	month = jun,
	year = {1992},
	pages = {33--45},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/U4H926ZK/Watson - 1992 - Characterization of the subdifferential of some ma.html:text/html}
}

@article{mazumder_spectral_2010,
	title = {Spectral {Regularization} {Algorithms} for {Learning} {Large} {Incomplete} {Matrices}},
	volume = {11},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v11/mazumder10a.html},
	number = {Aug},
	urldate = {2016-07-16},
	journal = {Journal of Machine Learning Research},
	author = {Mazumder, Rahul and Hastie, Trevor and Tibshirani, Robert},
	year = {2010},
	pages = {2287--2322},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/3WF5ZZSR/Mazumder et al. - 2010 - Spectral Regularization Algorithms for Learning La.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/MHFCKCRF/Mazumder et al. - 2010 - Spectral Regularization Algorithms for Learning La.html:text/html}
}

@article{geramifard_tutorial_2013,
	title = {A {Tutorial} on {Linear} {Function} {Approximators} for {Dynamic} {Programming} and {Reinforcement} {Learning}},
	volume = {6},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-machine-learning/MAL-042},
	doi = {10.1561/2200000042},
	language = {en},
	number = {4},
	urldate = {2016-07-11},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Geramifard, Alborz},
	year = {2013},
	pages = {375--451}
}

@article{wibisono_variational_2016,
	title = {A {Variational} {Perspective} on {Accelerated} {Methods} in {Optimization}},
	url = {http://arxiv.org/abs/1603.04245},
	abstract = {Accelerated gradient methods play a central role in optimization, achieving optimal rates in many settings. While many generalizations and extensions of Nesterov's original acceleration method have been proposed, it is not yet clear what is the natural scope of the acceleration concept. In this paper, we study accelerated methods from a continuous-time perspective. We show that there is a Lagrangian functional that we call the {\textbackslash}emph\{Bregman Lagrangian\} which generates a large class of accelerated methods in continuous time, including (but not limited to) accelerated gradient descent, its non-Euclidean extension, and accelerated higher-order gradient methods. We show that the continuous-time limit of all of these methods correspond to traveling the same curve in spacetime at different speeds. From this perspective, Nesterov's technique and many of its generalizations can be viewed as a systematic way to go from the continuous-time curves generated by the Bregman Lagrangian to a family of discrete-time accelerated algorithms.},
	urldate = {2016-07-07},
	journal = {arXiv:1603.04245 [cs, math, stat]},
	author = {Wibisono, Andre and Wilson, Ashia C. and Jordan, Michael I.},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04245},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Mathematics - Optimization and Control},
	annote = {Comment: 38 pages. Subsumes an earlier working draft arXiv:1509.03616},
	file = {arXiv\:1603.04245 PDF:/home/jeremiah/Zotero/storage/MHZP3JMV/Wibisono et al. - 2016 - A Variational Perspective on Accelerated Methods i.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/8MN2UATE/1603.html:text/html}
}

@article{reddi_fast_2016,
	title = {Fast {Stochastic} {Methods} for {Nonsmooth} {Nonconvex} {Optimization}},
	url = {http://arxiv.org/abs/1605.06900},
	abstract = {We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonconvex part is smooth and the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we show provably faster convergence than batch proximal gradient descent. Finally, we prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, that subsumes several recent works. This paper builds upon our recent series of papers on fast stochastic methods for smooth nonconvex optimization [22, 23], with a novel analysis for nonconvex and nonsmooth functions.},
	urldate = {2016-07-07},
	journal = {arXiv:1605.06900 [cs, math, stat]},
	author = {Reddi, Sashank J. and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
	month = may,
	year = {2016},
	note = {arXiv: 1605.06900},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Mathematics - Optimization and Control}
}

@inproceedings{lim_variational_2007,
	title = {Variational {Bayesian} {Approach} to {Movie} {Rating} {Prediction}},
	url = {http://discovery.ucl.ac.uk/148000/},
	abstract = {UCL Discovery is UCL's open access repository, showcasing and providing access to UCL research outputs from all UCL disciplines.},
	urldate = {2017-03-07},
	booktitle = {Presented at: {KDD} {Cup} and {Workshop}. (2007)},
	author = {Lim, Y. J. and Teh, Y. W.},
	year = {2007},
	file = {Snapshot:/home/jeremiah/Zotero/storage/RPWKZ6M4/148000.html:text/html}
}

@incollection{paisley_bayesian_2014,
	series = {Chapman \& {Hall}/{CRC} {Handbooks} of {Modern} {Statistical} {Methods}},
	title = {Bayesian {Nonnegative} {Matrix} {Factorization} with {Stochastic} {Variational} 					{Inference}},
	isbn = {978-1-4665-0408-0},
	url = {http://www.crcnetbase.com/doi/abs/10.1201/b17520-15},
	urldate = {2017-03-07},
	booktitle = {Handbook of {Mixed} {Membership} {Models} and {Their} {Applications}},
	publisher = {Chapman and Hall/CRC},
	author = {Paisley, John and Blei, DavidM and Jordan, MichaelI},
	month = oct,
	year = {2014},
	doi = {10.1201/b17520-15},
	doi = {10.1201/b17520-15},
	pages = {205--224},
	file = {Snapshot:/home/jeremiah/Zotero/storage/73I6MVEM/b17520-15.html:text/html}
}

@article{schein_bayesian_2015,
	title = {Bayesian {Poisson} {Tensor} {Factorization} for {Inferring} {Multilateral} {Relations} from {Sparse} {Dyadic} {Event} {Counts}},
	url = {http://arxiv.org/abs/1506.03493},
	abstract = {We present a Bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns. For decades, political scientists have collected and analyzed records of the form "country \$i\$ took action \$a\$ toward country \$j\$ at time \$t\$"---known as dyadic events---in order to form and test theories of international relations. We represent these event data as a tensor of counts and develop Bayesian Poisson tensor factorization to infer a low-dimensional, interpretable representation of their salient patterns. We demonstrate that our model's predictive performance is better than that of standard non-negative tensor factorization methods. We also provide a comparison of our variational updates to their maximum likelihood counterparts. In doing so, we identify a better way to form point estimates of the latent factors than that typically used in Bayesian Poisson matrix factorization. Finally, we showcase our model as an exploratory analysis tool for political scientists. We show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international affairs.},
	urldate = {2017-03-07},
	journal = {arXiv:1506.03493 [cs, stat]},
	author = {Schein, Aaron and Paisley, John and Blei, David M. and Wallach, Hanna},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03493},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Social and Information Networks, Statistics - Applications},
	annote = {Comment: To appear in Proceedings of the 21st ACM SIGKDD Conference of Knowledge Discovery and Data Mining (KDD 2015)},
	file = {arXiv\:1506.03493 PDF:/home/jeremiah/Zotero/storage/KVX47IJP/Schein et al. - 2015 - Bayesian Poisson Tensor Factorization for Inferrin.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/9QPCE6K9/1506.html:text/html}
}

@inproceedings{li_simple_2010,
	title = {Simple {Exponential} {Family} {PCA}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/AISTATS2010_LiT10},
	urldate = {2017-03-07},
	author = {Li, Jun and Tao, Dacheng},
	year = {2010},
	pages = {453--460},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/NNW548UP/Li and Tao - 2010 - Simple Exponential Family PCA.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/4RR7857U/AISTATS2010_LiT10.html:text/html}
}

@inproceedings{gunasekar_exponential_2014,
	address = {Beijing, China},
	series = {{ICML}'14},
	title = {Exponential {Family} {Matrix} {Completion} {Under} {Structural} {Constraints}},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3045106},
	abstract = {We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low-rank, and the measurements consist of a subset, either of the exact individual entries, or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin-tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data-types, such as skewed-continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low-rank, such as block-sparsity, or a superposition structure of low-rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a matrix completion setting wherein the matrix entries are sampled from any member of the rich family of exponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a general regularizer R(.). We propose a simple convex regularized M-estimator for the generalized framework, and provide a unified and novel statistical analysis for this general class of estimators. We finally corroborate our theoretical results on simulated datasets.},
	urldate = {2017-03-07},
	booktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},
	publisher = {JMLR.org},
	author = {Gunasekar, Suriya and Ravikumar, Pradeep and Ghosh, Joydeep},
	year = {2014},
	pages = {II--1917--II--1925}
}

@article{park_assessment_2014,
	title = {Assessment of source-specific health effects associated with an unknown number of major sources of multiple air pollutants: a unified {Bayesian} approach},
	volume = {15},
	issn = {1465-4644},
	shorttitle = {Assessment of source-specific health effects associated with an unknown number of major sources of multiple air pollutants},
	url = {https://academic.oup.com/biostatistics/article/15/3/484/224338/Assessment-of-source-specific-health-effects},
	doi = {10.1093/biostatistics/kxu004},
	number = {3},
	urldate = {2017-03-07},
	journal = {Biostatistics},
	author = {Park, Eun Sug and Hopke, Philip K. and Oh, Man-Suk and Symanski, Elaine and Han, Daikwon and Spiegelman, Clifford H.},
	month = jul,
	year = {2014},
	pages = {484--497},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/DB6RMJJJ/Park et al. - 2014 - Assessment of source-specific health effects assoc.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/QQZGUVCZ/Assessment-of-source-specific-health-effects.html:text/html}
}

@incollection{wipf_new_2008,
	title = {A {New} {View} of {Automatic} {Relevance} {Determination}},
	url = {http://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf},
	urldate = {2017-03-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Wipf, David P. and Nagarajan, Srikantan S.},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {1625--1632},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/D7WHS539/Wipf and Nagarajan - 2008 - A New View of Automatic Relevance Determination.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/EACDUVF8/3372-a-new-view-of-automatic-relevance-determination.html:text/html}
}

@inproceedings{rezende_stochastic_2014,
	title = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
	url = {http://www.jmlr.org/proceedings/papers/v32/rezende14.html},
	urldate = {2017-03-06},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	year = {2014},
	pages = {1278--1286},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/TJ3BXGS9/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/6KBEDFID/rezende14.html:text/html}
}

@article{alain_what_2012,
	title = {What {Regularized} {Auto}-{Encoders} {Learn} from the {Data} {Generating} {Distribution}},
	url = {http://arxiv.org/abs/1211.4246},
	abstract = {What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.},
	urldate = {2017-03-05},
	journal = {arXiv:1211.4246 [cs, stat]},
	author = {Alain, Guillaume and Bengio, Yoshua},
	month = nov,
	year = {2012},
	note = {arXiv: 1211.4246},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1211.4246 PDF:/home/jeremiah/Zotero/storage/63MD3J9C/Alain and Bengio - 2012 - What Regularized Auto-Encoders Learn from the Data.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WKUQTRSX/1211.html:text/html}
}

@book{gu_smoothing_2013,
	title = {Smoothing {Spline} {ANOVA} {Models}},
	isbn = {978-1-4614-5369-7},
	abstract = {Nonparametric function estimation with stochastic data, otherwiseknown as smoothing, has been studied by several generations ofstatisticians. Assisted by the ample computing power in today'sservers, desktops, and laptops, smoothing methods have been findingtheir ways into everyday data analysis by practitioners. While scoresof methods have proved successful for univariate smoothing, onespractical in multivariate settings number far less. Smoothing splineANOVA models are a versatile family of smoothing methods derivedthrough roughness penalties, that are suitable for both univariate andmultivariate problems.In this book, the author presents a treatise on penalty smoothingunder a unified framework. Methods are developed for (i) regressionwith Gaussian and non-Gaussian responses as well as with censored lifetime data; (ii) density and conditional density estimation under avariety of sampling schemes; and (iii) hazard rate estimation withcensored life time data and covariates. The unifying themes are thegeneral penalized likelihood method and the construction ofmultivariate models with built-in ANOVA decompositions. Extensivediscussions are devoted to model construction, smoothing parameterselection, computation, and asymptotic convergence.Most of the computational and data analytical tools discussed in thebook are implemented in R, an open-source platform for statisticalcomputing and graphics. Suites of functions are embodied in the Rpackage gss, and are illustrated throughout the book using simulatedand real data examples.This monograph will be useful as a reference work for researchers intheoretical and applied statistics as well as for those in otherrelated disciplines. It can also be used as a text for graduate levelcourses on the subject. Most of the materials are accessible to asecond year graduate student with a good training in calculus andlinear algebra and working knowledge in basic statistical inferencessuch as linear models and maximum likelihood estimates.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Gu, Chong},
	month = jan,
	year = {2013},
	note = {Google-Books-ID: 5VxGAAAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{harville_maximum_1977,
	title = {Maximum {Likelihood} {Approaches} to {Variance} {Component} {Estimation} and to {Related} {Problems}},
	volume = {72},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2286796},
	doi = {10.2307/2286796},
	abstract = {Recent developments promise to increase greatly the popularity of maximum likelihood (ML) as a technique for estimating variance components. Patterson and Thompson (1971) proposed a restricted maximum likelihood (REML) approach which takes into account the loss in degrees of freedom resulting from estimating fixed effects. Miller (1973) developed a satisfactory asymptotic theory for ML estimators of variance components. There are many iterative algorithms that can be considered for computing the ML or REML estimates. The computations on each iteration of these algorithms are those associated with computing estimates of fixed and random effects for given values of the variance components.},
	number = {358},
	urldate = {2017-03-02},
	journal = {Journal of the American Statistical Association},
	author = {Harville, David A.},
	year = {1977},
	pages = {320--338}
}

@article{self_asymptotic_1987,
	title = {Asymptotic {Properties} of {Maximum} {Likelihood} {Estimators} and {Likelihood} {Ratio} {Tests} {Under} {Nonstandard} {Conditions}},
	volume = {82},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2289471},
	doi = {10.2307/2289471},
	abstract = {Large sample properties of the likelihood function when the true parameter value may be on the boundary of the parameter space are described. Specifically, the asymptotic distribution of maximum likelihood estimators and likelihood ratio statistics are derived. These results generalize the work of Moran (1971), Chant (1974), and Chernoff (1954). Some of Chant's results are shown to be incorrect. The approach used in deriving these results follows from comments made by Moran and Chant. The problem is shown to be asymptotically equivalent to the problem of estimating the restricted mean of a multivariate Gaussian distribution from a sample of size 1. In this representation the Gaussian random variable corresponds to the limit of the normalized score statistic and the estimate of the mean corresponds to the limit of the normalized maximum likelihood estimator. Thus the limiting distribution of the maximum likelihood estimator is the same as the distribution of the projection of the Gaussian random variable onto the region of admissible values for the mean. A variety of examples is provided for which the limiting distributions of likelihood ratio statistics are mixtures of chi-squared distributions. One example is provided with a nuisance parameter on the boundary for which the asymptotic distribution is not a mixture of chi-squared distributions.},
	number = {398},
	urldate = {2017-03-02},
	journal = {Journal of the American Statistical Association},
	author = {Self, Steven G. and Liang, Kung-Yee},
	year = {1987},
	pages = {605--610}
}

@article{zhang_hypothesis_2003,
	title = {Hypothesis testing in semiparametric additive mixed models},
	volume = {4},
	issn = {1465-4644},
	doi = {10.1093/biostatistics/4.1.57},
	abstract = {We consider testing whether the nonparametric function in a semiparametric additive mixed model is a simple fixed degree polynomial, for example, a simple linear function. This test provides a goodness-of-fit test for checking parametric models against nonparametric models. It is based on the mixed-model representation of the smoothing spline estimator of the nonparametric function and the variance component score test by treating the inverse of the smoothing parameter as an extra variance component. We also consider testing the equivalence of two nonparametric functions in semiparametric additive mixed models for two groups, such as treatment and placebo groups. The proposed tests are applied to data from an epidemiological study and a clinical trial and their performance is evaluated through simulations.},
	language = {eng},
	number = {1},
	journal = {Biostatistics (Oxford, England)},
	author = {Zhang, Daowen and Lin, Xihong},
	month = jan,
	year = {2003},
	pmid = {12925330},
	keywords = {Computer Simulation, Humans, Models, Statistical, Female, Male, Likelihood Functions, Anticonvulsants, Child, Data Interpretation, Statistical, Epilepsy, gamma-Aminobutyric Acid, Indonesia, Longitudinal Studies, Placebos, Randomized Controlled Trials as Topic, Respiratory Tract Infections, Statistics, Nonparametric, Xerophthalmia},
	pages = {57--74}
}

@article{yu_center_2011,
	title = {To {Center} or {Not} to {Center}: {That} {Is} {Not} the {Question}—{An} {Ancillarity}–{Sufficiency} {Interweaving} {Strategy} ({ASIS}) for {Boosting} {MCMC} {Efficiency}},
	volume = {20},
	issn = {1061-8600},
	shorttitle = {To {Center} or {Not} to {Center}},
	url = {http://dx.doi.org/10.1198/jcgs.2011.203main},
	doi = {10.1198/jcgs.2011.203main},
	abstract = {For a broad class of multilevel models, there exist two well-known competing parameterizations, the centered parameterization (CP) and the non-centered parameterization (NCP), for effective MCMC implementation. Much literature has been devoted to the questions of when to use which and how to compromise between them via partial CP/NCP. This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving—but not alternating—the two parameterizations. This strategy has the surprising property that failure of both the CP and NCP chains to converge geometrically does not prevent the interweaving algorithm from doing so. It achieves this seemingly magical property by taking advantage of the discordance of the two parameterizations, namely, the sufficiency of CP and the ancillarity of NCP, to substantially reduce the Markovian dependence, especially when the original CP and NCP form a “beauty and beast” pair (i.e., when one chain mixes far more rapidly than the other). The ancillarity–sufficiency reformulation of the CP–NCP dichotomy allows us to borrow insight from the well-known Basu’s theorem on the independence of (complete) sufficient and ancillary statistics, albeit a Bayesian version of Basu’s theorem is currently lacking. To demonstrate the competitiveness and versatility of this ancillarity–sufficiency interweaving strategy (ASIS) for real-world problems, we apply it to fit (1) a Cox process model for detecting changes in source intensity of photon counts observed by the Chandra X-ray telescope from a (candidate) neutron/quark star, which was the problem that motivated the ASIS strategy as it defeated other methods we initially tried; (2) a probit model for predicting latent membranous lupus nephritis; and (3) an interval-censored normal model for studying the lifetime of fluorescent lights. A bevy of open questions are presented, from the mysterious but exceedingly suggestive connections between ASIS and fiducial/structural inferences to nested ASIS for further boosting MCMC efficiency. This article has supplementary material online.},
	number = {3},
	urldate = {2017-02-28},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Yu, Yaming and Meng, Xiao-Li},
	month = jan,
	year = {2011},
	keywords = {Ancillary augmentation, Basu’s theorem, Centered parameterization, Data augmentation, EM, GLMM, Interval censoring, Latent variables, Missing data, Non-centered parameterization, Parameter-driven model, Poisson time series, Probit regression, Sufficient augmentation},
	pages = {531--570},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/BXQJ4SUG/Yu and Meng - 2011 - To Center or Not to Center That Is Not the Questi.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/5FV35CTJ/jcgs.2011.html:text/html}
}

@article{hardt_identity_2016,
	title = {Identity {Matters} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1611.04231},
	abstract = {An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as {\textbackslash}emph\{batch normalization\}, but was also key to the immense success of {\textbackslash}emph\{residual networks\}. In this work, we put the principle of {\textbackslash}emph\{identity parameterization\} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.},
	urldate = {2017-02-28},
	journal = {arXiv:1611.04231 [cs, stat]},
	author = {Hardt, Moritz and Ma, Tengyu},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.04231},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: fixed minor errors in the previous version},
	file = {arXiv\:1611.04231 PDF:/home/jeremiah/Zotero/storage/5N5XFJ86/Hardt and Ma - 2016 - Identity Matters in Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/F969TK7D/1611.html:text/html}
}

@article{sriperumbudur_empirical_2012,
	title = {On the empirical estimation of integral probability metrics},
	volume = {6},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1347974672},
	doi = {10.1214/12-EJS722},
	abstract = {Given two probability measures, ℙP{\textbackslash}mathbb\{P\} and ℚQ{\textbackslash}mathbb\{Q\} defined on a measurable space, SSS, the integral probability metric (IPM) is defined as γ{\textbackslash}EuScriptF(ℙ,ℚ)=sup\{∣∣∣∫Sfdℙ−∫Sfdℚ∣∣∣:f∈{\textbackslash}EuScriptF\},γ{\textbackslash}EuScriptF(P,Q)=sup\{{\textbar}∫SfdP−∫SfdQ{\textbar}:f∈{\textbackslash}EuScriptF\},{\textbackslash}gamma\_\{{\textbackslash}EuScript\{F\}\}({\textbackslash}mathbb\{P\},{\textbackslash}mathbb\{Q\})={\textbackslash}sup{\textbackslash}left{\textbackslash}\{{\textbackslash}left{\textbackslash}vert {\textbackslash}int\_\{S\}f{\textbackslash},d{\textbackslash}mathbb\{P\}-{\textbackslash}int\_\{S\}f{\textbackslash},d{\textbackslash}mathbb\{Q\}{\textbackslash}right{\textbackslash}vert{\textbackslash},:{\textbackslash},f{\textbackslash}in{\textbackslash}EuScript\{F\}{\textbackslash}right{\textbackslash}\}, where {\textbackslash}EuScriptF{\textbackslash}EuScriptF{\textbackslash}EuScript\{F\} is a class of real-valued bounded measurable functions on SSS. By appropriately choosing {\textbackslash}EuScriptF{\textbackslash}EuScriptF{\textbackslash}EuScript\{F\}, various popular distances between ℙP{\textbackslash}mathbb\{P\} and ℚQ{\textbackslash}mathbb\{Q\}, including the Kantorovich metric, Fortet-Mourier metric, dual-bounded Lipschitz distance (also called the Dudley metric), total variation distance, and kernel distance, can be obtained. In this paper, we consider the problem of estimating γ{\textbackslash}EuScriptFγ{\textbackslash}EuScriptF{\textbackslash}gamma\_\{{\textbackslash}EuScript\{F\}\} from finite random samples drawn i.i.d. from ℙP{\textbackslash}mathbb\{P\} and ℚQ{\textbackslash}mathbb\{Q\}. Although the above mentioned distances cannot be computed in closed form for every ℙP{\textbackslash}mathbb\{P\} and ℚQ{\textbackslash}mathbb\{Q\}, we show their empirical estimators to be easily computable, and strongly consistent (except for the total-variation distance). We further analyze their rates of convergence. Based on these results, we discuss the advantages of certain choices of {\textbackslash}EuScriptF{\textbackslash}EuScriptF{\textbackslash}EuScript\{F\} (and therefore the corresponding IPMs) over others—in particular, the kernel distance is shown to have three favorable properties compared with the other mentioned distances: it is computationally cheaper, the empirical estimate converges at a faster rate to the population value, and the rate of convergence is independent of the dimension ddd of the space (for S=ℝdS=RdS={\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}). We also provide a novel interpretation of IPMs and their empirical estimators by relating them to the problem of binary classification: while the IPM between class-conditional distributions is the negative of the optimal risk associated with a binary classifier, the smoothness of an appropriate binary classifier (e.g., support vector machine, Lipschitz classifier, etc.) is inversely related to the empirical estimator of the IPM between these class-conditional distributions.},
	language = {EN},
	urldate = {2017-02-28},
	journal = {Electronic Journal of Statistics},
	author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
	year = {2012},
	mrnumber = {MR2988458},
	zmnumber = {1295.62035},
	keywords = {dual-bounded Lipschitz distance (Dudley metric), empirical estimation, Integral probability metrics, Kantorovich metric, kernel distance, Lipschitz classifier, Rademacher average, reproducing kernel Hilbert space, support vector machine},
	pages = {1550--1599},
	file = {Snapshot:/home/jeremiah/Zotero/storage/7TZBH7VJ/1347974672.html:text/html}
}

@article{muller_integral_1997,
	title = {Integral {Probability} {Metrics} and {Their} {Generating} {Classes} of {Functions}},
	volume = {29},
	issn = {0001-8678},
	url = {http://www.jstor.org/stable/1428011},
	doi = {10.2307/1428011},
	abstract = {We consider probability metrics of the following type: for a class F of functions and probability measures P, Q we define \$d\_\{{\textbackslash}germ\{F\}\}(P,{\textbackslash} Q){\textbackslash}coloneq {\textbackslash}text\{sup\}\_\{f{\textbackslash}in {\textbackslash}germ\{F\}\}{\textbar}{\textbackslash}int f{\textbackslash},dP-{\textbackslash}int f{\textbackslash},dQ{\textbar}\$. A unified study of such integral probability metrics is given. We characterize the maximal class of functions that generates such a metric. Further, we show how some interesting properties of these probability metrics arise directly from conditions on the generating class of functions. The results are illustrated by several examples, including the Kolmogorov metric, the Dudley metric and the stop-loss metric.},
	number = {2},
	urldate = {2017-02-28},
	journal = {Advances in Applied Probability},
	author = {Müller, Alfred},
	year = {1997},
	pages = {429--443}
}

@article{safran_depth_2016,
	title = {Depth {Separation} in {ReLU} {Networks} for {Approximating} {Smooth} {Non}-{Linear} {Functions}},
	url = {http://arxiv.org/abs/1610.09887},
	abstract = {We provide a depth-based separation result for feed-forward ReLU neural networks, showing that a wide family of non-linear, twice-differentiable functions on \$[0,1]{\textasciicircum}d\$, which can be approximated to accuracy \${\textbackslash}epsilon\$ by ReLU networks of depth and width \${\textbackslash}mathcal\{O\}({\textbackslash}text\{poly\}({\textbackslash}log(1/{\textbackslash}epsilon)))\$, cannot be approximated to similar accuracy by constant-depth ReLU networks, unless their width is at least \${\textbackslash}Omega(1/{\textbackslash}epsilon)\$.},
	urldate = {2017-02-27},
	journal = {arXiv:1610.09887 [cs, stat]},
	author = {Safran, Itay and Shamir, Ohad},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.09887},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1610.09887 PDF:/home/jeremiah/Zotero/storage/8Q7HMNKE/Safran and Shamir - 2016 - Depth Separation in ReLU Networks for Approximatin.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/EGVA5PQH/1610.html:text/html}
}

@article{eldan_power_2015,
	title = {The {Power} of {Depth} for {Feedforward} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1512.03965},
	abstract = {We show that there is a simple (approximately radial) function on \${\textbackslash}reals{\textasciicircum}d\$, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectified linear units, sigmoids and thresholds, and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different.},
	urldate = {2017-02-27},
	journal = {arXiv:1512.03965 [cs, stat]},
	author = {Eldan, Ronen and Shamir, Ohad},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03965},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Accepted to COLT 2016; Fixed a bug in the proof of claim 2 (now requiring the mild assumption that the activations are polynomially bounded); Other minor revisions},
	file = {arXiv\:1512.03965 PDF:/home/jeremiah/Zotero/storage/EBE4W66B/Eldan and Shamir - 2015 - The Power of Depth for Feedforward Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/E5VXER8G/1512.html:text/html}
}

@article{cohen_expressive_2015,
	title = {On the {Expressive} {Power} of {Deep} {Learning}: {A} {Tensor} {Analysis}},
	shorttitle = {On the {Expressive} {Power} of {Deep} {Learning}},
	url = {http://arxiv.org/abs/1509.05009},
	abstract = {It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones. Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited. In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date. In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling. An equivalence between the networks and hierarchical tensor factorizations is established. We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network. Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance. The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community.},
	urldate = {2017-02-27},
	journal = {arXiv:1509.05009 [cs, stat]},
	author = {Cohen, Nadav and Sharir, Or and Shashua, Amnon},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.05009},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Numerical Analysis},
	file = {arXiv\:1509.05009 PDF:/home/jeremiah/Zotero/storage/AGW9IWDW/Cohen et al. - 2015 - On the Expressive Power of Deep Learning A Tensor.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/CD2IGI4R/1509.html:text/html}
}

@article{telgarsky_representation_2015,
	title = {Representation {Benefits} of {Deep} {Feedforward} {Networks}},
	url = {http://arxiv.org/abs/1509.08101},
	abstract = {This note provides a family of classification problems, indexed by a positive integer \$k\$, where all shallow networks with fewer than exponentially (in \$k\$) many nodes exhibit error at least \$1/6\$, whereas a deep network with 2 nodes in each of \$2k\$ layers achieves zero error, as does a recurrent network with 3 distinct nodes iterated \$k\$ times. The proof is elementary, and the networks are standard feedforward networks with ReLU (Rectified Linear Unit) nonlinearities.},
	urldate = {2017-02-27},
	journal = {arXiv:1509.08101 [cs]},
	author = {Telgarsky, Matus},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.08101},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1509.08101 PDF:/home/jeremiah/Zotero/storage/6KMD5CBC/Telgarsky - 2015 - Representation Benefits of Deep Feedforward Networ.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/K3GQ4GKC/1509.html:text/html}
}

@article{hornik_approximation_1991,
	title = {Approximation capabilities of multilayer feedforward networks},
	volume = {4},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
	doi = {10.1016/0893-6080(91)90009-T},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	number = {2},
	urldate = {2017-02-27},
	journal = {Neural Networks},
	author = {Hornik, Kurt},
	year = {1991},
	keywords = {Activation function, Input environment measure, Lp(μ) approximation, Multilayer feedforward networks, Smooth approximation, Sobolev spaces, Uniform approximation, Universal approximation capabilities},
	pages = {251--257},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/74AKMPMR/Hornik - 1991 - Approximation capabilities of multilayer feedforwa.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/9FXIAX22/089360809190009T.html:text/html}
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {0932-4194, 1435-568X},
	url = {https://link.springer.com/article/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	language = {en},
	number = {4},
	urldate = {2017-02-27},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/QUCRPINM/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/RVXRNJIZ/BF02551274.html:text/html}
}

@article{sejdinovic_hypothesis_2012,
	title = {Hypothesis testing using pairwise distances and associated kernels (with {Appendix})},
	url = {http://arxiv.org/abs/1205.0411},
	abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. The equivalence holds when energy distances are computed with semimetrics of negative type, in which case a kernel may be defined such that the RKHS distance between distributions corresponds exactly to the energy distance. We determine the class of probability distributions for which kernels induced by semimetrics are characteristic (that is, for which embeddings of the distributions to an RKHS are injective). Finally, we investigate the performance of this family of kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
	urldate = {2017-02-26},
	journal = {arXiv:1205.0411 [cs, stat]},
	author = {Sejdinovic, Dino and Gretton, Arthur and Sriperumbudur, Bharath and Fukumizu, Kenji},
	month = may,
	year = {2012},
	note = {arXiv: 1205.0411},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology},
	annote = {Comment: Appearing in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012},
	file = {arXiv\:1205.0411 PDF:/home/jeremiah/Zotero/storage/4ZAVC4EG/Sejdinovic et al. - 2012 - Hypothesis testing using pairwise distances and as.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/JCVXG594/1205.html:text/html}
}

@article{viana_source_2008,
	title = {Source apportionment of particulate matter in {Europe}: {A} review of methods and results},
	volume = {39},
	issn = {0021-8502},
	shorttitle = {Source apportionment of particulate matter in {Europe}},
	url = {http://www.sciencedirect.com/science/article/pii/S0021850208001018},
	doi = {10.1016/j.jaerosci.2008.05.007},
	abstract = {European publications dealing with source apportionment (SA) of atmospheric particulate matter (PM) between 1987 and 2007 were reviewed in the present work, with a focus on methods and results. The main goal of this meta-analysis was to provide a review of the most commonly used SA methods in Europe, their comparability and results, and to evaluate current trends and identify possible gaps of the methods and future research directions. Our analysis showed that studies throughout Europe agree on the identification of four main source types ( PM 10 and PM 2.5 ): a vehicular source (traced by carbon/Fe/Ba/Zn/Cu), a crustal source (Al/Si/Ca/Fe), a sea-salt source (Na/Cl/Mg), and a mixed industrial/fuel-oil combustion ( V / Ni / SO 4 2 - ) and a secondary aerosol ( SO 4 2 - / NO 3 - / NH 4 + ) source (the latter two probably representing the same source type). Their contributions to bulk PM levels varied widely at different monitoring sites, and showed clear spatial patterns in the cases of the crustal and sea-salt sources. Other specific sources such as biomass combustion or shipping emissions were rarely identified, even though they may contribute significantly to PM levels in specific locations.},
	number = {10},
	urldate = {2017-02-24},
	journal = {Journal of Aerosol Science},
	author = {Viana, M. and Kuhlbusch, T. A. J. and Querol, X. and Alastuey, A. and Harrison, R. M. and Hopke, P. K. and Winiwarter, W. and Vallius, M. and Szidat, S. and Prévôt, A. S. H. and Hueglin, C. and Bloemen, H. and Wåhlin, P. and Vecchi, R. and Miranda, A. I. and Kasper-Giebl, A. and Maenhaut, W. and Hitzenberger, R.},
	month = oct,
	year = {2008},
	keywords = {Biomass burning, Emission sources, PM    10, PM    2.5, receptor modelling, Research directions, Shipping emissions, Tracers},
	pages = {827--849},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/S4V8WS77/S0021850208001018.html:text/html}
}

@article{viana_source_2008-1,
	title = {Source apportionment of particulate matter in {Europe}: {A} review of methods and results},
	volume = {39},
	issn = {0021-8502},
	shorttitle = {Source apportionment of particulate matter in {Europe}},
	url = {http://www.sciencedirect.com/science/article/pii/S0021850208001018},
	doi = {10.1016/j.jaerosci.2008.05.007},
	abstract = {European publications dealing with source apportionment (SA) of atmospheric particulate matter (PM) between 1987 and 2007 were reviewed in the present work, with a focus on methods and results. The main goal of this meta-analysis was to provide a review of the most commonly used SA methods in Europe, their comparability and results, and to evaluate current trends and identify possible gaps of the methods and future research directions. Our analysis showed that studies throughout Europe agree on the identification of four main source types ( PM 10 and PM 2.5 ): a vehicular source (traced by carbon/Fe/Ba/Zn/Cu), a crustal source (Al/Si/Ca/Fe), a sea-salt source (Na/Cl/Mg), and a mixed industrial/fuel-oil combustion ( V / Ni / SO 4 2 - ) and a secondary aerosol ( SO 4 2 - / NO 3 - / NH 4 + ) source (the latter two probably representing the same source type). Their contributions to bulk PM levels varied widely at different monitoring sites, and showed clear spatial patterns in the cases of the crustal and sea-salt sources. Other specific sources such as biomass combustion or shipping emissions were rarely identified, even though they may contribute significantly to PM levels in specific locations.},
	number = {10},
	urldate = {2017-02-24},
	journal = {Journal of Aerosol Science},
	author = {Viana, M. and Kuhlbusch, T. A. J. and Querol, X. and Alastuey, A. and Harrison, R. M. and Hopke, P. K. and Winiwarter, W. and Vallius, M. and Szidat, S. and Prévôt, A. S. H. and Hueglin, C. and Bloemen, H. and Wåhlin, P. and Vecchi, R. and Miranda, A. I. and Kasper-Giebl, A. and Maenhaut, W. and Hitzenberger, R.},
	month = oct,
	year = {2008},
	keywords = {Biomass burning, Emission sources, PM    10, PM    2.5, receptor modelling, Research directions, Shipping emissions, Tracers},
	pages = {827--849},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/GSKHEE5P/S0021850208001018.html:text/html}
}

@article{jun_multivariate_2013,
	title = {Multivariate {Receptor} {Models} for {Spatially} {Correlated} {Multipollutant} {Data}},
	issn = {0040-1706},
	url = {http://repository.kaust.edu.sa/kaust/handle/10754/598923},
	doi = {10.1080/00401706.2013.765321},
	abstract = {The goal of multivariate receptor modeling is to estimate the profiles of major pollution sources and quantify their impacts based on ambient measurements of pollutants. Traditionally, multivariate receptor modeling has been applied to multiple air pollutant data measured at a single monitoring site or measurements of a single pollutant collected at multiple monitoring sites. Despite the growing availability of multipollutant data collected from multiple monitoring sites, there has not yet been any attempt to incorporate spatial dependence that may exist in such data into multivariate receptor modeling. We propose a spatial statistics extension of multivariate receptor models that enables us to incorporate spatial dependence into estimation of source composition profiles and contributions given the prespecified number of sources and the model identification conditions. The proposed method yields more precise estimates of source profiles by accounting for spatial dependence in the estimation. More importantly, it enables predictions of source contributions at unmonitored sites as well as when there are missing values at monitoring sites. The method is illustrated with simulated data and real multipollutant data collected from eight monitoring sites in Harris County, Texas. Supplementary materials for this article, including data and R code for implementing the methods, are available online on the journal web site. © 2013 Copyright Taylor and Francis Group, LLC.},
	urldate = {2017-02-24},
	journal = {Technometrics},
	author = {Jun, Mikyoung and Park, Eun Sug},
	month = aug,
	year = {2013},
	file = {Snapshot:/home/jeremiah/Zotero/storage/SIX2VVH2/598923.html:text/html}
}

@article{sug_park_bilinear_2002,
	title = {Bilinear estimation of pollution source profiles and amounts by using multivariate receptor models},
	volume = {13},
	issn = {1099-095X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/env.557/abstract},
	doi = {10.1002/env.557},
	abstract = {Multivariate receptor models aim to identify the pollution sources based on multivariate air pollution data. This article is concerned with estimation of the source profiles (pollution recipes) and their contributions (amounts of pollution). The estimation procedures are based on constrained nonlinear least squares methods with the constraints given by nonnegativity and identifiability conditions of the model parameters. We investigate several identifiability conditions that are appropriate in the context of receptor models, and also present new sets of identifiability conditions, which are often reasonable in practice when the other traditional identifiability conditions fail. The resulting estimators are consistent under appropriate identifiability conditions, and standard errors for the estimators are also provided. Simulation and application to real air pollution data illustrate the results. Copyright © 2002 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {7},
	urldate = {2017-02-24},
	journal = {Environmetrics},
	author = {Sug Park, Eun and Spiegelman, Clifford H. and Henry, Ronald C.},
	month = nov,
	year = {2002},
	keywords = {bootstrap, consistency, constrained nonlinear least squares, model identifiability, multivariate receptor model},
	pages = {775--798},
	file = {Snapshot:/home/jeremiah/Zotero/storage/ASVWTP7S/abstract.html:text/html}
}

@article{park_multivariate_2001,
	title = {Multivariate {Receptor} {Modeling} for {Temporally} {Correlated} {Data} by {Using} {MCMC}},
	volume = {96},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/3085880},
	abstract = {Multivariate receptor modeling aims to estimate pollution source profiles and the amounts of pollution based on a series of ambient concentrations of multiple chemical species over time. Air pollution data often show temporal dependence due to meteorology and/or background sources. Previous approaches to receptor modeling do not incorporate this dependence. We model dependence in the data using a time series approach so that we can incorporate extra sources of variability in parameter estimation and uncertainty estimation. We estimate parameters using the Markov chain Monte Carlo method, which makes simultaneous estimation of parameters and uncertainties possible. The methods are applied to simulated data and 1990 Atlanta air pollution data. The results show promise towards the goal of accounting for the dependence in the data.},
	number = {456},
	urldate = {2017-02-24},
	journal = {Journal of the American Statistical Association},
	author = {Park, Eun Sug and Guttorp, Peter and Henry, Ronald C.},
	year = {2001},
	pages = {1171--1183}
}

@article{heaton_incorporating_2010,
	title = {Incorporating {Time}-{Dependent} {Source} {Profiles} {Using} the {Dirichlet} {Distribution} in {Multivariate} {Receptor} {Models}},
	volume = {52},
	issn = {0040-1706},
	url = {http://dukespace.lib.duke.edu/dspace/handle/10161/4405},
	doi = {10.1198/TECH.2009.08134},
	abstract = {Multivariate receptor modeling is used to estimate profiles and contributions of pollution sources from concentrations of pollutants such as particulate matter in the air. The majority of previous approaches to multivariate receptor modeling assume pollution source profiles are constant through time. In an effort to relax this assumption, this article uses the Dirichlet distribution in a dynamic linear receptor model for pollution source profiles. The receptor model developed herein is evaluated using simulated datasets and then applied to a physical dataset of chemical species concentrations measured at the U.S. Environmental Protection Agency's St. Louis Midwest supersite. Supplemental materials to this articles are available online.},
	language = {en\_US},
	number = {1},
	urldate = {2017-02-24},
	author = {Heaton, Matthew J.},
	year = {2010},
	pages = {67--79},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/VZWWC8S3/Heaton - 2010 - Incorporating Time-Dependent Source Profiles Using.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/TBCJ3UK9/4405.html:text/html}
}

@article{heaton_incorporating_2010-1,
	title = {Incorporating {Time}-{Dependent} {Source} {Profiles} {Using} the {Dirichlet} {Distribution} in {Multivariate} {Receptor} {Models}},
	volume = {52},
	issn = {0040-1706},
	url = {http://dukespace.lib.duke.edu/dspace/handle/10161/4405},
	doi = {10.1198/TECH.2009.08134},
	abstract = {Multivariate receptor modeling is used to estimate profiles and contributions of pollution sources from concentrations of pollutants such as particulate matter in the air. The majority of previous approaches to multivariate receptor modeling assume pollution source profiles are constant through time. In an effort to relax this assumption, this article uses the Dirichlet distribution in a dynamic linear receptor model for pollution source profiles. The receptor model developed herein is evaluated using simulated datasets and then applied to a physical dataset of chemical species concentrations measured at the U.S. Environmental Protection Agency's St. Louis Midwest supersite. Supplemental materials to this articles are available online.},
	language = {en\_US},
	number = {1},
	urldate = {2017-02-24},
	author = {Heaton, Matthew J.},
	year = {2010},
	pages = {67--79},
	file = {Snapshot:/home/jeremiah/Zotero/storage/KQ5ZPQJN/Heaton - 2010 - Incorporating Time-Dependent Source Profiles Using.html:text/html}
}

@article{lingwall_dirichlet_2008,
	title = {Dirichlet based {Bayesian} multivariate receptor modeling},
	volume = {19},
	issn = {1099-095X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/env.902/abstract},
	doi = {10.1002/env.902},
	abstract = {We propose a simple, fully Bayesian approach for multivariate receptor modeling that allows for flexible and consistent incorporation of a priori information. The model uses a generalization of the Dirichlet distribution as the prior distribution on source profiles that allows great flexibility in the specification of prior information. Heavy-tailed lognormal distributions are used as priors on source contributions to match the nature of particulate concentrations. A simulation study based on the Washington, DC airshed shows that the model compares favorably to Positive Matrix Factorization, a standard analysis approach used for pollution source apportionment. A significant advantage of the proposed approach compared to most popularly used methods is that the Bayesian framework yields complete distributional results for each parameter of interest (including distributions for each element of the source profile and source contribution matrices). These distributions offer a great deal of power and versatility when addressing complex questions of interest to the researcher. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2017-02-24},
	journal = {Environmetrics},
	author = {Lingwall, Jeff W. and Christensen, William F. and Reese, C. Shane},
	month = sep,
	year = {2008},
	keywords = {air pollution, Bayesian methods, chemical mass balance, pollution source apportionment, source attribution},
	pages = {618--629},
	file = {Snapshot:/home/jeremiah/Zotero/storage/JPTBRAB4/Lingwall et al. - 2008 - Dirichlet based Bayesian multivariate receptor mod.html:text/html}
}

@article{nikolov_statistical_2008,
	title = {Statistical methods to evaluate health effects associated with major sources of air pollution: a case-study of breathing patterns during exposure to concentrated {Boston} air particles},
	volume = {57},
	issn = {1467-9876},
	shorttitle = {Statistical methods to evaluate health effects associated with major sources of air pollution},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2008.00618.x/abstract},
	doi = {10.1111/j.1467-9876.2008.00618.x},
	abstract = {Summary.  We conduct a case-study evaluating the source-specific effects of particulate matter on respiratory function. Using a structural equation approach, we assess the effect of different receptor models on the estimated source-specific effects for univariate respiratory response. Furthermore, we extend the structural equation model by placing a factor analysis model on the response to represent the measured respiratory responses in terms of underlying respiratory patterns. We estimate the particulate matter source-specific effects on respiratory rate, accentuated normal breathing and airway irritation and find a strong increase in airway irritation that is associated with exposure to motor vehicle particulate matter.},
	language = {en},
	number = {3},
	urldate = {2017-02-24},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Nikolov, Margaret C. and Coull, Brent A. and Catalano, Paul J. and Diaz, Edgar and Godleski, John J.},
	month = jun,
	year = {2008},
	keywords = {Latent variables, Particulate matter, Receptor model, Respiratory response, source apportionment, Structural equation model},
	pages = {357--378},
	file = {Snapshot:/home/jeremiah/Zotero/storage/AVNBE8VC/abstract.html:text/html}
}

@article{park_assessment_2014-1,
	title = {Assessment of source-specific health effects associated with an unknown number of major sources of multiple air pollutants: a unified {Bayesian} approach},
	volume = {15},
	issn = {1465-4644},
	shorttitle = {Assessment of source-specific health effects associated with an unknown number of major sources of multiple air pollutants},
	url = {https://academic.oup.com/biostatistics/article/15/3/484/224338/Assessment-of-source-specific-health-effects},
	doi = {10.1093/biostatistics/kxu004},
	number = {3},
	urldate = {2017-02-24},
	journal = {Biostatistics},
	author = {Park, Eun Sug and Hopke, Philip K. and Oh, Man-Suk and Symanski, Elaine and Han, Daikwon and Spiegelman, Clifford H.},
	month = jul,
	year = {2014},
	pages = {484--497},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/GA9KEKJK/Park et al. - 2014 - Assessment of source-specific health effects assoc.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/D68H65WQ/Assessment-of-source-specific-health-effects.html:text/html}
}

@article{nikolov_informative_2007,
	title = {An informative {Bayesian} structural equation model to assess source-specific health effects of air pollution},
	volume = {8},
	issn = {1465-4644},
	doi = {10.1093/biostatistics/kxl032},
	abstract = {A primary objective of current air pollution research is the assessment of health effects related to specific sources of air particles or particulate matter (PM). Quantifying source-specific risk is a challenge because most PM health studies do not directly observe the contributions of the pollution sources themselves. Instead, given knowledge of the chemical characteristics of known sources, investigators infer pollution source contributions via a source apportionment or multivariate receptor analysis applied to a large number of observed elemental concentrations. Although source apportionment methods are well established for exposure assessment, little work has been done to evaluate the appropriateness of characterizing unobservable sources thus in health effects analyses. In this article, we propose a structural equation framework to assess source-specific health effects using speciated elemental data. This approach corresponds to fitting a receptor model and the health outcome model jointly, such that inferences on the health effects account for the fact that uncertainty is associated with the source contributions. Since the structural equation model (SEM) typically involves a large number of parameters, for small-sample settings, we propose a fully Bayesian estimation approach that leverages historical exposure data from previous related exposure studies. We compare via simulation the performance of our approach in estimating source-specific health effects to that of 2 existing approaches, a tracer approach and a 2-stage approach. Simulation results suggest that the proposed informative Bayesian SEM is effective in eliminating the bias incurred by the 2 existing approaches, even when the number of exposures is limited. We employ the proposed methods in the analysis of a concentrator study investigating the association between ST-segment, a cardiovascular outcome, and major sources of Boston PM and discuss the implications of our findings with respect to the design of future PM concentrator studies.},
	language = {eng},
	number = {3},
	journal = {Biostatistics (Oxford, England)},
	author = {Nikolov, Margaret C. and Coull, Brent A. and Catalano, Paul J. and Godleski, John J.},
	month = jul,
	year = {2007},
	pmid = {17032699},
	keywords = {Humans, Bayes Theorem, air pollution, Animals, Biometry, Dogs, Environmental Health, Models, Biological, Myocardial Ischemia},
	pages = {609--624}
}

@article{lin_variance_1997,
	title = {Variance component testing in generalised linear models with random effects},
	volume = {84},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article-abstract/84/2/309/233889/Variance-component-testing-in-generalised-linear},
	doi = {10.1093/biomet/84.2.309},
	number = {2},
	urldate = {2017-02-23},
	journal = {Biometrika},
	author = {Lin, Xihong},
	month = jun,
	year = {1997},
	pages = {309--326},
	file = {Snapshot:/home/jeremiah/Zotero/storage/I8KNVU9Z/Variance-component-testing-in-generalised-linear.html:text/html}
}

@article{litiere_impact_2008,
	title = {The impact of a misspecified random-effects distribution on the estimation and the performance of inferential procedures in generalized linear mixed models},
	volume = {27},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/sim.3157/abstract},
	doi = {10.1002/sim.3157},
	abstract = {Estimation in generalized linear mixed models (GLMMs) is often based on maximum likelihood theory, assuming that the underlying probability model is correctly specified. However, the validity of this assumption is sometimes difficult to verify. In this paper we study, through simulations, the impact of misspecifying the random-effects distribution on the estimation and hypothesis testing in GLMMs. It is shown that the maximum likelihood estimators are inconsistent in the presence of misspecification. The bias induced in the mean-structure parameters is generally small, as far as the variability of the underlying random-effects distribution is small as well. However, the estimates of this variability are always severely biased. Given that the variance components are the only tool to study the variability of the true distribution, it is difficult to assess whether problems in the estimation of the mean structure occur. The type I error rate and the power of the commonly used inferential procedures are also severely affected. The situation is aggravated if more than one random effect is included in the model. Further, we propose to deal with possible misspecification by way of sensitivity analysis, considering several random-effects distributions. All the results are illustrated using data from a clinical trial in schizophrenia. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {16},
	urldate = {2017-02-23},
	journal = {Statistics in Medicine},
	author = {Litière, S. and Alonso, A. and Molenberghs, G.},
	month = jul,
	year = {2008},
	keywords = {consistency, heterogeneity model, Kullback–Leibler information criterion, non-normal random effects, power, type I error},
	pages = {3125--3144},
	file = {Snapshot:/home/jeremiah/Zotero/storage/64M5MTZ2/abstract.html:text/html}
}

@article{exterkate_model_2013,
	title = {Model selection in kernel ridge regression},
	volume = {68},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947313002181},
	doi = {10.1016/j.csda.2013.06.006},
	abstract = {Kernel ridge regression is a technique to perform ridge regression with a potentially infinite number of nonlinear transformations of the independent variables as regressors. This method is gaining popularity as a data-rich nonlinear forecasting tool, which is applicable in many different contexts. The influence of the choice of kernel and the setting of tuning parameters on forecast accuracy is investigated. Several popular kernels are reviewed, including polynomial kernels, the Gaussian kernel, and the Sinc kernel. The latter two kernels are interpreted in terms of their smoothing properties, and the tuning parameters associated to all these kernels are related to smoothness measures of the prediction function and to the signal-to-noise ratio. Based on these interpretations, guidelines are provided for selecting the tuning parameters from small grids using cross-validation. A Monte Carlo study confirms the practical usefulness of these rules of thumb. Finally, the flexible and smooth functional forms provided by the Gaussian and Sinc kernels make them widely applicable. Therefore, their use is recommended instead of the popular polynomial kernels in general settings, where no information on the data-generating process is available.},
	urldate = {2017-02-22},
	journal = {Computational Statistics \& Data Analysis},
	author = {Exterkate, Peter},
	month = dec,
	year = {2013},
	keywords = {High dimensionality, Kernel methods, Nonlinear forecasting, Shrinkage estimation},
	pages = {1--16},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/K93TF5RE/Exterkate - 2013 - Model selection in kernel ridge regression.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/7SQN2AAU/S0167947313002181.html:text/html}
}

@article{paatero_methods_2014,
	title = {Methods for estimating uncertainty in factor analytic solutions},
	volume = {7},
	issn = {1867-8548},
	url = {http://www.atmos-meas-tech.net/7/781/2014/},
	doi = {10.5194/amt-7-781-2014},
	abstract = {The EPA PMF (Environmental Protection Agency positive matrix factorization) version 5.0 and the underlying multilinear engine-executable ME-2 contain three methods for estimating uncertainty in factor analytic models: classical bootstrap (BS), displacement of factor elements (DISP), and bootstrap enhanced by displacement of factor elements (BS-DISP). The goal of these methods is to capture the uncertainty of PMF analyses due to random errors and rotational ambiguity. It is shown that the three methods complement each other: depending on characteristics of the data set, one method may provide better results than the other two. Results are presented using synthetic data sets, including interpretation of diagnostics, and recommendations are given for parameters to report when documenting uncertainty estimates from EPA PMF or ME-2 applications.},
	number = {3},
	urldate = {2017-02-21},
	journal = {Atmos. Meas. Tech.},
	author = {Paatero, P. and Eberly, S. and Brown, S. G. and Norris, G. A.},
	month = mar,
	year = {2014},
	pages = {781--797},
	file = {Atmos. Meas. Tech. PDF:/home/jeremiah/Zotero/storage/WD48FZVG/Paatero et al. - 2014 - Methods for estimating uncertainty in factor analy.pdf:application/pdf}
}

@article{henry_multivariate_2003,
	title = {Multivariate receptor modeling by {N}-dimensional edge detection},
	volume = {65},
	issn = {0169-7439},
	url = {http://www.sciencedirect.com/science/article/pii/S0169743902001089},
	doi = {10.1016/S0169-7439(02)00108-9},
	abstract = {The mathematical details of the Unmix multivariate receptor model for air quality data are given. Primary among these is an algorithm to find edges (more correctly hyperplanes) in sets of points in N-dimensional space. An example with simulated data is given.},
	number = {2},
	urldate = {2017-02-21},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Henry, Ronald C.},
	month = feb,
	year = {2003},
	keywords = {Particulate matter, Air quality, Edge detection, Multivariate statistics, Receptor modeling},
	pages = {179--189},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/95XAKV8T/Henry - 2003 - Multivariate receptor modeling by N-dimensional ed.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/F9H3V6JK/S0169743902001089.html:text/html}
}

@article{henry_multivariate_2003-1,
	title = {Multivariate receptor modeling by {N}-dimensional edge detection},
	volume = {65},
	issn = {0169-7439},
	url = {http://www.sciencedirect.com/science/article/pii/S0169743902001089},
	doi = {10.1016/S0169-7439(02)00108-9},
	abstract = {The mathematical details of the Unmix multivariate receptor model for air quality data are given. Primary among these is an algorithm to find edges (more correctly hyperplanes) in sets of points in N-dimensional space. An example with simulated data is given.},
	number = {2},
	urldate = {2017-02-21},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Henry, Ronald C.},
	month = feb,
	year = {2003},
	keywords = {Particulate matter, Air quality, Edge detection, Multivariate statistics, Receptor modeling},
	pages = {179--189},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/GZZSJQJC/Henry - 2003 - Multivariate receptor modeling by N-dimensional ed.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/VTU6U4DR/S0169743902001089.html:text/html}
}

@article{hopke_review_2016,
	title = {Review of receptor modeling methods for source apportionment},
	volume = {66},
	issn = {1096-2247},
	url = {http://dx.doi.org/10.1080/10962247.2016.1140693},
	doi = {10.1080/10962247.2016.1140693},
	abstract = {Efforts have been made to relate measured concentrations of airborne constituents to their origins for more than 50 years. During this time interval, there have been developments in the measurement technology to gather highly time-resolved, detailed chemical compositional data. Similarly, the improvements in computers have permitted a parallel development of data analysis tools that permit the extraction of information from these data. There is now a substantial capability to provide useful insights into the sources of pollutants and their atmospheric processing that can help inform air quality management options. Efforts have been made to combine receptor and chemical transport models to provide improved apportionments. Tools are available to utilize limited numbers of known profiles with the ambient data to obtain more accurate apportionments for targeted sources. In addition, tools are in place to allow more advanced models to be fitted to the data based on conceptual models of the nature of the sources and the sampling/analytical approach. Each of the approaches has its strengths and weaknesses. However, the field as a whole suffers from a lack of measurements of source emission compositions. There has not been an active effort to develop source profiles for stationary sources for a long time, and with many significant sources built in developing countries, the lack of local profiles is a serious problem in effective source apportionment. The field is now relatively mature in terms of its methods and its ability to adapt to new measurement technologies, so that we can be assured of a high likelihood of extracting the maximal information from the collected data.Implications: Efforts have been made over the past 50 years to use air quality data to estimate the influence of air pollution sources. These methods are now relatively mature and many are readily accessible through publically available software. This review examines the development of receptor models and the current state of the art in extracting source identification and apportionments from ambient air quality data.},
	number = {3},
	urldate = {2017-02-21},
	journal = {Journal of the Air \& Waste Management Association},
	author = {Hopke, Philip K.},
	month = mar,
	year = {2016},
	pmid = {26756961},
	pages = {237--259},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/B34289DX/Hopke - 2016 - Review of receptor modeling methods for source app.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/XG425IJ4/10962247.2016.html:text/html}
}

@article{watson_receptor_1989,
	title = {Receptor {Models} in {Air} {Resources} {Management}: {A} {Summary} of the {APCA} {International} {Specialty} {Conference}},
	volume = {39},
	issn = {0894-0630},
	shorttitle = {Receptor {Models} in {Air} {Resources} {Management}},
	url = {http://dx.doi.org/10.1080/08940630.1989.10466539},
	doi = {10.1080/08940630.1989.10466539},
	abstract = {An APCA (now, the Air \& Waste Management Association) International Specialty Conference was held in San Francisco, California in February 1988 to exchange new information on novel applications, model theory, measurement processes, and software related to receptor models used in the management of air resources. Forty-six papers were presented in eight sessions which addressed: 1) PM10 source apportionment for state implementation plan development; 2) measurements and source apportionment of pollutants other than PM10; 3) the requirements and availability of receptor model data bases; 4) the implementation of receptor models and their input data bases on microcomputers; 5) source characterization methods and results; and 6) model evaluation and development.},
	number = {4},
	urldate = {2017-02-21},
	journal = {JAPCA},
	author = {Watson, John G. and Chow, Judith C. and Mathai, C. V.},
	month = apr,
	year = {1989},
	pages = {419--426},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/2R9NEC8J/Watson et al. - 1989 - Receptor Models in Air Resources Management A Sum.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/WRHI3W6V/08940630.1989.html:text/html}
}

@article{thurston_quantitative_1985,
	title = {A quantitative assessment of source contributions to inhalable particulate matter pollution in metropolitan {Boston}},
	volume = {19},
	issn = {0004-6981},
	url = {http://www.sciencedirect.com/science/article/pii/0004698185901325},
	doi = {10.1016/0004-6981(85)90132-5},
	abstract = {In this paper, source apportionment techniques are employed to identify and quantify the major particle pollution source classes affecting a monitoring site in metropolitan Boston, MA. A Principal Component Analysis (PCA) of paniculate elemental data allows the estimation of mass contributions for five fine mass panicle source classes (soil, motor vehicle, coal related, oil and salt aerosols), and six coarse panicle source classes (soil, motor vehicle, refuse incineration, residual oil, salt and sulfate aerosols). Also derived are the elemental characteristics of those source aerosols and their contributions to the total recorded elemental concentrations (i.e. an elemental mass balance). These are estimated by applying a new approach to apportioning mass among various PCA source components: the calculation of Absolute Principal Component Scores, and the subsequent regression of daily mass and elemental concentrations on these scores. One advantage of the PCA source apportionment approach developed is that it allows the estimation of mass and source particle characteristics for an unconventional source category: transported (coal combustion related) aerosols. This particle class is estimated to represent a major portion of the aerosol mass, averaging roughly 40 per cent of the fine mass and 25 per cent of the inhalable particle mass at the Watertown, MA site. About 45 per cent of the fine particle sulfur is ascribed to this one component, with only 20 per cent assigned to pollution from local sources. The composition of the coal related aerosol at this site is found to be quite different from particles measured in the stacks of coal-fired power plants. Sulfates were estimated to comprise a much larger percentage of the ambient coal related aerosol than has been measured in stacks, while crustal element percentages were much reduced. This is thought to be due to primary panicle deposition and secondary aerosol accretion experienced during transport. Overall, the results indicate that the application of further emission controls to local point sources of particles would have less influence on fine aerosol and sulfate concentrations than would the control of more distant emissions causing aerosols transported into the Boston vicinity.},
	number = {1},
	urldate = {2017-02-21},
	journal = {Atmospheric Environment (1967)},
	author = {Thurston, George D. and Spengler, John D.},
	month = jan,
	year = {1985},
	keywords = {principal component analysis, source apportionment, Aerosols, dichotomous sampler, elemental composition, inhalable particles, particles, pollution sources, pollution transport, selenium, sulfur, trace metals},
	pages = {9--25},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/JM4WQJS4/0004698185901325.html:text/html}
}

@article{paatero_multilinear_1999,
	title = {The {Multilinear} {Engine}: {A} {Table}-{Driven}, {Least} {Squares} {Program} for {Solving} {Multilinear} {Problems}, including the n-{Way} {Parallel} {Factor} {Analysis} {Model}},
	volume = {8},
	issn = {1061-8600},
	shorttitle = {The {Multilinear} {Engine}},
	url = {http://www.jstor.org/stable/1390831},
	doi = {10.2307/1390831},
	abstract = {A technique for fitting multilinear and quasi-multilinear mathematical expressions or models to two-, three-, and many-dimensional data arrays is described. Principal component analysis and three-way PARAFAC factor analysis are examples of bilinear and trilinear least squares fit. This work presents a technique for specifying the problem in a structured way so that one program (the Multilinear Engine) may be used for solving widely different multilinear problems. The multilinear equations to be solved are specified as a large table of integer code values. The end user creates this table by using a small preprocessing program. For each different case, an individual structure table is needed. The solution is computed by using the conjugate gradient algorithm. Non-negativity constraints are implemented by using the well-known technique of preconditioning in opposite way for slowing down changes of variables that are about to become negative. The iteration converges to a minimum that may be local or global. Local uniqueness of the solution may be determined by inspecting the singular values of the Jacobian matrix. A global solution may be searched for by starting the iteration from different pseudorandom starting points. Application examples are discussed--for example, n-way PARAFAC, PARAFAC2, Linked mode PARAFAC, blind deconvolution, and nonstandard variants of these.},
	number = {4},
	urldate = {2017-02-21},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Paatero, Pentti},
	year = {1999},
	pages = {854--888}
}

@article{paatero_least_1997,
	title = {Least squares formulation of robust non-negative factor analysis},
	volume = {37},
	issn = {0169-7439},
	url = {http://www.sciencedirect.com/science/article/pii/S0169743996000445},
	doi = {10.1016/S0169-7439(96)00044-5},
	abstract = {Positive matrix factorization (PMF) is a recently published factor analytic technique where the left and right factor matrices (corresponding to scores and loadings) are constrained to non-negative values. The PMF model is a weighted least squares fit, weights based on the known standard deviations of the elements of the data matrix. The following aspects of PMF are discussed in this work: (1) Robust factorization (based on the Huber influence function) is achieved by iterative reweighting of individual data values. This appears especially useful if individual data values may be in error. (2) Desired rotations may be obtained automatically with the help of suitably chosen regularization terms. (3) The algorithms for PMF are discussed. A synthetic spectroscopic example is shown, demonstrating both the robust processing and the automatic rotations.},
	number = {1},
	urldate = {2017-02-21},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Paatero, Pentti},
	month = may,
	year = {1997},
	keywords = {Huber influence function, Iterative reweighting, positive matrix factorization},
	pages = {23--35},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/8WHRGQC9/Paatero - 1997 - Least squares formulation of robust non-negative f.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/8642UKNG/S0169743996000445.html:text/html}
}

@article{kang_interlab_2014,
	title = {Interlab {Comparison} of {Elemental} {Analysis} for {Low} {Ambient} {Urban} {PM}2.5 {Levels}},
	volume = {48},
	issn = {0013-936X},
	url = {http://dx.doi.org/10.1021/es502989j},
	doi = {10.1021/es502989j},
	abstract = {There is growing concern about the accuracy of trace elemental analysis of ambient particulate matter (PM) samples. This has become important because ambient PM concentrations have decreased over the years, and the lower filter loadings result in difficulties in accurate analysis. The performance of energy-dispersive X-ray reflectance spectrometry was evaluated at Harvard School of Public Health using several methodologies, including intercomparison between two other laboratories. In reanalysis of standard films as unknown samples following calibration, the HSPH ED XRF measurements represented good performance: 2\% errors in precision and 4\% errors in accuracy. Replicate analysis of ambient air filters with low PM2.5 levels indicated that S, K, Fe, and Ca showed excellent reproducibility, most other quantifiable elements were below 15\% error, and the elements with larger percent of flagged measurements had less in precision. Results from the interlaboratory comparison demonstrated that most quantifiable elements, except Na and Al, were quite comparable for the three laboratories. Na performance could be validated from the stoichiometry of Na to Cl of indoor PM2.5 filter samples.},
	number = {20},
	urldate = {2017-02-21},
	journal = {Environmental Science \& Technology},
	author = {Kang, Choong-Min and Achilleos, Souzana and Lawrence, Joy and Wolfson, Jack M. and Koutrakis, Petros},
	month = oct,
	year = {2014},
	pages = {12150--12156},
	file = {ACS Full Text PDF w/ Links:/home/jeremiah/Zotero/storage/R539JK5A/Kang et al. - 2014 - Interlab Comparison of Elemental Analysis for Low .pdf:application/pdf;ACS Full Text Snapshot:/home/jeremiah/Zotero/storage/N3QMMDZB/es502989j.html:text/html}
}

@article{masri_composition_2015,
	title = {Composition and sources of fine and coarse particles collected during 2002-2010 in {Boston}, {MA}},
	volume = {65},
	issn = {1096-2247},
	doi = {10.1080/10962247.2014.982307},
	abstract = {Identifying the sources, composition, and temporal variability of fine (PM2.5) and coarse (PM2.5-10) particles is a crucial component in understanding particulate matter (PM) toxicity and establishing proper PM regulations. In this study, a Harvard Impactor was used to collect daily integrated fine and coarse particle samples every third day for 9 years at a single site in Boston, MA. In total, 1,960 filters were analyzed for elements, black carbon (BC), and total PM mass. Positive Matrix Factorization (PMF) was used to identify source types and quantify their contributions to ambient PM2.5 and PM2.5-10. BC and 17 elements were identified as the main constituents in our samples. Results showed that BC, S, and Pb were associated exclusively with the fine particle mode, while 84\% of V and 79\% of Ni were associated with this mode. Elements mostly found in the coarse mode, over 80\%, included Ca, Mn (road dust), and Cl (sea salt). PMF identified six source types for PM2.5 and three source types for PM2.5-10. Source types for PM2.5 included regional pollution, motor vehicles, sea salt, crustal/road dust, oil combustion, and wood burning. Regional pollution contributed the most, accounting for 48\% of total PM2.5 mass, followed by motor vehicles (21\%) and wood burning (19\%). Source types for PM2.5-10 included crustal/road dust (62\%), motor vehicles (22\%), and sea salt (16\%). A linear decrease in PM concentrations with time was observed for both fine (-5.2\%/yr) and coarse (-3.6\%/yr) particles. The fine-mode trend was mostly related to oil combustion and regional pollution contributions. Average PM2.5 concentrations peaked in summer (10.4 µg/m3), while PM2.5-10 concentrations were lower and demonstrated little seasonal variability. The findings of this study show that PM2.5 is decreasing more sharply than PM2.5-10 over time. This suggests the increasing importance of PM2.5-10 and traffic-related sources for PM exposure and future policies.
IMPLICATIONS: Although many studies have examined fine and coarse particle composition and sources, few studies have used concurrent measurements of these two fractions. Our analysis suggests that fine and coarse particles exhibit distinct compositions and sources. With better knowledge of the compositional and source differences between these two PM fractions, better decisions can be made about PM regulations. Further, such information is valuable in enabling epidemiologists to understand the ensuing health implications of PM exposure.},
	language = {eng},
	number = {3},
	journal = {Journal of the Air \& Waste Management Association (1995)},
	author = {Masri, Shahir and Kang, Choong-Min and Koutrakis, Petros},
	month = mar,
	year = {2015},
	pmid = {25947125},
	pmcid = {PMC4740916},
	keywords = {Particulate matter, Air Pollutants, Boston, Environmental Monitoring, Particle Size, Time Factors},
	pages = {287--297}
}

@unpublished{yger_wavelet_2010,
	title = {Wavelet {Kernel} {Learning}},
	url = {https://hal.archives-ouvertes.fr/hal-00510484},
	abstract = {This paper addresses the problem of optimal feature extraction from a wavelet representation. Our work aims at building features by selecting wavelet coefficients resulting from signal or image decomposition on a adapted wavelet basis. For this purpose, we jointly learn in a kernelized large-margin context the wavelet shape as well as the appropriate scale and translation of the wavelets, hence the name “wavelet kernel learning”. This problem is posed as a multiple kernel learning problem where the number of kernels can be very large. For solving such a problem, we introduce a novel multiple kernel learning algorithm based on active constraints methods. We furthermore propose some variants of this algorithm that can produce approximate solutions more efficiently. Empirical analysis show that our active constraint MKL algorithm achieves state-of-the art efficiency. When applied to wavelet kernel learning, our experimental results show that the approaches we propose are competitive with respect to the state of the art on Brain-Computer Interface and Brodatz texture datasets.},
	urldate = {2017-02-21},
	author = {Yger, Florian and Rakotomamonjy, Alain},
	month = aug,
	year = {2010},
	keywords = {multiple kernel learning, quadratic mirror filter, SVM, wavelet},
	annote = {working paper or preprint},
	file = {HAL PDF Full Text:/home/jeremiah/Zotero/storage/483C73U2/Yger and Rakotomamonjy - 2010 - Wavelet Kernel Learning.pdf:application/pdf}
}

@article{europe_health_2007,
	title = {Health relevance of particulate matter from various sources : report on a {WHO} workshop, {Bonn}, {Germany} 26-27 {March} 2007},
	shorttitle = {Health relevance of particulate matter from various sources},
	url = {http://www.who.int/iris/handle/10665/107846},
	abstract = {EUR/07/5067587},
	language = {en},
	urldate = {2017-02-20},
	author = {Europe, World Health Organization Regional Office for},
	year = {2007},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/B3VKRXJD/Europe - 2007 - Health relevance of particulate matter from variou.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/BNQUSBRC/107846.html:text/html}
}

@article{burnett_integrated_2014,
	title = {An integrated risk function for estimating the global burden of disease attributable to ambient fine particulate matter exposure},
	volume = {122},
	issn = {1552-9924},
	doi = {10.1289/ehp.1307049},
	abstract = {BACKGROUND: Estimating the burden of disease attributable to long-term exposure to fine particulate matter (PM2.5) in ambient air requires knowledge of both the shape and magnitude of the relative risk (RR) function. However, adequate direct evidence to identify the shape of the mortality RR functions at the high ambient concentrations observed in many places in the world is lacking.
OBJECTIVE: We developed RR functions over the entire global exposure range for causes of mortality in adults: ischemic heart disease (IHD), cerebrovascular disease (stroke), chronic obstructive pulmonary disease (COPD), and lung cancer (LC). We also developed RR functions for the incidence of acute lower respiratory infection (ALRI) that can be used to estimate mortality and lost-years of healthy life in children {\textless} 5 years of age.
METHODS: We fit an integrated exposure-response (IER) model by integrating available RR information from studies of ambient air pollution (AAP), second hand tobacco smoke, household solid cooking fuel, and active smoking (AS). AS exposures were converted to estimated annual PM2.5 exposure equivalents using inhaled doses of particle mass. We derived population attributable fractions (PAFs) for every country based on estimated worldwide ambient PM2.5 concentrations.
RESULTS: The IER model was a superior predictor of RR compared with seven other forms previously used in burden assessments. The percent PAF attributable to AAP exposure varied among countries from 2 to 41 for IHD, 1 to 43 for stroke, {\textless} 1 to 21 for COPD, {\textless} 1 to 25 for LC, and {\textless} 1 to 38 for ALRI.
CONCLUSIONS: We developed a fine particulate mass-based RR model that covered the global range of exposure by integrating RR information from different combustion types that generate emissions of particulate matter. The model can be updated as new RR information becomes available.},
	language = {eng},
	number = {4},
	journal = {Environmental Health Perspectives},
	author = {Burnett, Richard T. and Pope, C. Arden and Ezzati, Majid and Olives, Casey and Lim, Stephen S. and Mehta, Sumi and Shin, Hwashin H. and Singh, Gitanjali and Hubbell, Bryan and Brauer, Michael and Anderson, H. Ross and Smith, Kirk R. and Balmes, John R. and Bruce, Nigel G. and Kan, Haidong and Laden, Francine and Prüss-Ustün, Annette and Turner, Michelle C. and Gapstur, Susan M. and Diver, W. Ryan and Cohen, Aaron},
	month = apr,
	year = {2014},
	pmid = {24518036},
	pmcid = {PMC3984213},
	keywords = {Humans, Female, Male, Environmental Exposure, Particulate matter, Cost of Illness, Models, Theoretical},
	pages = {397--403}
}

@article{karagulian_contributions_2015,
	title = {Contributions to cities' ambient particulate matter ({PM}): {A} systematic review of local source contributions at global level},
	volume = {120},
	issn = {1352-2310},
	shorttitle = {Contributions to cities' ambient particulate matter ({PM})},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231015303320},
	doi = {10.1016/j.atmosenv.2015.08.087},
	abstract = {For reducing health impacts from air pollution, it is important to know the sources contributing to human exposure. This study systematically reviewed and analysed available source apportionment studies on particulate matter (of diameter of 10 and 2.5 microns, PM10 and PM2.5) performed in cities to estimate typical shares of the sources of pollution by country and by region. A database with city source apportionment records, estimated with the use of receptor models, was also developed and available at the website of the World Health Organization.
Systematic Scopus and Google searches were performed to retrieve city studies of source apportionment for particulate matter. Six source categories were defined. Country and regional averages of source apportionment were estimated based on city population weighting.
A total of 419 source apportionment records from studies conducted in cities of 51 countries were used to calculate regional averages of sources of ambient particulate matter. Based on the available information, globally 25\% of urban ambient air pollution from PM2.5 is contributed by traffic, 15\% by industrial activities, 20\% by domestic fuel burning, 22\% from unspecified sources of human origin, and 18\% from natural dust and salt. The available source apportionment records exhibit, however, important heterogeneities in assessed source categories and incompleteness in certain countries/regions.
Traffic is one important contributor to ambient PM in cities. To reduce air pollution in cities and the substantial disease burden it causes, solutions to sustainably reduce ambient PM from traffic, industrial activities and biomass burning should urgently be sought. However, further efforts are required to improve data availability and evaluation, and possibly to combine with other types of information in view of increasing usefulness for policy making.},
	urldate = {2017-02-20},
	journal = {Atmospheric Environment},
	author = {Karagulian, Federico and Belis, Claudio A. and Dora, Carlos Francisco C. and Prüss-Ustün, Annette M. and Bonjour, Sophie and Adair-Rohani, Heather and Amann, Markus},
	month = nov,
	year = {2015},
	keywords = {Particulate matter, source apportionment, PM10, PM2.5, Receptor models, Urban ambient PM},
	pages = {475--483},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/DM3ZD4K7/Karagulian et al. - 2015 - Contributions to cities' ambient particulate matte.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/RTQ9UWUK/S1352231015303320.html:text/html}
}

@article{pope_lung_2002,
	title = {Lung cancer, cardiopulmonary mortality, and long-term exposure to fine particulate air pollution},
	volume = {287},
	issn = {0098-7484},
	abstract = {CONTEXT: Associations have been found between day-to-day particulate air pollution and increased risk of various adverse health outcomes, including cardiopulmonary mortality. However, studies of health effects of long-term particulate air pollution have been less conclusive.
OBJECTIVE: To assess the relationship between long-term exposure to fine particulate air pollution and all-cause, lung cancer, and cardiopulmonary mortality.
DESIGN, SETTING, AND PARTICIPANTS: Vital status and cause of death data were collected by the American Cancer Society as part of the Cancer Prevention II study, an ongoing prospective mortality study, which enrolled approximately 1.2 million adults in 1982. Participants completed a questionnaire detailing individual risk factor data (age, sex, race, weight, height, smoking history, education, marital status, diet, alcohol consumption, and occupational exposures). The risk factor data for approximately 500 000 adults were linked with air pollution data for metropolitan areas throughout the United States and combined with vital status and cause of death data through December 31, 1998.
MAIN OUTCOME MEASURE: All-cause, lung cancer, and cardiopulmonary mortality.
RESULTS: Fine particulate and sulfur oxide--related pollution were associated with all-cause, lung cancer, and cardiopulmonary mortality. Each 10-microg/m(3) elevation in fine particulate air pollution was associated with approximately a 4\%, 6\%, and 8\% increased risk of all-cause, cardiopulmonary, and lung cancer mortality, respectively. Measures of coarse particle fraction and total suspended particles were not consistently associated with mortality.
CONCLUSION: Long-term exposure to combustion-related fine particulate air pollution is an important environmental risk factor for cardiopulmonary and lung cancer mortality.},
	language = {eng},
	number = {9},
	journal = {JAMA},
	author = {Pope, C. Arden and Burnett, Richard T. and Thun, Michael J. and Calle, Eugenia E. and Krewski, Daniel and Ito, Kazuhiko and Thurston, George D.},
	month = mar,
	year = {2002},
	pmid = {11879110},
	pmcid = {PMC4037163},
	keywords = {Humans, Female, Risk Factors, Lung Neoplasms, Male, air pollution, Air Pollutants, Particle Size, Adult, Cause of Death, Proportional Hazards Models, Pulmonary Heart Disease, United States, Urban Population},
	pages = {1132--1141}
}

@article{dockery_association_1993,
	title = {An association between air pollution and mortality in six {U}.{S}. cities},
	volume = {329},
	issn = {0028-4793},
	doi = {10.1056/NEJM199312093292401},
	abstract = {BACKGROUND: Recent studies have reported associations between particulate air pollution and daily mortality rates. Population-based, cross-sectional studies of metropolitan areas in the United States have also found associations between particulate air pollution and annual mortality rates, but these studies have been criticized, in part because they did not directly control for cigarette smoking and other health risks.
METHODS: In this prospective cohort study, we estimated the effects of air pollution on mortality, while controlling for individual risk factors. Survival analysis, including Cox proportional-hazards regression modeling, was conducted with data from a 14-to-16-year mortality follow-up of 8111 adults in six U.S. cities.
RESULTS: Mortality rates were most strongly associated with cigarette smoking. After adjusting for smoking and other risk factors, we observed statistically significant and robust associations between air pollution and mortality. The adjusted mortality-rate ratio for the most polluted of the cities as compared with the least polluted was 1.26 (95 percent confidence interval, 1.08 to 1.47). Air pollution was positively associated with death from lung cancer and cardiopulmonary disease but not with death from other causes considered together. Mortality was most strongly associated with air pollution with fine particulates, including sulfates.
CONCLUSIONS: Although the effects of other, unmeasured risk factors cannot be excluded with certainty, these results suggest that fine-particulate air pollution, or a more complex pollution mixture associated with fine particulate matter, contributes to excess mortality in certain U.S. cities.},
	language = {eng},
	number = {24},
	journal = {The New England Journal of Medicine},
	author = {Dockery, D. W. and Pope, C. A. and Xu, X. and Spengler, J. D. and Ware, J. H. and Fay, M. E. and Ferris, B. G. and Speizer, F. E.},
	month = dec,
	year = {1993},
	pmid = {8179653},
	keywords = {Humans, Female, Risk Factors, Male, Smoking, air pollution, Adult, Cause of Death, United States, Aged, Confidence Intervals, Follow-Up Studies, Middle Aged, Mortality, Prospective Studies, Survival Analysis, Urban Health},
	pages = {1753--1759}
}

@article{hopke_pm_2005,
	title = {{PM} source apportionment and health effects: 1. {Intercomparison} of source apportionment results},
	volume = {16},
	copyright = {© 2005 Nature Publishing Group},
	issn = {1559-0631},
	shorttitle = {{PM} source apportionment and health effects},
	url = {http://www.nature.com/jes/journal/v16/n3/full/7500458a.html},
	doi = {10.1038/sj.jea.7500458},
	abstract = {During the past three decades, receptor models have been used to identify and apportion ambient concentrations to sources. A number of groups are employing these methods to provide input into air quality management planning. A workshop has explored the use of resolved source contributions in health effects models. Multiple groups have analyzed particulate composition data sets from Washington, DC and Phoenix, AZ. Similar source profiles were extracted from these data sets by the investigators using different factor analysis methods. There was good agreement among the major resolved source types. Crustal (soil), sulfate, oil, and salt were the sources that were most unambiguously identified (generally highest correlation across the sites). Traffic and vegetative burning showed considerable variability among the results with variability in the ability of the methods to partition the motor vehicle contributions between gasoline and diesel vehicles. However, if the total motor vehicle contributions are estimated, good correspondence was obtained among the results. The source impacts were especially similar across various analyses for the larger mass contributors (e.g., in Washington, secondary sulfate SE=7\% and 11\% for traffic; in Phoenix, secondary sulfate SE=17\% and 7\% for traffic). Especially important for time-series health effects assessment, the source-specific impacts were found to be highly correlated across analysis methods/researchers for the major components (e.g., mean analysis to analysis correlation, r{\textgreater}0.9 for traffic and secondary sulfates in Phoenix and for traffic and secondary nitrates in Washington. The sulfate mean r value is {\textgreater}0.75 in Washington.). Overall, although these intercomparisons suggest areas where further research is needed (e.g., better division of traffic emissions between diesel and gasoline vehicles), they provide support the contention that PM2.5 mass source apportionment results are consistent across users and methods, and that today's source apportionment methods are robust enough for application to PM2.5 health effects assessments.},
	language = {en},
	number = {3},
	urldate = {2017-02-20},
	journal = {Journal of Exposure Science and Environmental Epidemiology},
	author = {Hopke, Philip K. and Ito, Kazuhiko and Mar, Therese and Christensen, William F. and Eatough, Delbert J. and Henry, Ronald C. and Kim, Eugene and Laden, Francine and Lall, Ramona and Larson, Timothy V. and Liu, Hao and Neas, Lucas and Pinto, Joseph and Stölzel, Matthias and Suh, Helen and Paatero, Pentti and Thurston, George D.},
	month = oct,
	year = {2005},
	keywords = {receptor modelling, source apportionment, positive matrix factorization, PM2.5, PMF, Unmix.},
	pages = {275--286},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/F34QEE2A/Hopke et al. - 2005 - PM source apportionment and health effects 1. Int.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/JUDGKRGG/7500458a.html:text/html}
}

@article{lanckriet_learning_2004,
	title = {Learning the {Kernel} {Matrix} with {Semidefinite} {Programming}},
	volume = {5},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v5/lanckriet04a.html},
	number = {Jan},
	urldate = {2017-02-18},
	journal = {Journal of Machine Learning Research},
	author = {Lanckriet, Gert R. G. and Cristianini, Nello and Bartlett, Peter and Ghaoui, Laurent El and Jordan, Michael I.},
	year = {2004},
	pages = {27--72},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/KKKKF6ZS/Lanckriet et al. - 2004 - Learning the Kernel Matrix with Semidefinite Progr.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/FSHXVGG3/lanckriet04a.html:text/html}
}

@article{zhuang_family_2011,
	title = {A {Family} of {Simple} {Non}-{Parametric} {Kernel} {Learning} {Algorithms}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v12/zhuang11a.html},
	number = {Apr},
	urldate = {2017-02-18},
	journal = {Journal of Machine Learning Research},
	author = {Zhuang, Jinfeng and Tsang, Ivor W. and Hoi, Steven C. H.},
	year = {2011},
	pages = {1313--1347},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/5JS2IXZ6/Zhuang et al. - 2011 - A Family of Simple Non-Parametric Kernel Learning .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/VW8RTH9M/zhuang11a.html:text/html}
}

@inproceedings{oliva_bayesian_2016,
	title = {Bayesian {Nonparametric} {Kernel}-{Learning}},
	url = {http://jmlr.org/proceedings/papers/v51/oliva16.html},
	urldate = {2017-02-18},
	author = {Oliva, Junier B. and Dubey, Avinava and Wilson, Andrew G. and Poczos, Barnabas and Schneider, Jeff and Xing, Eric P.},
	year = {2016},
	pages = {1078--1086},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/J2D3VVHH/Oliva et al. - 2016 - Bayesian Nonparametric Kernel-Learning.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/282W2ZA2/oliva16.html:text/html}
}

@incollection{seeger_bayesian_2000,
	title = {Bayesian {Model} {Selection} for {Support} {Vector} {Machines}, {Gaussian} {Processes} and {Other} {Kernel} {Classifiers}},
	url = {http://papers.nips.cc/paper/1722-bayesian-model-selection-for-support-vector-machines-gaussian-processes-and-other-kernel-classifiers.pdf},
	urldate = {2017-02-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 12},
	publisher = {MIT Press},
	author = {Seeger, Matthias},
	editor = {Solla, S. A. and Leen, T. K. and Müller, K.},
	year = {2000},
	pages = {603--609},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/IPFFEK9W/Seeger - 2000 - Bayesian Model Selection for Support Vector Machin.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/KKT56JZD/1722-bayesian-model-selection-for-support-vector-machines-gaussian-processes-and-other-kernel-c.html:text/html}
}

@article{sollich_bayesian_2002,
	title = {Bayesian {Methods} for {Support} {Vector} {Machines}: {Evidence} and {Predictive} {Class} {Probabilities}},
	volume = {46},
	issn = {0885-6125, 1573-0565},
	shorttitle = {Bayesian {Methods} for {Support} {Vector} {Machines}},
	url = {http://link.springer.com/article/10.1023/A:1012489924661},
	doi = {10.1023/A:1012489924661},
	abstract = {I describe a framework for interpreting Support Vector Machines (SVMs) as maximum a posteriori (MAP) solutions to inference problems with Gaussian Process priors. This probabilistic interpretation can provide intuitive guidelines for choosing a ‘good’ SVM kernel. Beyond this, it allows Bayesian methods to be used for tackling two of the outstanding challenges in SVM classification: how to tune hyperparameters—the misclassification penalty C, and any parameters specifying the ernel—and how to obtain predictive class probabilities rather than the conventional deterministic class label predictions. Hyperparameters can be set by maximizing the evidence; I explain how the latter can be defined and properly normalized. Both analytical approximations and numerical methods (Monte Carlo chaining) for estimating the evidence are discussed. I also compare different methods of estimating class probabilities, ranging from simple evaluation at the MAP or at the posterior average to full averaging over the posterior. A simple toy application illustrates the various concepts and techniques.},
	language = {en},
	number = {1-3},
	urldate = {2017-02-18},
	journal = {Machine Learning},
	author = {Sollich, Peter},
	month = jan,
	year = {2002},
	pages = {21--52},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/WSXWT6GB/Sollich - 2002 - Bayesian Methods for Support Vector Machines Evid.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/9HPSDHHR/A1012489924661.html:text/html}
}

@misc{noauthor_bayesian_nodate,
	title = {Bayesian {Model} {Selection} for {Support} {Vector} {Machines}, {Gaussian} {Processes} and {Other} {Kernel} {Classifiers}},
	url = {https://www.researchgate.net/publication/49459307_Bayesian_Model_Selection_for_Support_Vector_Machines_Gaussian_Processes_and_Other_Kernel_Classifiers},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	urldate = {2017-02-18},
	file = {Snapshot:/home/jeremiah/Zotero/storage/NU8RQBEI/49459307_Bayesian_Model_Selection_for_Support_Vector_Machines_Gaussian_Processes_and_Other_Kerne.pdf:application/pdf}
}

@article{liu_semiparametric_2007,
	title = {Semiparametric {Regression} of {Multidimensional} {Genetic} {Pathway} {Data}: {Least}-{Squares} {Kernel} {Machines} and {Linear} {Mixed} {Models}},
	volume = {63},
	issn = {0006-341X},
	shorttitle = {Semiparametric {Regression} of {Multidimensional} {Genetic} {Pathway} {Data}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2665800/},
	doi = {10.1111/j.1541-0420.2007.00799.x},
	abstract = {We consider a semiparametric regression model that relates a normal outcome to covariates and a genetic pathway, where the covariate effects are modeled parametrically and the pathway effect of multiple gene expressions is modeled parametrically or nonparametrically using least-squares kernel machines (LSKMs). This unified framework allows a flexible function for the joint effect of multiple genes within a pathway by specifying a kernel function and allows for the possibility that each gene expression effect might be nonlinear and the genes within the same pathway are likely to interact with each other in a complicated way. This semiparametric model also makes it possible to test for the overall genetic pathway effect. We show that the LSKM semiparametric regression can be formulated using a linear mixed model. Estimation and inference hence can proceed within the linear mixed model framework using standard mixed model software. Both the regression coefficients of the covariate effects and the LSKM estimator of the genetic pathway effect can be obtained using the best linear unbiased predictor in the corresponding linear mixed model formulation. The smoothing parameter and the kernel parameter can be estimated as variance components using restricted maximum likelihood. A score test is developed to test for the genetic pathway effect. Model/variable selection within the LSKM framework is discussed. The methods are illustrated using a prostate cancer data set and evaluated using simulations.},
	number = {4},
	urldate = {2017-02-18},
	journal = {Biometrics},
	author = {Liu, Dawei and Lin, Xihong and Ghosh, Debashis},
	month = dec,
	year = {2007},
	pmid = {18078480},
	pmcid = {PMC2665800},
	pages = {1079--1088},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/NFS4K5U3/Liu et al. - 2007 - Semiparametric Regression of Multidimensional Gene.pdf:application/pdf}
}

@book{wahba_spline_1990,
	title = {Spline {Models} for {Observational} {Data}},
	isbn = {978-0-89871-244-5},
	abstract = {This book serves well as an introduction into the more theoretical aspects of the use of spline models. It develops a theory and practice for the estimation of functions from noisy data on functionals. The simplest example is the estimation of a smooth curve, given noisy observations on a finite number of its values. Convergence properties, data based smoothing parameter selection, confidence intervals, and numerical methods are established which are appropriate to a number of problems within this framework. Methods for including side conditions and other prior information in solving ill posed inverse problems are provided. Data which involves samples of random variables with Gaussian, Poisson, binomial, and other distributions are treated in a unified optimization context. Experimental design questions, i.e., which functionals should be observed, are studied in a general context. Extensions to distributed parameter system identification problems are made by considering implicitly defined functionals.},
	language = {en},
	publisher = {SIAM},
	author = {Wahba, Grace},
	month = sep,
	year = {1990},
	note = {Google-Books-ID: ScRQJEETs0EC},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@book{scholkopf_learning_2002,
	title = {Learning with {Kernels}: {Support} {Vector} {Machines}, {Regularization}, {Optimization}, and {Beyond}},
	isbn = {978-0-262-19475-4},
	shorttitle = {Learning with {Kernels}},
	abstract = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
	language = {en},
	publisher = {MIT Press},
	author = {Schölkopf, Bernhard and Smola, Alexander J.},
	month = jan,
	year = {2002},
	note = {Google-Books-ID: y8ORL3DWt4sC},
	keywords = {Computers / Intelligence (AI) \& Semantics, Computers / Programming / General}
}

@book{rasmussen_gaussian_2006,
	title = {Gaussian {Processes} for {Machine} {Learning}},
	isbn = {978-0-262-18253-9},
	abstract = {Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
	language = {en},
	publisher = {University Press Group Limited},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	month = jan,
	year = {2006},
	note = {Google-Books-ID: vWtwQgAACAAJ},
	keywords = {Computers / Computer Science, Computers / Machine Theory}
}

@article{aronszajn_theory_1950,
	title = {Theory of reproducing kernels},
	volume = {68},
	issn = {0002-9947, 1088-6850},
	url = {http://www.ams.org/tran/1950-068-03/S0002-9947-1950-0051437-7/},
	doi = {10.1090/S0002-9947-1950-0051437-7},
	number = {3},
	urldate = {2017-02-15},
	journal = {Transactions of the American Mathematical Society},
	author = {Aronszajn, N.},
	year = {1950},
	pages = {337--404},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/THJ5T4BM/Aronszajn - 1950 - Theory of reproducing kernels.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/A4STUFCE/S0002-9947-1950-0051437-7.html:text/html}
}

@book{reed_i:_1981,
	title = {I: {Functional} {Analysis}},
	isbn = {978-0-08-057048-8},
	shorttitle = {I},
	abstract = {This book is the first of a multivolume series devoted to an exposition of functional analysis methods in modern mathematical physics. It describes the fundamental principles of functional analysis and is essentially self-contained, although there are occasional references to later volumes. We have included a few applications when we thought that they would provide motivation for the reader. Later volumes describe various advanced topics in functional analysis and give numerous applications in classical physics, modern physics, and partial differential equations.},
	language = {en},
	publisher = {Academic Press},
	author = {Reed, Michael and Simon, Barry},
	month = feb,
	year = {1981},
	note = {Google-Books-ID: rpFTTjxOYpsC},
	keywords = {Mathematics / Functional Analysis}
}

@inproceedings{scholkopf_generalized_2001,
	title = {A {Generalized} {Representer} {Theorem}},
	url = {http://link.springer.com/chapter/10.1007/3-540-44581-1_27},
	abstract = {Wahba’s classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
	language = {en},
	urldate = {2017-02-15},
	booktitle = {Computational {Learning} {Theory}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Schölkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
	month = jul,
	year = {2001},
	doi = {10.1007/3-540-44581-1_27},
	pages = {416--426},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/D94DVQ3W/Schölkopf et al. - 2001 - A Generalized Representer Theorem.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/6FUHRABA/3-540-44581-1_27.html:text/html}
}

@incollection{kloft_efficient_2009,
	title = {Efficient and {Accurate} {Lp}-{Norm} {Multiple} {Kernel} {Learning}},
	url = {http://papers.nips.cc/paper/3675-efficient-and-accurate-lp-norm-multiple-kernel-learning.pdf},
	urldate = {2017-02-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Kloft, Marius and Brefeld, Ulf and Laskov, Pavel and Müller, Klaus-Robert and Zien, Alexander and Sonnenburg, Sören},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {997--1005},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/V9EXNRC4/Kloft et al. - 2009 - Efficient and Accurate Lp-Norm Multiple Kernel Lea.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/G8JXNQK8/3675-efficient-and-accurate-lp-norm-multiple-kernel-learning.html:text/html}
}

@article{brochu_tutorial_2010,
	title = {A {Tutorial} on {Bayesian} {Optimization} of {Expensive} {Cost} {Functions}, with {Application} to {Active} {User} {Modeling} and {Hierarchical} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1012.2599},
	abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
	urldate = {2017-02-14},
	journal = {arXiv:1012.2599 [cs]},
	author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
	month = dec,
	year = {2010},
	note = {arXiv: 1012.2599},
	keywords = {Computer Science - Learning, G.1.6, G.3, I.2.6},
	file = {arXiv\:1012.2599 PDF:/home/jeremiah/Zotero/storage/U6ZPIM38/Brochu et al. - 2010 - A Tutorial on Bayesian Optimization of Expensive C.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/7IIRJPJT/1012.html:text/html}
}

@article{bachoc_cross_2013,
	title = {Cross {Validation} and {Maximum} {Likelihood} estimations of hyper-parameters of {Gaussian} processes with model misspecification},
	url = {http://arxiv.org/abs/1301.4320},
	abstract = {The Maximum Likelihood (ML) and Cross Validation (CV) methods for estimating covariance hyper-parameters are compared, in the context of Kriging with a misspecified covariance structure. A two-step approach is used. First, the case of the estimation of a single variance hyper-parameter is addressed, for which the fixed correlation function is misspecified. A predictive variance based quality criterion is introduced and a closed-form expression of this criterion is derived. It is shown that when the correlation function is misspecified, the CV does better compared to ML, while ML is optimal when the model is well-specified. In the second step, the results of the first step are extended to the case when the hyper-parameters of the correlation function are also estimated from data.},
	urldate = {2017-02-05},
	journal = {arXiv:1301.4320 [math, stat]},
	author = {Bachoc, François},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.4320},
	keywords = {Mathematics - Statistics Theory},
	annote = {Comment: A supplementary material (pdf) is available in the arXiv sources},
	file = {arXiv\:1301.4320 PDF:/home/jeremiah/Zotero/storage/UN78WBVE/Bachoc - 2013 - Cross Validation and Maximum Likelihood estimation.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/TRJIEZNW/1301.html:text/html}
}

@inproceedings{stein_bound_1972,
	title = {A bound for the error in the normal approximation to the distribution of a sum of dependent random variables},
	url = {http://projecteuclid.org/euclid.bsmsp/1200514239},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-02-04},
	publisher = {The Regents of the University of California},
	author = {Stein, Charles},
	year = {1972},
	file = {Snapshot:/home/jeremiah/Zotero/storage/A4J9HW4D/1200514239.html:text/html}
}

@incollection{cawley_estimating_2006,
	title = {Estimating {Predictive} {Variances} with {Kernel} {Ridge} {Regression}},
	url = {http://link.springer.com/chapter/10.1007/11736790_5},
	abstract = {In many regression tasks, in addition to an accurate estimate of the conditional mean of the target distribution, an indication of the predictive uncertainty is also required. There are two principal sources of this uncertainty: the noise process contaminating the data and the uncertainty in estimating the model parameters based on a limited sample of training data. Both of them can be summarised in the predictive variance which can then be used to give confidence intervals. In this paper, we present various schemes for providing predictive variances for kernel ridge regression, especially in the case of a heteroscedastic regression, where the variance of the noise process contaminating the data is a smooth function of the explanatory variables. The use of leave-one-out cross-validation is shown to eliminate the bias inherent in estimates of the predictive variance. Results obtained on all three regression tasks comprising the predictive uncertainty challenge demonstrate the value of this approach.},
	language = {en},
	urldate = {2017-02-03},
	booktitle = {Machine {Learning} {Challenges}. {Evaluating} {Predictive} {Uncertainty}, {Visual} {Object} {Classification}, and {Recognising} {Tectual} {Entailment}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Cawley, Gavin C. and Talbot, Nicola L. C. and Chapelle, Olivier},
	year = {2006},
	doi = {10.1007/11736790_5},
	pages = {56--77},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/6GV4NFAN/Cawley et al. - 2006 - Estimating Predictive Variances with Kernel Ridge .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/IFQMH8X8/11736790_5.html:text/html}
}

@article{meijer_efficient_2010,
	title = {Efficient approximate leave-one-out cross-validation for ridge and lasso},
	url = {http://repository.tudelft.nl/islandora/object/uuid%3Ad9b5456d-722a-401d-9f1a-c530c46d6491},
	abstract = {In this thesis an approximation method is discussed that provides similar results to leave-one-out cross-validation but is less time-consuming. By means of this approximation method, estimating the optimal values of ridge and lasso parameters will take less time and carrying out (an approximated version of) double LOOCV will become practically feasible. The method can be used in generalized linear models as well as in Cox' proportional hazards model. In order to show its usefulness, the method is tested on several microarray data sets.},
	urldate = {2017-02-03},
	author = {Meijer, R. J.},
	year = {2010},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/KPIVNI43/Meijer - 2010 - Efficient approximate leave-one-out cross-validati.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/88TQVGG6/uuidd9b5456d-722a-401d-9f1a-c530c46d6491.html:text/html}
}

@article{meijer_efficient_2013,
	title = {Efficient approximate k-fold and leave-one-out cross-validation for ridge regression},
	volume = {55},
	issn = {1521-4036},
	doi = {10.1002/bimj.201200088},
	abstract = {In model building and model evaluation, cross-validation is a frequently used resampling method. Unfortunately, this method can be quite time consuming. In this article, we discuss an approximation method that is much faster and can be used in generalized linear models and Cox' proportional hazards model with a ridge penalty term. Our approximation method is based on a Taylor expansion around the estimate of the full model. In this way, all cross-validated estimates are approximated without refitting the model. The tuning parameter can now be chosen based on these approximations and can be optimized in less time. The method is most accurate when approximating leave-one-out cross-validation results for large data sets which is originally the most computationally demanding situation. In order to demonstrate the method's performance, it will be applied to several microarray data sets. An R package penalized, which implements the method, is available on CRAN.},
	language = {eng},
	number = {2},
	journal = {Biometrical Journal. Biometrische Zeitschrift},
	author = {Meijer, Rosa J. and Goeman, Jelle J.},
	month = mar,
	year = {2013},
	pmid = {23348970},
	keywords = {Models, Statistical, Linear Models, Regression analysis, Likelihood Functions, Reproducibility of Results, Proportional Hazards Models},
	pages = {141--155}
}

@article{griffiths_indian_2011,
	title = {The {Indian} {Buffet} {Process}: {An} {Introduction} and {Review}},
	volume = {12},
	issn = {1532-4435},
	shorttitle = {The {Indian} {Buffet} {Process}},
	url = {http://dl.acm.org/citation.cfm?id=1953048.2021039},
	abstract = {The Indian buffet process is a stochastic process defining a probability distribution over equivalence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent feature model. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.},
	urldate = {2017-01-31},
	journal = {J. Mach. Learn. Res.},
	author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
	month = jul,
	year = {2011},
	pages = {1185--1224}
}

@inproceedings{teh_stick-breaking_2007,
	title = {Stick-breaking {Construction} for the {Indian} {Buffet} {Process}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/AISTATS07_TehGG},
	urldate = {2017-01-31},
	author = {Teh, Yee W. and Görür, Dilan and Ghahramani, Zoubin},
	year = {2007},
	pages = {556--563},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/RHPIWJXK/Teh et al. - 2007 - Stick-breaking Construction for the Indian Buffet .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/UF3N5S98/AISTATS07_TehGG.html:text/html}
}

@article{buntine_bayesian_2010,
	title = {A {Bayesian} {View} of the {Poisson}-{Dirichlet} {Process}},
	url = {http://arxiv.org/abs/1007.0296},
	abstract = {The two parameter Poisson-Dirichlet Process (PDP), a generalisation of the Dirichlet Process, is increasingly being used for probabilistic modelling in discrete areas such as language technology, bioinformatics, and image analysis. There is a rich literature about the PDP and its derivative distributions such as the Chinese Restaurant Process (CRP). This article reviews some of the basic theory and then the major results needed for Bayesian modelling of discrete problems including details of priors, posteriors and computation. The PDP allows one to build distributions over countable partitions. The PDP has two other remarkable properties: first it is partially conjugate to itself, which allows one to build hierarchies of PDPs, and second using a marginalised relative the CRP, one gets fragmentation and clustering properties that lets one layer partitions to build trees. This article presents the basic theory for understanding the notion of partitions and distributions over them, the PDP and the CRP, and the important properties of conjugacy, fragmentation and clustering, as well as some key related properties such as consistency and convergence. This article also presents a Bayesian interpretation of the Poisson-Dirichlet process based on an improper and infinite dimensional Dirichlet distribution. This means we can understand the process as just another Dirichlet and thus all its sampling properties emerge naturally. The theory of PDPs is usually presented for continuous distributions (more generally referred to as non-atomic distributions), however, when applied to discrete distributions its remarkable conjugacy property emerges. This context and basic results are also presented, as well as techniques for computing the second order Stirling numbers that occur in the posteriors for discrete distributions.},
	urldate = {2017-01-31},
	journal = {arXiv:1007.0296 [cs, math, stat]},
	author = {Buntine, Wray and Hutter, Marcus},
	month = jul,
	year = {2010},
	note = {arXiv: 1007.0296},
	keywords = {Computer Science - Learning, Mathematics - Statistics Theory, Mathematics - Probability},
	annote = {Comment: 50 LaTeX pages, 10 figures, 3 tables, 1 algorithm},
	file = {arXiv\:1007.0296 PDF:/home/jeremiah/Zotero/storage/QDV35GI6/Buntine and Hutter - 2010 - A Bayesian View of the Poisson-Dirichlet Process.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MIHK6FIQ/1007.html:text/html}
}

@article{hernandez-lobato_probabilistic_2015,
	title = {Probabilistic {Backpropagation} for {Scalable} {Learning} of {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.05336},
	abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
	urldate = {2017-01-31},
	journal = {arXiv:1502.05336 [stat]},
	author = {Hernández-Lobato, José Miguel and Adams, Ryan P.},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05336},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1502.05336 PDF:/home/jeremiah/Zotero/storage/7NQK9HIQ/Hernández-Lobato and Adams - 2015 - Probabilistic Backpropagation for Scalable Learnin.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/FER23978/1502.html:text/html}
}

@article{chipman_bayesian_1998,
	title = {Bayesian {CART} {Model} {Search}},
	volume = {93},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1998.10473750},
	doi = {10.1080/01621459.1998.10473750},
	abstract = {In this article we put forward a Bayesian approach for finding classification and regression tree (CART) models. The two basic components of this approach consist of prior specification and stochastic search. The basic idea is to have the prior induce a posterior distribution that will guide the stochastic search toward more promising CART models. As the search proceeds, such models can then be selected with a variety of criteria, such as posterior probability, marginal likelihood, residual sum of squares or misclassification rates. Examples are used to illustrate the potential superiority of this approach over alternative methods.},
	number = {443},
	urldate = {2017-01-27},
	journal = {Journal of the American Statistical Association},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	month = sep,
	year = {1998},
	pages = {935--948},
	file = {Snapshot:/home/jeremiah/Zotero/storage/JEVG7V9T/01621459.1998.html:text/html}
}

@article{zhou_gamma_2015,
	title = {Gamma {Belief} {Networks}},
	url = {http://arxiv.org/abs/1512.03081},
	abstract = {To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer.},
	urldate = {2017-01-27},
	journal = {arXiv:1512.03081 [stat]},
	author = {Zhou, Mingyuan and Cong, Yulai and Chen, Bo},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03081},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: 44 pages, 24 figures},
	file = {arXiv\:1512.03081 PDF:/home/jeremiah/Zotero/storage/CECS8MDZ/Zhou et al. - 2015 - Gamma Belief Networks.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/GZE3V3DM/1512.html:text/html}
}

@incollection{zhou_poisson_2015,
	title = {The {Poisson} {Gamma} {Belief} {Network}},
	url = {http://papers.nips.cc/paper/5645-the-poisson-gamma-belief-network.pdf},
	urldate = {2017-01-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Zhou, Mingyuan and Cong, Yulai and Chen, Bo},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {3043--3051},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/BZKHN3BH/Zhou et al. - 2015 - The Poisson Gamma Belief Network.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/9HTAN75E/5645-the-poisson-gamma-belief-network.html:text/html}
}

@incollection{roy_mondrian_2009,
	title = {The {Mondrian} {Process}},
	url = {http://papers.nips.cc/paper/3622-the-mondrian-process.pdf},
	urldate = {2017-01-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 21},
	publisher = {Curran Associates, Inc.},
	author = {Roy, Daniel M and Teh, Yee W.},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
	pages = {1377--1384},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/U7GGPW7A/Roy and Teh - 2009 - The Mondrian Process.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/8KH3KQID/3622-the-mondrian-process.html:text/html}
}

@article{snoek_practical_2012,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	url = {http://arxiv.org/abs/1206.2944},
	abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
	urldate = {2017-01-27},
	journal = {arXiv:1206.2944 [cs, stat]},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.2944},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1206.2944 PDF:/home/jeremiah/Zotero/storage/RNR542UI/Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/9IEMXBWB/1206.html:text/html}
}

@article{srinivas_learning_2015,
	title = {Learning {Neural} {Network} {Architectures} using {Backpropagation}},
	url = {http://arxiv.org/abs/1511.05497},
	abstract = {Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.},
	urldate = {2017-01-27},
	journal = {arXiv:1511.05497 [cs]},
	author = {Srinivas, Suraj and Babu, R. Venkatesh},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: BMVC 2016 ; Title modified from 'Learning the Architecture of Deep Neural Networks'},
	file = {arXiv\:1511.05497 PDF:/home/jeremiah/Zotero/storage/932U3XQZ/Srinivas and Babu - 2015 - Learning Neural Network Architectures using Backpr.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/EKNBQV8Z/1511.html:text/html}
}

@article{zoph_neural_2016,
	title = {Neural {Architecture} {Search} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.01578},
	abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.84, which is only 0.1 percent worse and 1.2x faster than the current state-of-the-art model. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art.},
	urldate = {2017-01-27},
	journal = {arXiv:1611.01578 [cs]},
	author = {Zoph, Barret and Le, Quoc V.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01578},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1611.01578 PDF:/home/jeremiah/Zotero/storage/734VA5VW/Zoph and Le - 2016 - Neural Architecture Search with Reinforcement Lear.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/AWAJHC3U/1611.html:text/html}
}

@article{kingston_bayesian_2008,
	title = {Bayesian model selection applied to artificial neural networks used for water resources modeling},
	volume = {44},
	issn = {1944-7973},
	url = {http://onlinelibrary.wiley.com/doi/10.1029/2007WR006155/abstract},
	doi = {10.1029/2007WR006155},
	abstract = {Artificial neural networks (ANNs) have proven to be extremely valuable tools in the field of water resources engineering. However, one of the most difficult tasks in developing an ANN is determining the optimum level of complexity required to model a given problem, as there is no formal systematic model selection method. This paper presents a Bayesian model selection (BMS) method for ANNs that provides an objective approach for comparing models of varying complexity in order to select the most appropriate ANN structure. The approach uses Markov Chain Monte Carlo posterior simulations to estimate the evidence in favor of competing models and, in this study, three known methods for doing this are compared in terms of their suitability for being incorporated into the proposed BMS framework for ANNs. However, it is acknowledged that it can be particularly difficult to accurately estimate the evidence of ANN models. Therefore, the proposed BMS approach for ANNs incorporates a further check of the evidence results by inspecting the marginal posterior distributions of the hidden-to-output layer weights, which unambiguously indicate any redundancies in the hidden layer nodes. The fact that this check is available is one of the greatest advantages of the proposed approach over conventional model selection methods, which do not provide such a test and instead rely on the modeler's subjective choice of selection criterion. The advantages of a total Bayesian approach to ANN development, including training and model selection, are demonstrated on two synthetic and one real world water resources case study.},
	language = {en},
	number = {4},
	urldate = {2017-01-27},
	journal = {Water Resources Research},
	author = {Kingston, Greer B. and Maier, Holger R. and Lambert, Martin F.},
	month = apr,
	year = {2008},
	keywords = {0555 Neural networks, fuzzy logic, machine learning, 1871 Surface water quality, 1873 Uncertainty assessment, 1894 Instruments and techniques: modeling, artificial neural networks, Bayesian model selection, evidence estimation, Markov chain Monte Carlo, Uncertainty, water resources modeling},
	pages = {W04419},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/TIT7DSK9/Kingston et al. - 2008 - Bayesian model selection applied to artificial neu.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/2AC2AIPG/abstract.html:text/html}
}

@misc{noauthor_uncertainty_nodate,
	title = {Uncertainty in {Deep} {Learning} ({PhD} {Thesis}) {\textbar} {Yarin} {Gal} - {Blog} {\textbar} {Cambridge} {Machine} {Learning} {Group}},
	url = {http://mlg.eng.cam.ac.uk/yarin/blog_2248.html},
	abstract = {So I finally submitted my PhD thesis, collecting already published results on how to obtain uncertainty in deep learning, and lots of bits and pieces of new research I had lying around...},
	urldate = {2017-01-27},
	file = {Snapshot:/home/jeremiah/Zotero/storage/FCJKFX4W/blog_2248.html:text/html}
}

@misc{noauthor_uncertainty_nodate-1,
	title = {Uncertainty in {Deep} {Learning} - {Semantic} {Scholar}},
	url = {/paper/Uncertainty-in-Deep-Learning-Gal/55cd9e1bb7ce02cd2bb01b364e7b331fcc1ef2c7},
	abstract = {Semantic Scholar extracted view of \&quot;Uncertainty in Deep Learning\&quot; by Yarin Gal},
	urldate = {2017-01-27},
	file = {Snapshot:/home/jeremiah/Zotero/storage/HMPAZCQ4/55cd9e1bb7ce02cd2bb01b364e7b331fcc1ef2c7.html:text/html}
}

@article{murdock_blockout:_2015,
	title = {Blockout: {Dynamic} {Model} {Selection} for {Hierarchical} {Deep} {Networks}},
	shorttitle = {Blockout},
	url = {http://arxiv.org/abs/1512.05246},
	abstract = {Most deep architectures for image classification--even those that are trained to classify a large number of diverse categories--learn shared image representations with a single model. Intuitively, however, categories that are more similar should share more information than those that are very different. While hierarchical deep networks address this problem by learning separate features for subsets of related categories, current implementations require simplified models using fixed architectures specified via heuristic clustering methods. Instead, we propose Blockout, a method for regularization and model selection that simultaneously learns both the model architecture and parameters. A generalization of Dropout, our approach gives a novel parametrization of hierarchical architectures that allows for structure learning via back-propagation. To demonstrate its utility, we evaluate Blockout on the CIFAR and ImageNet datasets, demonstrating improved classification accuracy, better regularization performance, faster training, and the clear emergence of hierarchical network structures.},
	urldate = {2017-01-27},
	journal = {arXiv:1512.05246 [cs]},
	author = {Murdock, Calvin and Li, Zhen and Zhou, Howard and Duerig, Tom},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.05246},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	file = {arXiv\:1512.05246 PDF:/home/jeremiah/Zotero/storage/S7FW95F2/Murdock et al. - 2015 - Blockout Dynamic Model Selection for Hierarchical.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/TD5I3PBW/1512.html:text/html}
}

@techreport{steinwart_explicit_2005,
	title = {An explicit description of the reproducing kernel {Hilbert} spaces of {Gaussian} {RBF} kernels},
	abstract = {Although Gaussian RBF kernels are one of the most often used kernels in modern machine learning methods such as support vector machines (SVMs), little is known about the structure of their reproducing kernel Hilbert spaces (RKHSs). In this work we give two distinct explicit descriptions of the RKHSs corresponding to Gaussian RBF kernels and discuss some consequences. Furthermore, we present an orthonormal system for these spaces. Finally we discuss how our results can be used for analyzing the learning performance of SVMs.},
	institution = {IEEE Trans. Inform. Theory},
	author = {Steinwart, Ingo and Hush, Don and Scovel, Clint},
	year = {2005},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/REAH6SG5/Steinwart et al. - 2005 - An explicit description of the reproducing kernel .pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/85D2MKBA/summary.html:text/html}
}

@book{stein_use_2004,
	title = {Use of exchangeable pairs in the analysis of simulations},
	isbn = {978-0-940600-62-1},
	url = {http://projecteuclid.org/euclid.lnms/1196283797},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-01-25},
	publisher = {Institute of Mathematical Statistics},
	author = {Stein, Charles and Diaconis, Persi and Holmes, Susan and Reinert, Gesine},
	year = {2004},
	file = {Snapshot:/home/jeremiah/Zotero/storage/8FBKFURU/1196283797.html:text/html}
}

@article{gretton_kernel_2012,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v13/gretton12a.html},
	number = {Mar},
	urldate = {2017-01-24},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	year = {2012},
	pages = {723--773},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/MF5E5IKH/Gretton et al. - 2012 - A Kernel Two-Sample Test.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/MNTSS4AT/gretton12a.html:text/html}
}

@incollection{wood_particle_2007,
	title = {Particle {Filtering} for {Nonparametric} {Bayesian} {Matrix} {Factorization}},
	url = {http://papers.nips.cc/paper/3147-particle-filtering-for-nonparametric-bayesian-matrix-factorization.pdf},
	urldate = {2017-01-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Wood, Frank and Griffiths, Thomas L.},
	editor = {Schölkopf, P. B. and Platt, J. C. and Hoffman, T.},
	year = {2007},
	pages = {1513--1520},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/PZXHDSA7/Wood and Griffiths - 2007 - Particle Filtering for Nonparametric Bayesian Matr.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/T2BW22NX/3147-particle-filtering-for-nonparametric-bayesian-matrix-factorization.html:text/html}
}

@article{wu_kernel_2013,
	title = {Kernel {Machine} {SNP}-set {Testing} under {Multiple} {Candidate} {Kernels}},
	volume = {37},
	issn = {0741-0395},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3769109/},
	doi = {10.1002/gepi.21715},
	abstract = {Joint testing for the cumulative effect of multiple single nucleotide polymorphisms grouped on the basis of prior biological knowledge has become a popular and powerful strategy for the analysis of large scale genetic association studies. The kernel machine (KM) testing framework is a useful approach that has been proposed for testing associations between multiple genetic variants and many different types of complex traits by comparing pairwise similarity in phenotype between subjects to pairwise similarity in genotype, with similarity in genotype defined via a kernel function. An advantage of the KM framework is its flexibility: choosing different kernel functions allows for different assumptions concerning the underlying model and can allow for improved power. In practice, it is difficult to know which kernel to use a priori since this depends on the unknown underlying trait architecture and selecting the kernel which gives the lowest p-value can lead to inflated type I error. Therefore, we propose practical strategies for KM testing when multiple candidate kernels are present based on constructing composite kernels and based on efficient perturbation procedures. We demonstrate through simulations and real data applications that the procedures protect the type I error rate and can lead to substantially improved power over poor choices of kernels and only modest differences in power versus using the best candidate kernel.},
	number = {3},
	urldate = {2017-01-22},
	journal = {Genetic epidemiology},
	author = {Wu, Michael C. and Maity, Arnab and Lee, Seunggeun and Simmons, Elizabeth M. and Harmon, Quaker E. and Lin, Xinyi and Engel, Stephanie M. and Molldrem, Jeffrey J. and Armistead, Paul M.},
	month = apr,
	year = {2013},
	pmid = {23471868},
	pmcid = {PMC3769109},
	pages = {267--275},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/4JHXII3D/Wu et al. - 2013 - Kernel Machine SNP-set Testing under Multiple Cand.pdf:application/pdf}
}

@article{marceau_fast_2015,
	title = {A {Fast} {Multiple}-{Kernel} {Method} with {Applications} to {Detect} {Gene}-{Environment} {Interaction}},
	volume = {39},
	issn = {0741-0395},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4544636/},
	doi = {10.1002/gepi.21909},
	abstract = {Kernel machine (KM) models are a powerful tool for exploring associations between sets of genetic variants and complex traits. While most KM methods use a single kernel function to assess the marginal effect of a variable set, KM analyses involving multiple kernels have become increasingly popular. Multi-kernel analysis allows researchers to study more complex problems, such as assessing gene-gene or gene-environment interactions, incorporating variance-component based methods for population substructure into rare-variant association testing, and assessing the conditional effects of a variable set adjusting for other variable sets. The KM framework is robust, powerful, and provides efficient dimension reduction for multi-factor analyses, but requires the estimation of high dimensional nuisance parameters. Traditional estimation techniques, including regularization and the EM algorithm, have a large computational cost and are not scalable to large sample sizes needed for rare variant analysis. Therefore, under the context of gene-environment interaction, we propose a computationally efficient and statistically rigorous “fastKM” algorithm for multi-kernel analysis that is based on a low-rank approximation to the nuisance-effect kernel matrices. Our algorithm is applicable to various trait types (e.g., continuous, binary, and survival traits) and can be implemented using any existing single-kernel analysis software. Through extensive simulation studies, we show that our algorithm has similar performance to an EM-based KM approach for quantitative traits while running much faster. We also apply our method to the Vitamin Intervention for Stroke Prevention (VISP) clinical trial, examining gene-by-vitamin effects on recurrent stroke risk and gene-by-age effects on change in homocysteine level.},
	number = {6},
	urldate = {2017-01-22},
	journal = {Genetic epidemiology},
	author = {Marceau, Rachel and Lu, Wenbin and Holloway, Shannon and Sale, Michèle M. and Worrall, Bradford B. and Williams, Stephen R. and Hsu, Fang-Chi and Tzeng, Jung-Ying},
	month = sep,
	year = {2015},
	pmid = {26139508},
	pmcid = {PMC4544636},
	pages = {456--468},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/BMDUW2NM/Marceau et al. - 2015 - A Fast Multiple-Kernel Method with Applications to.pdf:application/pdf}
}

@article{cornelis_gene-environment_2012-1,
	title = {Gene-{Environment} {Interactions} in {Genome}-{Wide} {Association} {Studies}: {A} {Comparative} {Study} of {Tests} {Applied} to {Empirical} {Studies} of {Type} 2 {Diabetes}},
	volume = {175},
	issn = {0002-9262},
	shorttitle = {Gene-{Environment} {Interactions} in {Genome}-{Wide} {Association} {Studies}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3261439/},
	doi = {10.1093/aje/kwr368},
	abstract = {The question of which statistical approach is the most effective for investigating gene-environment (G-E) interactions in the context of genome-wide association studies (GWAS) remains unresolved. By using 2 case-control GWAS (the Nurses’ Health Study, 1976–2006, and the Health Professionals Follow-up Study, 1986–2006) of type 2 diabetes, the authors compared 5 tests for interactions: standard logistic regression-based case-control; case-only; semiparametric maximum-likelihood estimation of an empirical-Bayes shrinkage estimator; and 2-stage tests. The authors also compared 2 joint tests of genetic main effects and G-E interaction. Elevated body mass index was the exposure of interest and was modeled as a binary trait to avoid an inflated type I error rate that the authors observed when the main effect of continuous body mass index was misspecified. Although both the case-only and the semiparametric maximum-likelihood estimation approaches assume that the tested markers are independent of exposure in the general population, the authors did not observe any evidence of inflated type I error for these tests in their studies with 2,199 cases and 3,044 controls. Both joint tests detected markers with known marginal effects. Loci with the most significant G-E interactions using the standard, empirical-Bayes, and 2-stage tests were strongly correlated with the exposure among controls. Study findings suggest that methods exploiting G-E independence can be efficient and valid options for investigating G-E interactions in GWAS.},
	number = {3},
	urldate = {2017-01-21},
	journal = {American Journal of Epidemiology},
	author = {Cornelis, Marilyn C. and Tchetgen Tchetgen, Eric J. and Liang, Liming and Qi, Lu and Chatterjee, Nilanjan and Hu, Frank B. and Kraft, Peter},
	month = feb,
	year = {2012},
	pmid = {22199026},
	pmcid = {PMC3261439},
	pages = {191--202},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/4H98TI5H/Cornelis et al. - 2012 - Gene-Environment Interactions in Genome-Wide Assoc.pdf:application/pdf}
}

@article{voorman_behavior_2011-1,
	title = {Behavior of {QQ}-{Plots} and {Genomic} {Control} in {Studies} of {Gene}-{Environment} {Interaction}},
	volume = {6},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0019416},
	doi = {10.1371/journal.pone.0019416},
	abstract = {Genome-wide association studies of gene-environment interaction (GxE GWAS) are becoming popular. As with main effects GWAS, quantile-quantile plots (QQ-plots) and Genomic Control are being used to assess and correct for population substructure. However, in GE work these approaches can be seriously misleading, as we illustrate; QQ-plots may give strong indications of substructure when absolutely none is present. Using simulation and theory, we show how and why spurious QQ-plot inflation occurs in GE GWAS, and how this differs from main-effects analyses. We also explain how simple adjustments to standard regression-based methods used in GE GWAS can alleviate this problem.},
	number = {5},
	urldate = {2017-01-21},
	journal = {PLOS ONE},
	author = {Voorman, Arend and Lumley, Thomas and McKnight, Barbara and Rice, Kenneth},
	month = may,
	year = {2011},
	keywords = {Genome-wide association studies, Genetic drift, Linear regression analysis, Population genetics, Regression analysis, Simulation and modeling, Test statistics, Variant genotypes},
	pages = {e19416},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/R3AFBR4G/Voorman et al. - 2011 - Behavior of QQ-Plots and Genomic Control in Studie.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/3GQEAJ7Q/article.html:text/html}
}

@article{lin_test_2016,
	title = {Test for rare variants by environment interactions in sequencing association studies},
	volume = {72},
	issn = {1541-0420},
	doi = {10.1111/biom.12368},
	abstract = {We consider in this article testing rare variants by environment interactions in sequencing association studies. Current methods for studying the association of rare variants with traits cannot be readily applied for testing for rare variants by environment interactions, as these methods do not effectively control for the main effects of rare variants, leading to unstable results and/or inflated Type 1 error rates. We will first analytically study the bias of the use of conventional burden-based tests for rare variants by environment interactions, and show the tests can often be invalid and result in inflated Type 1 error rates. To overcome these difficulties, we develop the interaction sequence kernel association test (iSKAT) for assessing rare variants by environment interactions. The proposed test iSKAT is optimal in a class of variance component tests and is powerful and robust to the proportion of variants in a gene that interact with environment and the signs of the effects. This test properly controls for the main effects of the rare variants using weighted ridge regression while adjusting for covariates. We demonstrate the performance of iSKAT using simulation studies and illustrate its application by analysis of a candidate gene sequencing study of plasma adiponectin levels.},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Lin, Xinyi and Lee, Seunggeun and Wu, Michael C. and Wang, Chaolong and Chen, Han and Li, Zilin and Lin, Xihong},
	month = mar,
	year = {2016},
	pmid = {26229047},
	pmcid = {PMC4733434},
	keywords = {High-Throughput Nucleotide Sequencing, Humans, Polymorphism, Single Nucleotide, Genetic Association Studies, Genetic Predisposition to Disease, Genetic Variation, Sensitivity and Specificity, Reproducibility of Results, Data Interpretation, Statistical, Adiponectin, Alcoholism, Bias analysis, Gene-environment interactions, Rare Diseases, Sequencing association studies},
	pages = {156--164}
}

@article{tchetgen_tchetgen_robustness_2011-1,
	title = {On the robustness of tests of genetic associations incorporating gene-environment interaction when the environmental exposure is misspecified},
	volume = {22},
	issn = {1531-5487},
	doi = {10.1097/EDE.0b013e31820877c5},
	abstract = {We consider the robustness of tests of genetic associations that incorporate gene-environment interactions when the environmental exposure is misspecified, which is likely the case when the exposure is continuous. We formally prove that, under the null hypothesis of no genetic association, misspecified ordinary logistic regression and profile likelihood (Chatterjee and Carroll, Biometrika. 2005;92:399-418) analyses of case-control data both consistently estimate the null parameters of no genetic main effect and interaction, provided that genetic and environmental factors are unrelated in the underlying population. However, we argue that the associated likelihood ratio test, score test, and Wald test statistics obtained using the estimated information matrix have incorrect type-1 error rates due to model mis-specification. Based on these observations, we propose the use of the sandwich estimator of variance in conjunction with the consistent maximum (profile) likelihood estimates to construct Wald-type test statistics with correct type-1 error rate for the null of no genetic association.},
	language = {eng},
	number = {2},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Tchetgen Tchetgen, Eric J. and Kraft, Peter},
	month = mar,
	year = {2011},
	pmid = {21228699},
	keywords = {Humans, Genetic Predisposition to Disease, Algorithms, Environmental Exposure, Likelihood Functions, Logistic Models, Sensitivity and Specificity},
	pages = {257--261}
}

@article{he_set-based_2016,
	title = {Set-{Based} {Tests} for {Gene}-{Environment} {Interaction} in {Longitudinal} {Studies}},
	volume = {0},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2016.1252266},
	doi = {10.1080/01621459.2016.1252266},
	abstract = {We propose a generalized score type test for set-based inference for gene-environment interaction with longitudinally measured quantitative traits. The test is robust to misspecification of within subject correlation structure and has enhanced power compared to existing alternatives. Unlike tests for marginal genetic association, set-based tests for gene-environment interaction face the challenges of a potentially misspecified and high-dimensional main effect model under the null hypothesis. We show that our proposed test is robust to main effect misspecification of environmental exposure and genetic factors under the gene-environment independence condition. When genetic and environmental factors are dependent, the method of sieves is further proposed to eliminate potential bias due to a misspecified main effect of a continuous environmental exposure. A weighted principal component analysis approach is developed to perform dimension reduction when the number of genetic variants in the set is large relative to the sample size. The methods are motivated by an example from the Multi-Ethnic Study of Atherosclerosis (MESA), investigating interaction between measures of neighborhood environment and genetic regions on longitudinal measures of blood pressure over a study period of about seven years with 4 exams.},
	number = {ja},
	urldate = {2017-01-20},
	journal = {Journal of the American Statistical Association},
	author = {He, Zihuai and Zhang, Min and Lee, Seunggeun and Smith, Jennifer A. and Kardia, Sharon L. R. and Roux, Ana V. Diez and Mukherjee, Bhramar},
	month = dec,
	year = {2016},
	keywords = {Gene-environment independence, Generalized score test, MESA neighborhood study, Model misspecification, Robustness},
	pages = {0--0},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/3EFFPDWF/He et al. - 2016 - Set-Based Tests for Gene-Environment Interaction i.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/R9EGMFD7/01621459.2016.html:text/html}
}

@article{ma_gene-based_2013,
	title = {Gene-{Based} {Testing} of {Interactions} in {Association} {Studies} of {Quantitative} {Traits}},
	volume = {9},
	issn = {1553-7404},
	url = {http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1003321},
	doi = {10.1371/journal.pgen.1003321},
	abstract = {Author Summary Epistasis is likely to play a significant role in complex diseases or traits and is one of the many possible explanations for “missing heritability.” However, epistatic interactions have been difficult to detect in genome-wide association studies (GWAS) due to the limited power caused by the multiple-testing correction from the large number of tests conducted. Gene-based gene–gene interaction (GGG) tests might hold the key to relaxing the multiple-testing correction burden and increasing the power for identifying epistatic interactions in GWAS. Here, we developed GGG tests of quantitative traits by extending four P value combining methods and evaluated their type I error rates and power using extensive simulations. All four GGG tests are more powerful than a principal component-based test. We also applied our GGG tests to data from the Atherosclerosis Risk in Communities study and found five gene-level interactions associated with the levels of total cholesterol and high-density lipoprotein cholesterol (HDL-C). One interaction between SMAD3 and NEDD9 on HDL-C was further replicated in an independent sample from the Multi-Ethnic Study of Atherosclerosis.},
	number = {2},
	urldate = {2017-01-20},
	journal = {PLOS Genetics},
	author = {Ma, Li and Clark, Andrew G. and Keinan, Alon},
	month = feb,
	year = {2013},
	keywords = {Genome-wide association studies, Test statistics, Variant genotypes, Cholesterol, Lipids, Protein interactions, Protein-protein interactions, Quantitative traits},
	pages = {e1003321},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/9FR3ZTR3/Ma et al. - 2013 - Gene-Based Testing of Interactions in Association .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/66TK4EHG/article.html:text/html}
}

@article{ge_kernel_2015-1,
	title = {A kernel machine method for detecting effects of interaction between multidimensional variable sets: {An} imaging genetics application},
	volume = {109},
	issn = {1053-8119},
	shorttitle = {A kernel machine method for detecting effects of interaction between multidimensional variable sets},
	url = {//www.sciencedirect.com/science/article/pii/S1053811915000440},
	doi = {10.1016/j.neuroimage.2015.01.029},
	abstract = {Measurements derived from neuroimaging data can serve as markers of disease and/or healthy development, are largely heritable, and have been increasingly utilized as (intermediate) phenotypes in genetic association studies. To date, imaging genetic studies have mostly focused on discovering isolated genetic effects, typically ignoring potential interactions with non-genetic variables such as disease risk factors, environmental exposures, and epigenetic markers. However, identifying significant interaction effects is critical for revealing the true relationship between genetic and phenotypic variables, and shedding light on disease mechanisms. In this paper, we present a general kernel machine based method for detecting effects of the interaction between multidimensional variable sets. This method can model the joint and epistatic effect of a collection of single nucleotide polymorphisms (SNPs), accommodate multiple factors that potentially moderate genetic influences, and test for nonlinear interactions between sets of variables in a flexible framework. As a demonstration of application, we applied the method to the data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) to detect the effects of the interactions between candidate Alzheimer's disease (AD) risk genes and a collection of cardiovascular disease (CVD) risk factors, on hippocampal volume measurements derived from structural brain magnetic resonance imaging (MRI) scans. Our method identified that two genes, CR1 and EPHA1, demonstrate significant interactions with CVD risk factors on hippocampal volume, suggesting that CR1 and EPHA1 may play a role in influencing AD-related neurodegeneration in the presence of CVD risks.},
	urldate = {2017-01-20},
	journal = {NeuroImage},
	author = {Ge, Tian and Nichols, Thomas E. and Ghosh, Debashis and Mormino, Elizabeth C. and Smoller, Jordan W. and Sabuncu, Mert R.},
	month = apr,
	year = {2015},
	keywords = {Alzheimer's disease, Cardiovascular disease, Imaging genetics, Interaction, Kernel machines},
	pages = {505--514},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/A2IG9EMP/S1053811915000440.html:text/html}
}

@article{zhao_testing_2015,
	title = {Testing in {Microbiome}-{Profiling} {Studies} with {MiRKAT}, the {Microbiome} {Regression}-{Based} {Kernel} {Association} {Test}},
	volume = {96},
	issn = {0002-9297},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4570290/},
	doi = {10.1016/j.ajhg.2015.04.003},
	abstract = {High-throughput sequencing technology has enabled population-based studies of the role of the human microbiome in disease etiology and exposure response. Distance-based analysis is a popular strategy for evaluating the overall association between microbiome diversity and outcome, wherein the phylogenetic distance between individuals’ microbiome profiles is computed and tested for association via permutation. Despite their practical popularity, distance-based approaches suffer from important challenges, especially in selecting the best distance and extending the methods to alternative outcomes, such as survival outcomes. We propose the microbiome regression-based kernel association test (MiRKAT), which directly regresses the outcome on the microbiome profiles via the semi-parametric kernel machine regression framework. MiRKAT allows for easy covariate adjustment and extension to alternative outcomes while non-parametrically modeling the microbiome through a kernel that incorporates phylogenetic distance. It uses a variance-component score statistic to test for the association with analytical p value calculation. The model also allows simultaneous examination of multiple distances, alleviating the problem of choosing the best distance. Our simulations demonstrated that MiRKAT provides correctly controlled type I error and adequate power in detecting overall association. “Optimal” MiRKAT, which considers multiple candidate distances, is robust in that it suffers from little power loss in comparison to when the best distance is used and can achieve tremendous power gain in comparison to when a poor distance is chosen. Finally, we applied MiRKAT to real microbiome datasets to show that microbial communities are associated with smoking and with fecal protease levels after confounders are controlled for.},
	number = {5},
	urldate = {2017-01-20},
	journal = {American Journal of Human Genetics},
	author = {Zhao, Ni and Chen, Jun and Carroll, Ian M. and Ringel-Kulka, Tamar and Epstein, Michael P. and Zhou, Hua and Zhou, Jin J. and Ringel, Yehuda and Li, Hongzhe and Wu, Michael C.},
	month = may,
	year = {2015},
	pmid = {25957468},
	pmcid = {PMC4570290},
	pages = {797--807},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/8WZEW9SA/Zhao et al. - 2015 - Testing in Microbiome-Profiling Studies with MiRKA.pdf:application/pdf}
}

@misc{noauthor_statistical_nodate,
	title = {Statistical {Approaches} to {Gene} x {Environment} {Interactions} for {Complex} {Phenotypes}},
	url = {https://mitpress.mit.edu/statistical},
	abstract = {Diverse methodological and statistical approaches for investigating the role of gene-environment interactions in a range of complex diseases and traits.},
	urldate = {2017-01-20},
	journal = {MIT Press},
	file = {Snapshot:/home/jeremiah/Zotero/storage/IVR4TSB9/statistical.html:text/html}
}

@article{liu_semiparametric_2007-1,
	title = {Semiparametric {Regression} of {Multidimensional} {Genetic} {Pathway} {Data}: {Least}-{Squares} {Kernel} {Machines} and {Linear} {Mixed} {Models}},
	volume = {63},
	issn = {0006-341X},
	shorttitle = {Semiparametric {Regression} of {Multidimensional} {Genetic} {Pathway} {Data}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2665800/},
	doi = {10.1111/j.1541-0420.2007.00799.x},
	abstract = {We consider a semiparametric regression model that relates a normal outcome to covariates and a genetic pathway, where the covariate effects are modeled parametrically and the pathway effect of multiple gene expressions is modeled parametrically or nonparametrically using least-squares kernel machines (LSKMs). This unified framework allows a flexible function for the joint effect of multiple genes within a pathway by specifying a kernel function and allows for the possibility that each gene expression effect might be nonlinear and the genes within the same pathway are likely to interact with each other in a complicated way. This semiparametric model also makes it possible to test for the overall genetic pathway effect. We show that the LSKM semiparametric regression can be formulated using a linear mixed model. Estimation and inference hence can proceed within the linear mixed model framework using standard mixed model software. Both the regression coefficients of the covariate effects and the LSKM estimator of the genetic pathway effect can be obtained using the best linear unbiased predictor in the corresponding linear mixed model formulation. The smoothing parameter and the kernel parameter can be estimated as variance components using restricted maximum likelihood. A score test is developed to test for the genetic pathway effect. Model/variable selection within the LSKM framework is discussed. The methods are illustrated using a prostate cancer data set and evaluated using simulations.},
	number = {4},
	urldate = {2017-01-20},
	journal = {Biometrics},
	author = {Liu, Dawei and Lin, Xihong and Ghosh, Debashis},
	month = dec,
	year = {2007},
	pmid = {18078480},
	pmcid = {PMC2665800},
	pages = {1079--1088},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/A6WUZETN/Liu et al. - 2007 - Semiparametric Regression of Multidimensional Gene.pdf:application/pdf}
}

@article{broadaway_kernel_2015,
	title = {Kernel {Approach} for {Modeling} {Interaction} {Effects} in {Genetic} {Association} {Studies} of {Complex} {Quantitative} {Traits}},
	volume = {39},
	issn = {0741-0395},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4469530/},
	doi = {10.1002/gepi.21901},
	abstract = {The etiology of complex traits likely involves the effects of genetic and environmental factors, along with complicated interaction effects between them. Consequently, there has been interest in applying genetic association tests of complex traits that account for potential modification of the genetic effect in the presence of an environmental factor. One can perform such an analysis using a joint test of gene and gene-environment interaction. An optimal joint test would be one that remains powerful under a variety of models ranging from those of strong gene-environment interaction effect to those of little or no gene-environment interaction effect. To fill this demand, we have extended a kernel-machine based approach for association mapping of multiple SNPs to consider joint tests of gene and gene-environment interaction. The kernel-based approach for joint testing is promising, since it incorporates linkage disequilibrium information from multiple SNPs simultaneously in analysis and permits flexible modeling of interaction effects. Using simulated data, we show that our kernel-machine approach typically outperforms the traditional joint test under strong gene-environment interaction models and further outperforms the traditional main-effect association test under models of weak or no gene-environment interaction effects. We illustrate our test using genome-wide association data from the Grady Trauma Project, a cohort of highly traumatized, at-risk individuals, which has previously been investigated for interaction effects.},
	number = {5},
	urldate = {2017-01-20},
	journal = {Genetic epidemiology},
	author = {Broadaway, K. Alaine and Duncan, Richard and Conneely, Karen N. and Almli, Lynn M. and Bradley, Bekh and Ressler, Kerry J. and Epstein, Michael P.},
	month = jul,
	year = {2015},
	pmid = {25885490},
	pmcid = {PMC4469530},
	pages = {366--375},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/NF3JXGQF/Broadaway et al. - 2015 - Kernel Approach for Modeling Interaction Effects i.pdf:application/pdf}
}

@article{maity_powerful_2011,
	title = {Powerful tests for detecting a gene effect in the presence of possible gene-gene interactions using garrote kernel machines},
	volume = {67},
	issn = {0006-341X},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3142308/},
	doi = {10.1111/j.1541-0420.2011.01598.x},
	abstract = {We propose in this paper a powerful testing procedure for detecting a gene effect on a continuous outcome in the presence of possible gene-gene interactions (epistasis) in a gene set, e.g. a genetic pathway or network. Traditional tests for this purpose require a large number of degrees of freedom by testing the main effect and all the corresponding interactions under a parametric assumption, and hence suffer from low power. In this paper, we propose a powerful kernel machine based test. Specifically, our test is based on a garrote kernel method and is constructed as a score test. Here, the term garrote refers to an extra nonnegative parameter which is multiplied to the covariate of interest so that our score test can be formulated in terms of this nonnegative parameter. A key feature of the proposed test is that it is flexible and developed for both parametric and nonparametric models within a unified framework, and is more powerful than the standard test by accounting for the correlation among genes and hence often uses a much smaller degrees of freedom. We investigate the theoretical properties of the proposed test. We evaluate its finite sample performance using simulation studies, and apply the method to the Michigan prostate cancer gene expression data.},
	number = {4},
	urldate = {2017-01-20},
	journal = {Biometrics},
	author = {Maity, Arnab and Lin, Xihong},
	month = dec,
	year = {2011},
	pmid = {21504419},
	pmcid = {PMC3142308},
	pages = {1271--1284},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/9X28TCC9/Maity and Lin - 2011 - Powerful tests for detecting a gene effect in the .pdf:application/pdf}
}

@article{sejdinovic_equivalence_2013,
	title = {Equivalence of distance-based and {RKHS}-based statistics in hypothesis testing},
	volume = {41},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1207.6076},
	doi = {10.1214/13-AOS1140},
	abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies (MMD), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the MMD corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the MMD as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
	number = {5},
	urldate = {2017-01-20},
	journal = {The Annals of Statistics},
	author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
	month = oct,
	year = {2013},
	note = {arXiv: 1207.6076},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology, Mathematics - Statistics Theory},
	pages = {2263--2291},
	annote = {Comment: Published in at http://dx.doi.org/10.1214/13-AOS1140 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv\:1207.6076 PDF:/home/jeremiah/Zotero/storage/8B2UXI2B/Sejdinovic et al. - 2013 - Equivalence of distance-based and RKHS-based stati.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/XPDMV2WJ/1207.html:text/html}
}

@article{claesen_ensemblesvm:_2014,
	title = {{EnsembleSVM}: {A} {Library} for {Ensemble} {Learning} {Using} {Support} {Vector} {Machines}},
	volume = {15},
	shorttitle = {{EnsembleSVM}},
	url = {http://jmlr.org/papers/v15/claesen14a.html},
	urldate = {2017-01-19},
	journal = {Journal of Machine Learning Research},
	author = {Claesen, Marc and Smet, Frank De and Suykens, Johan A. K. and Moor, Bart De},
	year = {2014},
	pages = {141--145},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/ERJBZCFF/Claesen et al. - 2014 - EnsembleSVM A Library for Ensemble Learning Using.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/ZKZEQRJM/claesen14a.html:text/html}
}

@article{mcmahan_ad_2013,
	title = {Ad {Click} {Prediction}: a {View} from the {Trenches}},
	shorttitle = {Ad {Click} {Prediction}},
	url = {https://research.google.com/pubs/pub41159.html},
	urldate = {2017-01-19},
	author = {McMahan, H. Brendan and Holt, Gary and Sculley, D. and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and Chikkerur, Sharat and Liu, Dan and Wattenberg, Martin and Hrafnkelsson, Arnar Mar and Boulos, Tom and Kubica, Jeremy},
	year = {2013},
	file = {Snapshot:/home/jeremiah/Zotero/storage/KPXEB8D7/pub41159.html:text/html}
}

@article{cordell_detecting_2009,
	title = {Detecting gene-gene interactions that underlie human diseases},
	volume = {10},
	issn = {1471-0056},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2872761/},
	doi = {10.1038/nrg2579},
	abstract = {Following the identification of several disease-associated polymorphisms by whole genome association analysis, interest is now focussing on the detection of effects that, due to their interaction with other genetic (or environmental) factors, may not be identified by using standard single-locus tests. In addition to increasing power to detect association, there is also a hope detecting interactions between loci will allow us to elucidate the biological and biochemical pathways underpinning disease. Here I provide a critical survey of the current methodological approaches (and related software packages) used to detect interactions between genetic loci that contribute to human genetic disease. I also discuss the difficulties in determining the biologcal relevance of statistical interactions.},
	number = {6},
	urldate = {2017-01-19},
	journal = {Nature reviews. Genetics},
	author = {Cordell, Heather J.},
	month = jun,
	year = {2009},
	pmid = {19434077},
	pmcid = {PMC2872761},
	pages = {392--404},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/7JZES97P/Cordell - 2009 - Detecting gene-gene interactions that underlie hum.pdf:application/pdf}
}

@article{chen_bootstrap_2016,
	title = {On bootstrap approximations for high-dimensional {U}-statistics and random quadratic forms},
	url = {http://arxiv.org/abs/1610.00032},
	abstract = {This paper establishes a unified theory of bootstrap approximations for the probabilities of a non-degenerate U-statistic belonging to the hyperrectangles in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ when \$d\$ is large. Specifically, we show that the empirical bootstrap with the multinomial weights and the randomly reweighted bootstrap with iid Gaussian weights can be viewed as a more general random quadratic form and they are inferentially first-order equivalent in the following sense. Subject to mild moment conditions on the kernel, both methods achieve the same uniform rate of convergence over all \$d\$-dimensional hyperrectangles. In particular, they are asymptotically valid with high probability when the dimension \$d\$ can be as large as \$O(e{\textasciicircum}\{n{\textasciicircum}c\})\$ for some constant \$c {\textbackslash}in (0,1/7)\$. We also establish their equivalence to a Gaussian multiplier bootstrap with the jackknife covariance matrix estimator of the U-statistics. The bootstrap limit theorems rely on a general Gaussian approximation result and the tail probability inequalities for the maxima of non-degenerate U-statistics with the unbounded kernel. In particular, we derive explicit non-asymptotic error bounds for uniform approximation over the class of hyperrectangles in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$. Results established in this paper are nonlinear generalizations of the Gaussian and bootstrap approximations of the maxima of the high-dimensional sample mean vectors to the U-statistics of order two.},
	urldate = {2017-01-17},
	journal = {arXiv:1610.00032 [math, stat]},
	author = {Chen, Xiaohui},
	month = sep,
	year = {2016},
	note = {arXiv: 1610.00032},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv\:1610.00032 PDF:/home/jeremiah/Zotero/storage/926IF4R5/Chen - 2016 - On bootstrap approximations for high-dimensional U.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/N46XJSWQ/1610.html:text/html}
}

@article{rubenstein_kernel_2016,
	title = {A {Kernel} {Test} for {Three}-{Variable} {Interactions} with {Random} {Processes}},
	url = {http://arxiv.org/abs/1603.00929},
	abstract = {We apply a wild bootstrap method to the Lancaster three-variable interaction measure in order to detect factorisation of the joint distribution on three variables forming a stationary random process, for which the existing permutation bootstrap method fails. As in the i.i.d. case, the Lancaster test is found to outperform existing tests in cases for which two independent variables individually have a weak influence on a third, but that when considered jointly the influence is strong. The main contributions of this paper are twofold: first, we prove that the Lancaster statistic satisfies the conditions required to estimate the quantiles of the null distribution using the wild bootstrap; second, the manner in which this is proved is novel, simpler than existing methods, and can further be applied to other statistics.},
	urldate = {2017-01-12},
	journal = {arXiv:1603.00929 [stat]},
	author = {Rubenstein, Paul K. and Chwialkowski, Kacper P. and Gretton, Arthur},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.00929},
	keywords = {Statistics - Machine Learning, 62G10},
	annote = {Comment: 15 pages including 5 pages of supplementary material, 3 figures},
	file = {arXiv\:1603.00929 PDF:/home/jeremiah/Zotero/storage/F8KKNMJP/Rubenstein et al. - 2016 - A Kernel Test for Three-Variable Interactions with.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/XMDQE6CD/1603.html:text/html}
}

@article{reddi_decreasing_2014,
	title = {On the {Decreasing} {Power} of {Kernel} and {Distance} based {Nonparametric} {Hypothesis} {Tests} in {High} {Dimensions}},
	url = {http://arxiv.org/abs/1406.2083},
	abstract = {This paper is about two related decision theoretic problems, nonparametric two-sample testing and independence testing. There is a belief that two recently proposed solutions, based on kernels and distances between pairs of points, behave well in high-dimensional settings. We identify different sources of misconception that give rise to the above belief. Specifically, we differentiate the hardness of estimation of test statistics from the hardness of testing whether these statistics are zero or not, and explicitly discuss a notion of "fair" alternative hypotheses for these problems as dimension increases. We then demonstrate that the power of these tests actually drops polynomially with increasing dimension against fair alternatives. We end with some theoretical insights and shed light on the {\textbackslash}textit\{median heuristic\} for kernel bandwidth selection. Our work advances the current understanding of the power of modern nonparametric hypothesis tests in high dimensions.},
	urldate = {2017-01-12},
	journal = {arXiv:1406.2083 [cs, math, stat]},
	author = {Reddi, Sashank J. and Ramdas, Aaditya and Póczos, Barnabás and Singh, Aarti and Wasserman, Larry},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2083},
	keywords = {Computer Science - Information Theory, Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology, Mathematics - Statistics Theory},
	annote = {Comment: 19 pages, 9 figures, published in AAAI-15: The 29th AAAI Conference on Artificial Intelligence (with author order reversed from ArXiv)},
	file = {arXiv\:1406.2083 PDF:/home/jeremiah/Zotero/storage/4R3CRWMD/Reddi et al. - 2014 - On the Decreasing Power of Kernel and Distance bas.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WZAW2P75/1406.html:text/html}
}

@article{javanmard_confidence_2014,
	title = {Confidence {Intervals} and {Hypothesis} {Testing} for {High}-{Dimensional} {Regression}},
	volume = {15},
	url = {http://jmlr.org/papers/v15/javanmard14a.html},
	urldate = {2017-01-12},
	journal = {Journal of Machine Learning Research},
	author = {Javanmard, Adel and Montanari, Andrea},
	year = {2014},
	pages = {2869--2909},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/PM7RXU2Z/Javanmard and Montanari - 2014 - Confidence Intervals and Hypothesis Testing for Hi.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/9MSPQ5HC/javanmard14a.html:text/html}
}

@inproceedings{liu_eigenvalues_2015,
	address = {Austin, Texas},
	series = {{AAAI}'15},
	title = {Eigenvalues {Ratio} for {Kernel} {Selection} of {Kernel} {Methods}},
	isbn = {978-0-262-51129-2},
	url = {http://dl.acm.org/citation.cfm?id=2886521.2886713},
	abstract = {The selection of kernel function which determines the mapping between the input space and the feature space is of crucial importance to kernel methods. Existing kernel selection approaches commonly use some measures of generalization error, which are usually difficult to estimate and have slow convergence rates. In this paper, we propose a novel measure, called eigenvalues ratio (ER), of the tight bound of generalization error for kernel selection. ER is the ratio between the sum of the main eigenvalues and that of the tail eigenvalues of the kernel matrix. Different from most of existing measures, ER is defined on the kernel matrix, so it can be estimated easily from the available training data, which makes it usable for kernel selection. We establish tight ER-based generalization error bounds of order O (1/n) for several kernel-based methods under certain general conditions, while for most of existing measures, the convergence rate is at most O (1/√n ). Finally, to guarantee good generalization performance, we propose a novel kernel selection criterion by minimizing the derived tight generalization error bounds. Theoretical analysis and experimental results demonstrate that our kernel selection criterion is a good choice for kernel selection.},
	urldate = {2017-01-10},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Liu, Yong and Liao, Shizhong},
	year = {2015},
	pages = {2814--2820}
}

@inproceedings{welling_exponential_2004,
	title = {Exponential {Family} {Harmoniums} with an {Application} to {Information} {Retrieval}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/NIPS2005_738},
	urldate = {2017-01-10},
	author = {Welling, Max and Rosen-zvi, Michal and Hinton, Geoffrey E.},
	year = {2004},
	pages = {1481--1488},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/Q735DESS/Welling et al. - 2004 - Exponential Family Harmoniums with an Application .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/8CWJRFV8/NIPS2005_738.html:text/html}
}

@article{bolthausen_estimate_1984,
	title = {An estimate of the remainder in a combinatorial central limit theorem},
	volume = {66},
	issn = {0178-8051},
	url = {http://www.zora.uzh.ch/23058/},
	number = {3},
	urldate = {2016-12-28},
	journal = {Probability Theory and Related Fields},
	author = {Bolthausen, E.},
	year = {1984},
	pages = {379--386},
	file = {Snapshot:/home/jeremiah/Zotero/storage/8UW997G5/23058.html:text/html}
}

@article{liu_stein_2016,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1608.04471},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	urldate = {2016-12-27},
	journal = {arXiv:1608.04471 [cs, stat]},
	author = {Liu, Qiang and Wang, Dilin},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.04471},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: To appear in NIPS 2016}
}

@article{van_der_laan_generally_2015,
	title = {A {Generally}  {Efficient} {Targeted} {Minimum} {Loss} {Based} {Estimator}},
	url = {http://biostats.bepress.com/ucbbiostat/paper343},
	journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {van der Laan, Mark},
	month = dec,
	year = {2015},
	file = {"A Generally Efficient Targeted Minimum Loss Based Estimator" by Mark J. van der Laan:/home/jeremiah/Zotero/storage/7KXGF96G/paper343.html:text/html}
}

@article{van_collaborative_2010,
	title = {Collaborative {Double} {Robust} {Targeted} {Maximum} {Likelihood} {Estimation}},
	volume = {6},
	issn = {1557-4679},
	url = {https://www.degruyter.com/view/j/ijb.2010.6.1/ijb.2010.6.1.1181/ijb.2010.6.1.1181.xml},
	doi = {10.2202/1557-4679.1181},
	abstract = {Collaborative double robust targeted maximum likelihood estimators represent a fundamental further advance over standard targeted maximum likelihood estimators of a pathwise differentiable parameter of a data generating distribution in a semiparametric model, introduced in van der Laan, Rubin (2006). The targeted maximum likelihood approach involves fluctuating an initial estimate of a relevant factor (Q) of the density of the observed data, in order to make a bias/variance tradeoff targeted towards the parameter of interest. The fluctuation involves estimation of a nuisance parameter portion of the likelihood, g. TMLE has been shown to be consistent and asymptotically normally distributed (CAN) under regularity conditions, when either one of these two factors of the likelihood of the data is correctly specified, and it is semiparametric efficient if both are correctly specified.In this article we provide a template for applying collaborative targeted maximum likelihood estimation (C-TMLE) to the estimation of pathwise differentiable parameters in semi-parametric models. The procedure creates a sequence of candidate targeted maximum likelihood estimators based on an initial estimate for Q coupled with a succession of increasingly non-parametric estimates for g. In a departure from current state of the art nuisance parameter estimation, C-TMLE estimates of g are constructed based on a loss function for the targeted maximum likelihood estimator of the relevant factor Q that uses the nuisance parameter to carry out the fluctuation, instead of a loss function for the nuisance parameter itself. Likelihood-based cross-validation is used to select the best estimator among all candidate TMLE estimators of Q0 in this sequence. A penalized-likelihood loss function for Q is suggested when the parameter of interest is borderline-identifiable.We present theoretical results for "collaborative double robustness," demonstrating that the collaborative targeted maximum likelihood estimator is CAN even when Q and g are both mis-specified, providing that g solves a specified score equation implied by the difference between the Q and the true Q0. This marks an improvement over the current definition of double robustness in the estimating equation literature.We also establish an asymptotic linearity theorem for the C-DR-TMLE of the target parameter, showing that the C-DR-TMLE is more adaptive to the truth, and, as a consequence, can even be super efficient if the first stage density estimator does an excellent job itself with respect to the target parameter.This research provides a template for targeted efficient and robust loss-based learning of a particular target feature of the probability distribution of the data within large (infinite dimensional) semi-parametric models, while still providing statistical inference in terms of confidence intervals and p-values. This research also breaks with a taboo (e.g., in the propensity score literature in the field of causal inference) on using the relevant part of likelihood to fine-tune the fitting of the nuisance parameter/censoring mechanism/treatment mechanism.},
	number = {1},
	urldate = {2016-12-26},
	journal = {The International Journal of Biostatistics},
	author = {van, der Laan Mark J. and Gruber, Susan},
	year = {2010}
}

@article{van_der_laan_super_2007,
	title = {Super {Learner}},
	url = {http://biostats.bepress.com/ucbbiostat/paper222},
	journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {van der Laan, Mark and Polley, Eric and Hubbard, Alan},
	month = jul,
	year = {2007},
	file = {"Super Learner" by Mark J. van der Laan, Eric C. Polley et al.:/home/jeremiah/Zotero/storage/5GSNHC65/paper222.html:text/html}
}

@article{oates_control_2014,
	title = {Control functionals for {Monte} {Carlo} integration},
	url = {http://arxiv.org/abs/1410.2392},
	abstract = {A non-parametric extension of control variates is presented. These leverage gradient information on the sampling density to achieve substantial variance reduction. It is not required that the sampling density be normalised. The novel contribution of this work is based on two important insights; (i) a trade-off between random sampling and deterministic approximation and (ii) a new gradient-based function space derived from Stein's identity. Unlike classical control variates, our estimators achieve super-root-\$n\$ convergence, often requiring orders of magnitude fewer simulations to achieve a fixed level of precision. Theoretical and empirical results are presented, the latter focusing on integration problems arising in hierarchical models and models based on non-linear ordinary differential equations.},
	urldate = {2016-12-23},
	journal = {arXiv:1410.2392 [stat]},
	author = {Oates, Chris J. and Girolami, Mark and Chopin, Nicolas},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.2392},
	keywords = {Statistics - Methodology},
	annote = {Comment: Accepted for publication in J. R. Statist. Soc. B},
	file = {arXiv\:1410.2392 PDF:/home/jeremiah/Zotero/storage/AFAUT733/Oates et al. - 2014 - Control functionals for Monte Carlo integration.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/7PX3PCNW/1410.html:text/html}
}

@article{chwialkowski_kernel_2016,
	title = {A {Kernel} {Test} of {Goodness} of {Fit}},
	url = {http://arxiv.org/abs/1602.02964},
	abstract = {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein's method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.},
	urldate = {2016-12-23},
	journal = {arXiv:1602.02964 [stat]},
	author = {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02964},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: 14 pages, 9 figures},
	file = {arXiv\:1602.02964 PDF:/home/jeremiah/Zotero/storage/RJCV7P2P/Chwialkowski et al. - 2016 - A Kernel Test of Goodness of Fit.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/RKGPCJ4X/1602.html:text/html}
}

@article{muller_risk_2013,
	title = {Risk of {Bayesian} {Inference} in {Misspecified} {Models}, and the {Sandwich} {Covariance} {Matrix}},
	volume = {81},
	issn = {1468-0262},
	url = {http://onlinelibrary.wiley.com/doi/10.3982/ECTA9097/abstract},
	doi = {10.3982/ECTA9097},
	abstract = {It is well known that, in misspecified parametric models, the maximum likelihood estimator (MLE) is consistent for the pseudo-true value and has an asymptotically normal sampling distribution with “sandwich” covariance matrix. Also, posteriors are asymptotically centered at the MLE, normal, and of asymptotic variance that is, in general, different than the sandwich matrix. It is shown that due to this discrepancy, Bayesian inference about the pseudo-true parameter value is, in general, of lower asymptotic frequentist risk when the original posterior is substituted by an artificial normal posterior centered at the MLE with sandwich covariance matrix. An algorithm is suggested that allows the implementation of this artificial posterior also in models with high dimensional nuisance parameters which cannot reasonably be estimated by maximizing the likelihood.},
	language = {en},
	number = {5},
	urldate = {2016-12-21},
	journal = {Econometrica},
	author = {Müller, Ulrich K.},
	month = sep,
	year = {2013},
	keywords = {interval estimation, Posterior variance, pseudo-true parameter value, quasi-likelihood},
	pages = {1805--1849},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/SIXMNEVZ/Müller - 2013 - Risk of Bayesian Inference in Misspecified Models,.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/VAQQXMZF/abstract.html:text/html}
}

@article{grosse_testing_2014,
	title = {Testing {MCMC} code},
	url = {http://arxiv.org/abs/1412.5218},
	abstract = {Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilistic modeling and inference, but are difficult to debug, and are prone to silent failure if implemented naively. We outline several strategies for testing the correctness of MCMC algorithms. Specifically, we advocate writing code in a modular way, where conditional probability calculations are kept separate from the logic of the sampler. We discuss strategies for both unit testing and integration testing. As a running example, we show how a Python implementation of Gibbs sampling for a mixture of Gaussians model can be tested.},
	urldate = {2016-12-21},
	journal = {arXiv:1412.5218 [cs, stat]},
	author = {Grosse, Roger B. and Duvenaud, David K.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.5218},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Software Engineering},
	annote = {Comment: Presented at the 2014 NIPS workshop on Software Engineering for Machine Learning},
	file = {arXiv\:1412.5218 PDF:/home/jeremiah/Zotero/storage/JW7WJPZ8/Grosse and Duvenaud - 2014 - Testing MCMC code.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/VW4I48VX/1412.html:text/html}
}

@article{brooks_approach_1997,
	title = {An {Approach} to {Diagnosing} {Total} {Variation} {Convergence} of {MCMC} {Algorithms}},
	volume = {6},
	issn = {1061-8600},
	url = {http://www.jstor.org/stable/1390732},
	doi = {10.2307/1390732},
	abstract = {We introduce a convergence diagnostic procedure for MCMC that operates by estimating total variation distances for the distribution of the algorithm after certain numbers of iterations. The method has advantages over many existing methods in terms of applicability, utility, and interpretability. It can be used to assess convergence of both marginal and joint posterior densities, and we show how it can be applied to the two most commonly used MCMC samplers--the Gibbs Sampler and the Metropolis Hastings algorithm. In some cases, the computational burden of this method may be large, but we show how lower dimensional analogues of the full-dimensional method are available at a lower computational cost. Illustrative examples highlight the utility and interpretability of the proposed diagnostic, but also highlight some of its limitations.},
	number = {3},
	urldate = {2016-12-21},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Brooks, S. P. and Dellaportas, P. and Roberts, G. O.},
	year = {1997},
	pages = {251--265},
	file = {JSTOR Full Text PDF:/home/jeremiah/Zotero/storage/JRTPEESU/Brooks et al. - 1997 - An Approach to Diagnosing Total Variation Converge.pdf:application/pdf}
}

@article{gretton_kernel_2008,
	title = {A {Kernel} {Method} for the {Two}-{Sample} {Problem}},
	url = {http://arxiv.org/abs/0805.2368},
	abstract = {We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	urldate = {2016-12-21},
	journal = {arXiv:0805.2368 [cs]},
	author = {Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte J. and Scholkopf, Bernhard and Smola, Alexander J.},
	month = may,
	year = {2008},
	note = {arXiv: 0805.2368},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, G.3, I.2.6},
	file = {arXiv\:0805.2368 PDF:/home/jeremiah/Zotero/storage/6AID6IHV/Gretton et al. - 2008 - A Kernel Method for the Two-Sample Problem.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/FVCT6FX2/0805.html:text/html}
}

@unpublished{chauveau_how_2007,
	title = {How to compare {MCMC} simulation strategies?},
	url = {https://hal.archives-ouvertes.fr/hal-00019174},
	abstract = {In MCMC methods, such as the Metropolis-Hastings (MH) algorithm, the Gibbs sampler, or recent adaptive methods, many different strategies can be proposed, often associated in practice to unknown rates of convergence. In this paper we propose a simulation-based methodology to compare these rates of convergence, grounded on an entropy criterion computed from parallel (i.i.d.) simulated Markov chains coming from each candidate strategy. Our criterion determines the most efficient strategy among the candidates. Theoretically, we give for the MH algorithm general conditions under which its successive densities satisfy adequate smoothness and tail properties, so that this entropy criterion can be estimated consistently using kernel density estimate and Monte Carlo integration. Simulated and actual examples in moderate dimensions are provided to illustrate this approach.},
	urldate = {2016-12-20},
	author = {Chauveau, Didier and Vandekerkhove, Pierre},
	month = apr,
	year = {2007},
	note = {working paper or preprint},
	keywords = {Markov chain Monte Carlo, Bayesian model, Entropy, Kullback divergence, Metropolis-Hastings algorithm, nonparametric statistic, proposal distribution},
	file = {HAL PDF Full Text:/home/jeremiah/Zotero/storage/3VGVRJ2A/Chauveau and Vandekerkhove - 2007 - How to compare MCMC simulation strategies.pdf:application/pdf}
}

@article{chauveau_selection_2006,
	title = {Selection of a {MCMC} simulation strategy via an entropy convergence criterion},
	url = {http://arxiv.org/abs/math/0605263},
	abstract = {In MCMC methods, such as the Metropolis-Hastings (MH) algorithm, the Gibbs sampler, or recent adaptive methods, many different strategies can be proposed, often associated in practice to unknown rates of convergence. In this paper we propose a simulation-based methodology to compare these rates of convergence, grounded on an entropy criterion computed from parallel (i.i.d.) simulated Markov chains coming from each candidate strategy. Our criterion determines on the very first iterations the best strategy among the candidates. Theoretically, we give for the MH algorithm general conditions under which its successive densities satisfy adequate smoothness and tail properties, so that this entropy criterion can be estimated consistently using kernel density estimate and Monte Carlo integration. Simulated examples are provided to illustrate this convergence criterion.},
	urldate = {2016-12-20},
	journal = {arXiv:math/0605263},
	author = {Chauveau, Didier and Vandekerkhove, Pierre},
	month = may,
	year = {2006},
	note = {arXiv: math/0605263},
	keywords = {Mathematics - Statistics Theory, 60J22, 62M05, 62G07},
	file = {arXiv\:math/0605263 PDF:/home/jeremiah/Zotero/storage/A3BXD92H/Chauveau and Vandekerkhove - 2006 - Selection of a MCMC simulation strategy via an ent.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/6UAJBQ4N/0605263.html:text/html}
}

@article{andersen_hit_2007,
	title = {Hit and run as a unifying device},
	volume = {148},
	issn = {1962-5197},
	url = {https://eudml.org/doc/93471},
	language = {eng},
	number = {4},
	urldate = {2016-12-20},
	journal = {Journal de la société française de statistique},
	author = {Andersen, Hans C. and Diaconis, Persi},
	year = {2007},
	pages = {5--28},
	file = {Snapshot:/home/jeremiah/Zotero/storage/6KHC7MPI/93471.html:text/html}
}

@article{chwialkowski_kernel_2016-1,
	title = {A {Kernel} {Test} of {Goodness} of {Fit}},
	url = {http://arxiv.org/abs/1602.02964},
	abstract = {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein's method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.},
	urldate = {2016-12-20},
	journal = {arXiv:1602.02964 [stat]},
	author = {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02964},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: 14 pages, 9 figures},
	file = {arXiv\:1602.02964 PDF:/home/jeremiah/Zotero/storage/H5NKNQA5/Chwialkowski et al. - 2016 - A Kernel Test of Goodness of Fit.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/9JZE5M6T/1602.html:text/html}
}

@article{liu_kernelized_2016,
	title = {A {Kernelized} {Stein} {Discrepancy} for {Goodness}-of-fit {Tests} and {Model} {Evaluation}},
	url = {http://arxiv.org/abs/1602.03253},
	abstract = {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein's identity with the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.},
	urldate = {2016-12-20},
	journal = {arXiv:1602.03253 [stat]},
	author = {Liu, Qiang and Lee, Jason D. and Jordan, Michael I.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.03253},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1602.03253 PDF:/home/jeremiah/Zotero/storage/2WR23SU8/Liu et al. - 2016 - A Kernelized Stein Discrepancy for Goodness-of-fit.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/TRX27W8F/1602.html:text/html}
}

@article{lin_why_2016,
	title = {Why does deep and cheap learning work so well?},
	url = {http://arxiv.org/abs/1608.08225},
	abstract = {We show how the success of deep learning depends not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can be approximated through "cheap learning" with exponentially fewer parameters than generic ones, because they have simplifying properties tracing back to the laws of physics. The exceptional simplicity of physics-based functions hinges on properties such as symmetry, locality, compositionality and polynomial log-probability, and we explore how these properties translate into exceptionally simple neural networks approximating both natural phenomena such as images and abstract representations thereof such as drawings. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to renormalization group procedures. We prove various "no-flattening theorems" showing when such efficient deep networks cannot be accurately approximated by shallow ones without efficiency loss: flattening even linear functions can be costly, and flattening polynomials is exponentially expensive; we use group theoretic techniques to show that n variables cannot be multiplied using fewer than 2{\textasciicircum}n neurons in a single hidden layer.},
	urldate = {2016-12-17},
	journal = {arXiv:1608.08225 [cond-mat, stat]},
	author = {Lin, Henry W. and Tegmark, Max},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.08225},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks},
	annote = {Comment: Added proof that n variables cannot be multiplied using fewer than 2{\textasciicircum}n neurons in a single layer; switched x{\textless}-{\textgreater}y to conform with ML-notation; added refs \& response to Mehta \& Schwab comment arXiv:1609.03541. 17 pages, 3 figs},
	file = {arXiv\:1608.08225 PDF:/home/jeremiah/Zotero/storage/ZVDUXCFB/Lin and Tegmark - 2016 - Why does deep and cheap learning work so well.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/CZBTNDKW/1608.html:text/html}
}

@article{santoro_one-shot_2016,
	title = {One-shot {Learning} with {Memory}-{Augmented} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1605.06065},
	abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
	urldate = {2016-12-16},
	journal = {arXiv:1605.06065 [cs]},
	author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
	month = may,
	year = {2016},
	note = {arXiv: 1605.06065},
	keywords = {Computer Science - Learning},
	annote = {Comment: 13 pages, 8 figures},
	file = {arXiv\:1605.06065 PDF:/home/jeremiah/Zotero/storage/34ICI3Z3/Santoro et al. - 2016 - One-shot Learning with Memory-Augmented Neural Net.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/62KUSVXC/1605.html:text/html}
}

@article{ales_neural_2016,
	title = {Neural {Turing} {Machines}: {Convergence} of {Copy} {Tasks}},
	shorttitle = {Neural {Turing} {Machines}},
	url = {http://arxiv.org/abs/1612.02336},
	abstract = {The architecture of neural Turing machines is differentiable end to end and is trainable with gradient descent methods. Due to their large unfolded depth Neural Turing Machines are hard to train and because of their linear access of complete memory they do not scale. Other architectures have been studied to overcome these difficulties. In this report we focus on improving the quality of prediction of the original linear memory architecture on copy and repeat copy tasks. Copy task predictions on sequences of length six times larger than those the neural Turing machine was trained on prove to be highly accurate and so do predictions of repeat copy tasks for sequences with twice the repetition number and twice the sequence length neural Turing machine was trained on.},
	urldate = {2016-12-16},
	journal = {arXiv:1612.02336 [cs]},
	author = {Aleš, Janez},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.02336},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Predictor weights can be provided upon request},
	file = {arXiv\:1612.02336 PDF:/home/jeremiah/Zotero/storage/EZAWUKUA/Aleš - 2016 - Neural Turing Machines Convergence of Copy Tasks.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/82CHGQ5W/1612.html:text/html}
}

@article{henaff_tracking_2016,
	title = {Tracking the {World} {State} with {Recurrent} {Entity} {Networks}},
	url = {http://arxiv.org/abs/1612.03969},
	abstract = {We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.},
	urldate = {2016-12-16},
	journal = {arXiv:1612.03969 [cs]},
	author = {Henaff, Mikael and Weston, Jason and Szlam, Arthur and Bordes, Antoine and LeCun, Yann},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.03969},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1612.03969 PDF:/home/jeremiah/Zotero/storage/GI647Q7I/Henaff et al. - 2016 - Tracking the World State with Recurrent Entity Net.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MNT72WSM/1612.html:text/html}
}

@article{henaff_tracking_2016-1,
	title = {Tracking the {World} {State} with {Recurrent} {Entity} {Networks}},
	url = {http://arxiv.org/abs/1612.03969},
	abstract = {We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.},
	urldate = {2016-12-16},
	journal = {arXiv:1612.03969 [cs]},
	author = {Henaff, Mikael and Weston, Jason and Szlam, Arthur and Bordes, Antoine and LeCun, Yann},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.03969},
	keywords = {Computer Science - Computation and Language}
}

@article{sinharay_assessing_2003,
	title = {Assessing {Convergence} of the {Markov} {Chain} {Monte} {Carlo} {Algorithms}: {A} {Review}},
	volume = {2003},
	issn = {2330-8516},
	shorttitle = {Assessing {Convergence} of the {Markov} {Chain} {Monte} {Carlo} {Algorithms}},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/j.2333-8504.2003.tb01899.x/abstract},
	doi = {10.1002/j.2333-8504.2003.tb01899.x},
	abstract = {Markov chain Monte Carlo (MCMC) algorithms are in wide use for fitting complicated statistical models in psychometrics in situations where the traditional estimation techniques are very difficult to apply. One of the stumbling blocks in using an MCMC algorithm is determining the convergence of the algorithm. Because the convergence is not that of a scalar quantity to a point, but that of a distribution to another distribution, the issue remains an enigma to many users of MCMC, especially to those without a sound knowledge of mathematical statistics. This article is an attempt to provide psychometricians using the MCMC algorithms a better understanding of the concept of convergence of the algorithms and an improved knowledge about the diagnostics tools to assess convergence of the MCMC algorithms.},
	language = {en},
	number = {1},
	urldate = {2016-12-14},
	journal = {ETS Research Report Series},
	author = {Sinharay, Sandip},
	month = jun,
	year = {2003},
	keywords = {bayesian analysis, convergence diagnostics, gibbs sampling, MCMC},
	pages = {i--52},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/247RDPAM/Sinharay - 2003 - Assessing Convergence of the Markov Chain Monte Ca.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/7MTBJD2B/abstract.html:text/html}
}

@article{gilks_adaptive_1992,
	title = {Adaptive {Rejection} {Sampling} for {Gibbs} {Sampling}},
	volume = {41},
	issn = {0035-9254},
	url = {http://www.jstor.org/stable/2347565},
	doi = {10.2307/2347565},
	abstract = {We propose a method for rejection sampling from any univariate log-concave probability density function. The method is adaptive: as sampling proceeds, the rejection envelope and the squeezing function converge to the density function. The rejection envelope and squeezing function are piece-wise exponential functions, the rejection envelope touching the density at previously sampled points, and the squeezing function forming arcs between those points of contact. The technique is intended for situations where evaluation of the density is computationally expensive, in particular for applications of Gibbs sampling to Bayesian models with non-conjugacy. We apply the technique to a Gibbs sampling analysis of monoclonal antibody reactivity.},
	number = {2},
	urldate = {2016-12-14},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Gilks, W. R. and Wild, P.},
	year = {1992},
	pages = {337--348}
}

@article{ahmadian_efficient_2011,
	title = {Efficient {Markov} {Chain} {Monte} {Carlo} {Methods} for {Decoding} {Neural} {Spike} {Trains}},
	volume = {23},
	issn = {0899-7667},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4740351/},
	doi = {10.1162/NECO_a_00059},
	abstract = {Stimulus reconstruction or decoding methods provide an important tool for understanding how sensory and motor information is represented in neural activity. We discuss Bayesian decoding methods based on an encoding generalized linear model (GLM) that accurately describes how stimuli are transformed into the spike trains of a group of neurons. The form of the GLM likelihood ensures that the posterior distribution over the stimuli that caused an observed set of spike trains is log-concave so long as the prior is. This allows the maximum a posteriori (MAP) stimulus estimate to be obtained using efficient optimization algorithms. Unfortunately, the MAP estimate can have a relatively large average error when the posterior is highly non-Gaussian. Here we compare several Markov chain Monte Carlo (MCMC) algorithms that allow for the calculation of general Bayesian estimators involving posterior expectations (conditional on model parameters). An efficient version of the hybrid Monte Carlo (HMC) algorithm was significantly superior to other MCMC methods for Gaussian priors. When the prior distribution has sharp edges and corners, on the other hand, the “hit-and-run” algorithm performed better than other MCMC methods. Using these algorithms we show that for this latter class of priors the posterior mean estimate can have a considerably lower average error than MAP, whereas for Gaussian priors the two estimators have roughly equal efficiency. We also address the application of MCMC methods for extracting non-marginal properties of the posterior distribution. For example, by using MCMC to calculate the mutual information between the stimulus and response, we verify the validity of a computationally efficient Laplace approximation to this quantity for Gaussian priors in a wide range of model parameters; this makes direct model-based computation of the mutual information tractable even in the case of large observed neural populations, where methods based on binning the spike train fail. Finally, we consider the effect of uncertainty in the GLM parameters on the posterior estimators.},
	number = {1},
	urldate = {2016-12-14},
	journal = {Neural computation},
	author = {Ahmadian, Yashar and Pillow, Jonathan W. and Paninski, Liam},
	month = jan,
	year = {2011},
	pmid = {20964539},
	pmcid = {PMC4740351},
	pages = {46--96},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/I2VC925D/Ahmadian et al. - 2011 - Efficient Markov Chain Monte Carlo Methods for Dec.pdf:application/pdf}
}

@article{christen_optimal_2012,
	title = {On optimal direction gibbs sampling},
	url = {http://arxiv.org/abs/1205.4062},
	abstract = {Generalized Gibbs kernels are those that may take any direction not necessarily bounded to each axis along the parameters of the objective function. We study how to optimally choose such directions in a Directional, random scan, Gibbs sampler setting. The optimal direction is chosen by minimizing to the mutual information (Kullback-Leibler divergence) of two steps of the MCMC for a truncated Normal objective function. The result is generalized to be used when a Multivariate Normal (local) approximation is available for the objective function. Three Gibbs direction distributions are tested in highly skewed non-normal objective functions.},
	urldate = {2016-12-14},
	journal = {arXiv:1205.4062 [math, stat]},
	author = {Christen, J. Andrés and Fox, Colin and Pérez-Ruiz, Diego Andrés and Santana-Cibrian, Mario},
	month = may,
	year = {2012},
	note = {arXiv: 1205.4062},
	keywords = {Statistics - Computation, 62F15, Mathematics - Numerical Analysis},
	file = {arXiv\:1205.4062 PDF:/home/jeremiah/Zotero/storage/QDTRVQJI/Christen et al. - 2012 - On optimal direction gibbs sampling.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/RMT94E6Q/1205.html:text/html}
}

@article{andersen_hit_2007-1,
	title = {Hit and run as a unifying device},
	volume = {148},
	issn = {1962-5197},
	url = {https://eudml.org/doc/93471},
	language = {eng},
	number = {4},
	urldate = {2016-12-14},
	journal = {Journal de la société française de statistique},
	author = {Andersen, Hans C. and Diaconis, Persi},
	year = {2007},
	pages = {5--28},
	file = {Snapshot:/home/jeremiah/Zotero/storage/48J425IG/93471.html:text/html}
}

@article{lovasz_hit-and-run_1999,
	title = {Hit-and-run mixes fast},
	volume = {86},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/article/10.1007/s101070050099},
	doi = {10.1007/s101070050099},
	abstract = {. It is shown that the “hit-and-run” algorithm for sampling from a convex body K (introduced by R.L. Smith) mixes in time O*(n2R2/r2), where R and r are the radii of the inscribed and circumscribed balls of K. Thus after appropriate preprocessing, hit-and-run produces an approximately uniformly distributed sample point in time O*(n3), which matches the best known bound for other sampling algorithms. We show that the bound is best possible in terms of R,r and n.},
	language = {en},
	number = {3},
	urldate = {2016-12-14},
	journal = {Mathematical Programming},
	author = {Lovász, László},
	month = dec,
	year = {1999},
	pages = {443--461},
	file = {Snapshot:/home/jeremiah/Zotero/storage/6966V5KX/s101070050099.html:text/html}
}

@book{smith_optimal_1991,
	title = {optimal direction choice for hit-and-run sampling},
	language = {en},
	author = {smith, robert l, david e kaufman},
	year = {1991},
	note = {Google-Books-ID: 9mpb5DS4G6QC}
}

@article{rudolf_comparison_2015,
	title = {Comparison of hit-and-run, slice sampling and random walk {Metropolis}},
	url = {http://arxiv.org/abs/1505.00579},
	abstract = {Different Markov chains can be used for approximate sampling of a distribution given by an unnormalized density function with respect to the Lebesgue measure. The hit-and-run, (hybrid) slice sampler and random walk Metropolis algorithm are popular tools to simulate such Markov chains. We develop a general approach to compare the efficiency of these sampling procedures by the use of a partial ordering of their Markov operators, the covariance ordering. In particular, we show that the hit-and-run and the simple slice sampler are more efficient than a hybrid slice sampler based on hit-and-run which, itself, is more efficient than a (lazy) random walk Metropolis algorithm.},
	urldate = {2016-12-13},
	journal = {arXiv:1505.00579 [math, stat]},
	author = {Rudolf, Daniel and Ullrich, Mario},
	month = may,
	year = {2015},
	note = {arXiv: 1505.00579},
	keywords = {Mathematics - Statistics Theory, Mathematics - Probability},
	annote = {Comment: 19 pages},
	file = {arXiv\:1505.00579 PDF:/home/jeremiah/Zotero/storage/D8NKR4FX/Rudolf and Ullrich - 2015 - Comparison of hit-and-run, slice sampling and rand.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/B7BB9KFJ/1505.html:text/html}
}

@article{foster_level-set_2012,
	title = {A {Level}-{Set} {Hit}-and-{Run} {Sampler} for {Quasi}-{Concave} {Distributions}},
	url = {http://arxiv.org/abs/1202.4094},
	abstract = {We develop a new sampling strategy that uses the hit-and-run algorithm within level sets of the target density. Our method can be applied to any quasi-concave density, which covers a broad class of models. Our sampler performs well in high-dimensional settings, which we illustrate with a comparison to Gibbs sampling on a spike-and-slab mixture model. We also extend our method to exponentially-tilted quasi-concave densities, which arise often in Bayesian models consisting of a log-concave likelihood and quasi-concave prior density. Within this class of models, our method is effective at sampling from posterior distributions with high dependence between parameters, which we illustrate with a simple multivariate normal example. We also implement our level-set sampler on a Cauchy-normal model where we demonstrate the ability of our level set sampler to handle multi-modal posterior distributions.},
	urldate = {2016-12-13},
	journal = {arXiv:1202.4094 [stat]},
	author = {Foster, Dean and Jensen, Shane T.},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.4094},
	keywords = {Statistics - Computation},
	file = {arXiv\:1202.4094 PDF:/home/jeremiah/Zotero/storage/VJJW5ARF/Foster and Jensen - 2012 - A Level-Set Hit-and-Run Sampler for Quasi-Concave .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/XSGGAXT2/1202.html:text/html}
}

@inproceedings{evgeniou_bounds_2000,
	address = {San Francisco, CA, USA},
	series = {{ICML} '00},
	title = {Bounds on the {Generalization} {Performance} of {Kernel} {Machine} {Ensembles}},
	isbn = {978-1-55860-707-1},
	url = {http://dl.acm.org/citation.cfm?id=645529.657816},
	urldate = {2016-12-10},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Evgeniou, Theodoros and Perez-Breva, Luis and Pontil, Massimiliano and Poggio, Tomaso},
	year = {2000},
	pages = {271--278}
}

@book{braun_spectral_2005,
	title = {Spectral {Properties} of the {Kernel} {Matrix} and {Their} {Relation} to {Kernel} {Methods} in {Machine} {Learning}},
	language = {en},
	author = {Braun, Mikio Ludwig},
	year = {2005},
	note = {Google-Books-ID: aMSaNwAACAAJ}
}

@article{braun_accurate_2006,
	title = {Accurate {Error} {Bounds} for the {Eigenvalues} of the {Kernel} {Matrix}},
	volume = {7},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v7/braun06a.html},
	number = {Nov},
	urldate = {2016-12-10},
	journal = {Journal of Machine Learning Research},
	author = {Braun, Mikio L.},
	year = {2006},
	pages = {2303--2328},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/XQ8RSN4H/Braun - 2006 - Accurate Error Bounds for the Eigenvalues of the K.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/4QE57397/braun06a.html:text/html}
}

@article{micchelli_error_2016,
	title = {Error bounds for learning the kernel},
	volume = {14},
	issn = {0219-5305},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0219530516400054},
	doi = {10.1142/S0219530516400054},
	abstract = {The problem of learning the kernel function has received considerable attention in machine learning. Much of the work has focused on kernel selection criteria, particularly on minimizing a regularized error functional over a prescribed set of kernels. Empirical studies indicate that this approach can enhance statistical performance and is computationally feasible. In this paper, we present a theoretical analysis of its generalization error. We establish for a wide variety of classes of kernels, such as the set of all multivariate Gaussian kernels, that this learning method generalizes well and, when the regularization parameter is appropriately chosen, it is consistent. A central role in our analysis is played by the interaction between the sample error and the approximation error.},
	number = {06},
	urldate = {2016-12-10},
	journal = {Analysis and Applications},
	author = {Micchelli, Charles A. and Pontil, Massimiliano and Wu, Qiang and Zhou, Ding-Xuan},
	month = sep,
	year = {2016},
	pages = {849--868},
	file = {Snapshot:/home/jeremiah/Zotero/storage/FZ4IWIFR/S0219530516400054.html:text/html}
}

@incollection{bousquet_complexity_2003,
	title = {On the {Complexity} of {Learning} the {Kernel} {Matrix}},
	url = {http://papers.nips.cc/paper/2300-on-the-complexity-of-learning-the-kernel-matrix.pdf},
	urldate = {2016-12-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 15},
	publisher = {MIT Press},
	author = {Bousquet, Olivier and Herrmann, Daniel},
	editor = {Becker, S. and Thrun, S. and Obermayer, K.},
	year = {2003},
	pages = {415--422},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/PACTNPPQ/Bousquet and Herrmann - 2003 - On the Complexity of Learning the Kernel Matrix.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/FSMXAV94/2300-on-the-complexity-of-learning-the-kernel-matrix.html:text/html}
}

@article{micchelli_learning_2005,
	title = {Learning the {Kernel} {Function} via {Regularization}},
	volume = {6},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v6/micchelli05a.html},
	number = {Jul},
	urldate = {2016-12-10},
	journal = {Journal of Machine Learning Research},
	author = {Micchelli, Charles A. and Pontil, Massimiliano},
	year = {2005},
	pages = {1099--1125},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/U5TPZMP5/Micchelli and Pontil - 2005 - Learning the Kernel Function via Regularization.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/AFZ2GIXZ/micchelli05a.html:text/html}
}

@inproceedings{lavalle_rapidly-exploring_2000,
	title = {Rapidly-{Exploring} {Random} {Trees}: {Progress} and {Prospects}},
	shorttitle = {Rapidly-{Exploring} {Random} {Trees}},
	abstract = {this paper, which presents randomized, algorithmic techniques for path planning that are particular suited for problems that involve dierential constraints.},
	booktitle = {Algorithmic and {Computational} {Robotics}: {New} {Directions}},
	author = {Lavalle, Steven M. and Kuffner, James J. and {Jr.}},
	year = {2000},
	pages = {293--308},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/FMXWBD8W/Lavalle et al. - 2000 - Rapidly-Exploring Random Trees Progress and Prosp.pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/QBM9KZI2/summary.html:text/html}
}

@article{thompson_covariance-adaptive_2010,
	title = {Covariance-{Adaptive} {Slice} {Sampling}},
	url = {http://arxiv.org/abs/1003.3201},
	abstract = {We describe two slice sampling methods for taking multivariate steps using the crumb framework. These methods use the gradients at rejected proposals to adapt to the local curvature of the log-density surface, a technique that can produce much better proposals when parameters are highly correlated. We evaluate our methods on four distributions and compare their performance to that of a non-adaptive slice sampling method and a Metropolis method. The adaptive methods perform favorably on low-dimensional target distributions with highly-correlated parameters.},
	urldate = {2016-12-05},
	journal = {arXiv:1003.3201 [stat]},
	author = {Thompson, Madeleine and Neal, Radford M.},
	month = mar,
	year = {2010},
	note = {arXiv: 1003.3201},
	keywords = {Statistics - Computation, 65C05}
}

@article{goldman_kolmogorov-smirnov_2008,
	title = {Kolmogorov-{Smirnov}, {Fluctuation}, and {Zg} {Tests} for {Convergence} of {Markov} {Chain} {Monte} {Carlo} {Draws}},
	volume = {37},
	issn = {03610918},
	url = {http://ezp-prod1.hul.harvard.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=28829557&site=ehost-live&scope=site},
	doi = {10.1080/03610910701792513},
	abstract = {We examine the sizes and powers of three tests of convergence of Markov Chain Monte Carlo draws: the Kolmogorov-Smirnov test, fluctuation test, and Geweke's test. We show that the sizes and powers are sensitive to the existence of autocorrelation in the draws. We propose a filtered test that is corrected for autocorrelation. We present a numerical illustration using the Federal funds rate.},
	number = {2},
	urldate = {2016-11-30},
	journal = {Communications in Statistics: Simulation \& Computation},
	author = {Goldman, Elena and Valiyeva, Elmira and Tsurumi, Hiroki},
	month = feb,
	year = {2008},
	keywords = {Stochastic processes, Autocorrelation (Statistics), Federal funds market (U.S.), Federal funds rates, Fluctuation test, Geweke's test, Kolmogorov-Smirnov test, Markov Chain Monte Carlo draws, Markov processes, Monte Carlo method, Numerical analysis, Simulation methods \& models},
	pages = {368--379},
	file = {EBSCO Full Text:/home/jeremiah/Zotero/storage/X99XIVCQ/Goldman et al. - 2008 - Kolmogorov-Smirnov, Fluctuation, and Zg Tests for .pdf:application/pdf}
}

@article{douglas_logic-gated_2012,
	title = {A {Logic}-{Gated} {Nanorobot} for {Targeted} {Transport} of {Molecular} {Payloads}},
	volume = {335},
	copyright = {Copyright © 2012, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/335/6070/831},
	doi = {10.1126/science.1214081},
	abstract = {We describe an autonomous DNA nanorobot capable of transporting molecular payloads to cells, sensing cell surface inputs for conditional, triggered activation, and reconfiguring its structure for payload delivery. The device can be loaded with a variety of materials in a highly organized fashion and is controlled by an aptamer-encoded logic gate, enabling it to respond to a wide array of cues. We implemented several different logical AND gates and demonstrate their efficacy in selective regulation of nanorobot function. As a proof of principle, nanorobots loaded with combinations of antibody fragments were used in two different types of cell-signaling stimulation in tissue culture. Our prototype could inspire new designs with different selectivities and biologically active payloads for cell-targeting tasks.
Cargoes stored in folded DNA origami are released when aptamers in the structure bind target protein molecules.
Cargoes stored in folded DNA origami are released when aptamers in the structure bind target protein molecules.},
	language = {en},
	number = {6070},
	urldate = {2016-11-29},
	journal = {Science},
	author = {Douglas, Shawn M. and Bachelet, Ido and Church, George M.},
	month = feb,
	year = {2012},
	pmid = {22344439},
	pages = {831--834},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/8I3KVKVS/Douglas et al. - 2012 - A Logic-Gated Nanorobot for Targeted Transport of .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/72BA6SC3/831.full.html:text/html}
}

@article{singh_programs_2016,
	title = {Programs as {Black}-{Box} {Explanations}},
	url = {http://arxiv.org/abs/1611.07579},
	abstract = {Recent work in model-agnostic explanations of black-box machine learning has demonstrated that interpretability of complex models does not have to come at the cost of accuracy or model flexibility. However, it is not clear what kind of explanations, such as linear models, decision trees, and rule lists, are the appropriate family to consider, and different tasks and models may benefit from different kinds of explanations. Instead of picking a single family of representations, in this work we propose to use "programs" as model-agnostic explanations. We show that small programs can be expressive yet intuitive as explanations, and generalize over a number of existing interpretable families. We propose a prototype program induction method based on simulated annealing that approximates the local behavior of black-box classifiers around a specific prediction using random perturbations. Finally, we present preliminary application on small datasets and show that the generated explanations are intuitive and accurate for a number of classifiers.},
	urldate = {2016-11-26},
	journal = {arXiv:1611.07579 [cs, stat]},
	author = {Singh, Sameer and Ribeiro, Marco Tulio and Guestrin, Carlos},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.07579},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems},
	file = {arXiv\:1611.07579 PDF:/home/jeremiah/Zotero/storage/AUHFGUSH/Singh et al. - 2016 - Programs as Black-Box Explanations.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/UI7CXPGW/1611.html:text/html}
}

@article{diaconis_gibbs_2008,
	title = {Gibbs {Sampling}, {Exponential} {Families} and {Orthogonal} {Polynomials}},
	volume = {23},
	issn = {0883-4237, 2168-8745},
	url = {http://projecteuclid.org/euclid.ss/1219339107},
	doi = {10.1214/07-STS252},
	abstract = {We give families of examples where sharp rates of convergence to stationarity of the widely used Gibbs sampler are available. The examples involve standard exponential families and their conjugate priors. In each case, the transition operator is explicitly diagonalizable with classical orthogonal polynomials as eigenfunctions.},
	language = {EN},
	number = {2},
	urldate = {2016-11-23},
	journal = {Statistical Science},
	author = {Diaconis, Persi and Khare, Kshitij and Saloff-Coste, Laurent},
	month = may,
	year = {2008},
	mrnumber = {MR2446500},
	zmnumber = {1327.62058},
	keywords = {conjugate priors, exponential families, Gibbs sampler, location families, orthogonal polynomials, running time analyses, singular value decomposition},
	pages = {151--178},
	file = {Snapshot:/home/jeremiah/Zotero/storage/2JEMQWFK/1219339107.html:text/html}
}

@article{geweke_getting_2004,
	title = {Getting {It} {Right}: {Joint} {Distribution} {Tests} of {Posterior} {Simulators}},
	volume = {99},
	issn = {0162-1459},
	shorttitle = {Getting {It} {Right}},
	url = {http://www.jstor.org/stable/27590449},
	abstract = {Analytical or coding errors in posterior simulators can produce reasonable but incorrect approximations of posterior moments. This article develops simple tests of posterior simulators that detect both kinds of errors, and uses them to detect and correct errors in two previously published articles. The tests exploit the fact that a Bayesian model specifies the joint distribution of observables (data) and unobservables (parameters). There are two joint distribution simulators. The marginal-conditional simulator draws unobservables from the prior and then observables conditional on unobservables. The successive-conditional simulator alternates between the posterior simulator and an observables simulator. Formal comparison of moment approximations of the two simulators reveals existing analytical or coding errors in the posterior simulator.},
	number = {467},
	urldate = {2016-11-17},
	journal = {Journal of the American Statistical Association},
	author = {Geweke, John},
	year = {2004},
	pages = {799--804}
}

@article{sorokina_modeling_2008,
	title = {Modeling {Additive} {Structure} and {Detecting} {Interactions} with {Groves} of {Trees}},
	url = {http://ecommons.cornell.edu/handle/1813/11176},
	abstract = {Discovery of additive structure is an important step towards understanding a complex multi-dimensional function, because it allows for expressing this function as the sum of lower-dimensional or otherwise simpler components.  Modeling additive structure also opens up opportunities for learning better regression models. 
 
The term statistical interaction is used to describe the presence of non-additive effects among two or more variables in a function. When variables interact, their effects must be modeled and interpreted simultaneously. Thus, detecting statistical interactions can be critical for an understanding of processes by domain researchers. 
 
This dissertation analyzes benefits of modelling additive structure for prediction and interaction detection problems. It describes a new learning algorithm called Groves, which is an ensemble of additive regression trees. Groves is based on such existing techniques as bagging and additive models; their combination allows us to use large trees in the ensemble and at the same time model additive structure of the response function. Regression version of the algorithm, Additive Groves, and its classification counterpart, Gradient Groves, yield consistently high performance across a variety of problems, outperforming on average a large number of other algorithms. 
 
Additive nature of Groves makes it particularly useful for interaction detection. This dissertation introduces a new approach to interaction detection: it is based on comparing the performance of restricted and unrestricted predictive models. Groves of trees allow variable interactions to be carefully controlled and therefore are especially useful for this framework.  
 
The details of proposed practical approach to interaction detection analysis are demonstrated on real data describing the abundance of different species of birds in the prairies east of the southern Rocky Mountains.},
	language = {en\_US},
	urldate = {2016-11-06},
	author = {Sorokina, Daria},
	month = jul,
	year = {2008},
	file = {Snapshot:/home/jeremiah/Zotero/storage/Z6SXMI6H/Sorokina - 2008 - Modeling Additive Structure and Detecting Interact.html:text/html}
}

@article{wilson_stochastic_2016,
	title = {Stochastic {Variational} {Deep} {Kernel} {Learning}},
	url = {http://arxiv.org/abs/1611.00336},
	abstract = {Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.},
	urldate = {2016-11-06},
	journal = {arXiv:1611.00336 [cs, stat]},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.00336},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology},
	annote = {Comment: 13 pages, 6 tables, 3 figures. Appearing in NIPS 2016},
	file = {arXiv\:1611.00336 PDF:/home/jeremiah/Zotero/storage/D7SZTMQC/Wilson et al. - 2016 - Stochastic Variational Deep Kernel Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/RU2W82T7/1611.html:text/html}
}

@article{davies_random_2014,
	title = {The {Random} {Forest} {Kernel} and other kernels for big data from random partitions},
	url = {http://arxiv.org/abs/1402.4293},
	abstract = {We present Random Partition Kernels, a new class of kernels derived by demonstrating a natural connection between random partitions of objects and kernels between those objects. We show how the construction can be used to create kernels from methods that would not normally be viewed as random partitions, such as Random Forest. To demonstrate the potential of this method, we propose two new kernels, the Random Forest Kernel and the Fast Cluster Kernel, and show that these kernels consistently outperform standard kernels on problems involving real-world datasets. Finally, we show how the form of these kernels lend themselves to a natural approximation that is appropriate for certain big data problems, allowing \$O(N)\$ inference in methods such as Gaussian Processes, Support Vector Machines and Kernel PCA.},
	urldate = {2016-11-06},
	journal = {arXiv:1402.4293 [cs, stat]},
	author = {Davies, Alex and Ghahramani, Zoubin},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.4293},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1402.4293 PDF:/home/jeremiah/Zotero/storage/CWFA4FW7/Davies and Ghahramani - 2014 - The Random Forest Kernel and other kernels for big.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/4KZG346K/1402.html:text/html}
}

@article{balog_mondrian_2016,
	title = {The {Mondrian} {Kernel}},
	url = {http://arxiv.org/abs/1606.05241},
	abstract = {We introduce the Mondrian kernel, a fast random feature approximation to the Laplace kernel. It is suitable for both batch and online learning, and admits a fast kernel-width-selection procedure as the random features can be re-used efficiently for all kernel widths. The features are constructed by sampling trees via a Mondrian process [Roy and Teh, 2009], and we highlight the connection to Mondrian forests [Lakshminarayanan et al., 2014], where trees are also sampled via a Mondrian process, but fit independently. This link provides a new insight into the relationship between kernel methods and random forests.},
	urldate = {2016-11-06},
	journal = {arXiv:1606.05241 [stat]},
	author = {Balog, Matej and Lakshminarayanan, Balaji and Ghahramani, Zoubin and Roy, Daniel M. and Teh, Yee Whye},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.05241},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: Accepted for presentation at the 32nd Conference on Uncertainty in Artificial Intelligence (UAI 2016)},
	file = {arXiv\:1606.05241 PDF:/home/jeremiah/Zotero/storage/KC9G86UJ/Balog et al. - 2016 - The Mondrian Kernel.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/XX9CDF2E/1606.html:text/html}
}

@article{scornet_random_2015,
	title = {Random forests and kernel methods},
	url = {http://arxiv.org/abs/1502.03836},
	abstract = {Random forests are ensemble methods which grow trees as base learners and combine their predictions by averaging. Random forests are known for their good practical performance, particularly in high dimensional set-tings. On the theoretical side, several studies highlight the potentially fruitful connection between random forests and kernel methods. In this paper, we work out in full details this connection. In particular, we show that by slightly modifying their definition, random forests can be rewrit-ten as kernel methods (called KeRF for Kernel based on Random Forests) which are more interpretable and easier to analyze. Explicit expressions of KeRF estimates for some specific random forest models are given, together with upper bounds on their rate of consistency. We also show empirically that KeRF estimates compare favourably to random forest estimates.},
	urldate = {2016-11-05},
	journal = {arXiv:1502.03836 [math, stat]},
	author = {Scornet, Erwan},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03836},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv\:1502.03836 PDF:/home/jeremiah/Zotero/storage/PAUCI4HK/Scornet - 2015 - Random forests and kernel methods.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/DDQPJWJS/1502.html:text/html}
}

@article{mentch_quantifying_2014,
	title = {Quantifying {Uncertainty} in {Random} {Forests} via {Confidence} {Intervals} and {Hypothesis} {Tests}},
	url = {http://arxiv.org/abs/1404.6473},
	abstract = {This work develops formal statistical inference procedures for machine learning ensemble methods. Ensemble methods based on bootstrapping, such as bagging and random forests, have improved the predictive accuracy of individual trees, but fail to provide a framework in which distributional results can be easily determined. Instead of aggregating full bootstrap samples, we consider predicting by averaging over trees built on subsamples of the training set and demonstrate that the resulting estimator takes the form of a U-statistic. As such, predictions for individual feature vectors are asymptotically normal, allowing for confidence intervals to accompany predictions. In practice, a subset of subsamples is used for computational speed; here our estimators take the form of incomplete U-statistics and equivalent results are derived. We further demonstrate that this setup provides a framework for testing the significance of features. Moreover, the internal estimation method we develop allows us to estimate the variance parameters and perform these inference procedures at no additional computational cost. Simulations and illustrations on a real dataset are provided.},
	urldate = {2016-11-04},
	journal = {arXiv:1404.6473 [stat]},
	author = {Mentch, Lucas and Hooker, Giles},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.6473},
	keywords = {Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology, Statistics - Applications},
	annote = {Comment: To appear in The Journal of Machine Learning Research},
	file = {arXiv\:1404.6473 PDF:/home/jeremiah/Zotero/storage/C834G6QH/Mentch and Hooker - 2014 - Quantifying Uncertainty in Random Forests via Conf.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/FGIAW738/1404.html:text/html}
}

@article{blanchard_statistical_2006,
	title = {Statistical properties of kernel principal component analysis},
	volume = {66},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1007/s10994-006-6895-9},
	doi = {10.1007/s10994-006-6895-9},
	abstract = {The main goal of this paper is to prove inequalities on the reconstruction error for kernel principal component analysis. With respect to previous work on this topic, our contribution is twofold: (1) we give bounds that explicitly take into account the empirical centering step in this algorithm, and (2) we show that a “localized” approach allows to obtain more accurate bounds. In particular, we show faster rates of convergence towards the minimum reconstruction error; more precisely, we prove that the convergence rate can typically be faster than n−1/2. We also obtain a new relative bound on the error.A secondary goal, for which we present similar contributions, is to obtain convergence bounds for the partial sums of the biggest or smallest eigenvalues of the kernel Gram matrix towards eigenvalues of the corresponding kernel operator. These quantities are naturally linked to the KPCA procedure; furthermore these results can have applications to the study of various other kernel algorithms.The results are presented in a functional analytic framework, which is suited to deal rigorously with reproducing kernel Hilbert spaces of infinite dimension.},
	language = {en},
	number = {2-3},
	urldate = {2016-11-04},
	journal = {Machine Learning},
	author = {Blanchard, Gilles and Bousquet, Olivier and Zwald, Laurent},
	month = mar,
	year = {2006},
	pages = {259--294},
	file = {Snapshot:/home/jeremiah/Zotero/storage/RUA229KR/s10994-006-6895-9.html:text/html}
}

@article{blanchard_statistical_2006-1,
	title = {Statistical properties of kernel principal component analysis},
	volume = {66},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1007/s10994-006-6895-9},
	doi = {10.1007/s10994-006-6895-9},
	abstract = {The main goal of this paper is to prove inequalities on the reconstruction error for kernel principal component analysis. With respect to previous work on this topic, our contribution is twofold: (1) we give bounds that explicitly take into account the empirical centering step in this algorithm, and (2) we show that a “localized” approach allows to obtain more accurate bounds. In particular, we show faster rates of convergence towards the minimum reconstruction error; more precisely, we prove that the convergence rate can typically be faster than n−1/2. We also obtain a new relative bound on the error.A secondary goal, for which we present similar contributions, is to obtain convergence bounds for the partial sums of the biggest or smallest eigenvalues of the kernel Gram matrix towards eigenvalues of the corresponding kernel operator. These quantities are naturally linked to the KPCA procedure; furthermore these results can have applications to the study of various other kernel algorithms.The results are presented in a functional analytic framework, which is suited to deal rigorously with reproducing kernel Hilbert spaces of infinite dimension.},
	language = {en},
	number = {2-3},
	urldate = {2016-11-04},
	journal = {Machine Learning},
	author = {Blanchard, Gilles and Bousquet, Olivier and Zwald, Laurent},
	month = mar,
	year = {2006},
	pages = {259--294},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/9PR6S6X8/Blanchard et al. - 2006 - Statistical properties of kernel principal compone.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/CJ4PRNWD/s10994-006-6895-9.html:text/html}
}

@article{koltchinskii_random_2000,
	title = {Random matrix approximation of spectra of integral operators},
	volume = {6},
	issn = {1350-7265},
	url = {http://projecteuclid.org/euclid.bj/1082665383},
	abstract = {LetH:L2(S,calS,P)→L2(S,calS,P)H:L2(S,calS,P)→L2(S,calS,P){\textless}math alttext="\$H:L\_2(S,\{{\textbackslash}cal S\},P) {\textbackslash}rightarrow L\_2(S,\{{\textbackslash}cal S\},P)\$" overflow="scroll"{\textgreater} {\textless}mi{\textgreater}H{\textless}/mi{\textgreater} {\textless}mo{\textgreater}:{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater} {\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}S{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mrow{\textgreater}{\textless}mo rspace="thinmathspace" lspace="0em"{\textgreater}cal{\textless}/mo{\textgreater} {\textless}mi{\textgreater}S{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}P{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}mo{\textgreater}→{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater} {\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}S{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mrow{\textgreater}{\textless}mo rspace="thinmathspace" lspace="0em"{\textgreater}cal{\textless}/mo{\textgreater} {\textless}mi{\textgreater}S{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}P{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater} {\textless}/math{\textgreater} be a compact integral operator with a symmetric kernel h. LetXi,i∈NXi,i∈N{\textless}math alttext="\$\{X\_i,{\textbackslash} i{\textbackslash}in{\textbackslash}N\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}X{\textless}/mi{\textgreater} {\textless}mi{\textgreater}i{\textless}/mi{\textgreater}{\textless}/msub{\textgreater} {\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}i{\textless}/mi{\textgreater}{\textless}mo{\textgreater}∈{\textless}/mo{\textgreater}{\textless}mo rspace="thinmathspace" lspace="0em"{\textgreater}N{\textless}/mo{\textgreater}{\textless}/mrow{\textgreater} {\textless}/math{\textgreater} , be independent S-valued random variables with common probability law P. Consider the n×n matrixH˜nH˜n{\textless}math alttext="\$\{{\textbackslash}tilde \{H\}\_n\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mover{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}H{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}mo{\textgreater}˜{\textless}/mo{\textgreater}{\textless}/mover{\textgreater} {\textless}mi{\textgreater}n{\textless}/mi{\textgreater}{\textless}/msub{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} with entriesn−1h(Xi,Xj),1≤i,j≤nn-1h(Xi,Xj),1≤i,j≤n{\textless}math alttext="\$\{n{\textasciicircum}\{-1\}h(X\_i, X\_j),{\textbackslash} 1{\textbackslash}leq i,j{\textbackslash}leq n\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msup{\textgreater}{\textless}mi{\textgreater}n{\textless}/mi{\textgreater} {\textless}mrow{\textgreater}{\textless}mo rspace="0em" lspace="thinthinmathspace"{\textgreater}-{\textless}/mo{\textgreater} {\textless}mn{\textgreater}1{\textless}/mn{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msup{\textgreater} {\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}X{\textless}/mi{\textgreater} {\textless}mi{\textgreater}i{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}X{\textless}/mi{\textgreater} {\textless}mi{\textgreater}j{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mn{\textgreater}1{\textless}/mn{\textgreater}{\textless}mo{\textgreater}≤{\textless}/mo{\textgreater}{\textless}mi{\textgreater}i{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}j{\textless}/mi{\textgreater}{\textless}mo{\textgreater}≤{\textless}/mo{\textgreater}{\textless}mi{\textgreater}n{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater} {\textless}/math{\textgreater} (this is the matrix of an empirical version of the operator H with P replaced by the empirical measure Pn), and let Hn denote the modification ofH˜n,H˜n,{\textless}math alttext="\$\{{\textbackslash}tilde H\_n,\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mover{\textgreater}{\textless}mi{\textgreater}H{\textless}/mi{\textgreater}{\textless}mo{\textgreater}˜{\textless}/mo{\textgreater}{\textless}/mover{\textgreater} {\textless}mi{\textgreater}n{\textless}/mi{\textgreater}{\textless}/msub{\textgreater} {\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}/mrow{\textgreater} {\textless}/math{\textgreater} obtained by deleting its diagonal. It is proved that theℓ2ℓ2{\textless}math alttext="\$\{{\textbackslash}ell\_2\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mo{\textgreater}ℓ{\textless}/mo{\textgreater} {\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} distance between the ordered spectrum of Hn and the ordered spectrum of H tends to zero a.s. if and only if H is Hilbert-Schmidt. Rates of convergence and distributional limit theorems for the difference between the ordered spectra of the operators Hn (orH˜nH˜n{\textless}math alttext="\$\{{\textbackslash}tilde H\_n\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mover{\textgreater}{\textless}mi{\textgreater}H{\textless}/mi{\textgreater}{\textless}mo{\textgreater}˜{\textless}/mo{\textgreater}{\textless}/mover{\textgreater} {\textless}mi{\textgreater}n{\textless}/mi{\textgreater}{\textless}/msub{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} ) and H are also obtained under somewhat stronger conditions. These results apply in particular to the kernels of certain functionsH=varphi(L)H=varphi(L){\textless}math alttext="\$\{H={\textbackslash}varphi (L)\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}mi{\textgreater}H{\textless}/mi{\textgreater} {\textless}mo{\textgreater}={\textless}/mo{\textgreater}{\textless}mo rspace="thinmathspace" lspace="0em"{\textgreater}varphi{\textless}/mo{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}/mrow{\textgreater} {\textless}/math{\textgreater} of partial differential operators L (heat kernels, Green functions).},
	number = {1},
	urldate = {2016-11-04},
	journal = {Bernoulli},
	author = {Koltchinskii, Vladimir and Giné, Evarist},
	month = feb,
	year = {2000},
	mrnumber = {MR2001e:47080},
	zmnumber = {0949.60078},
	keywords = {eigenvalues, heat kernels, integral operators, limit theorems, random matrices},
	pages = {113--167},
	file = {Snapshot:/home/jeremiah/Zotero/storage/TX4W9SZ2/1082665383.html:text/html}
}

@article{koltchinskii_random_2000-1,
	title = {Random matrix approximation of spectra of integral operators},
	volume = {6},
	issn = {1350-7265},
	url = {http://projecteuclid.org/euclid.bj/1082665383},
	abstract = {LetH:L2(S,calS,P)→L2(S,calS,P)H:L2(S,calS,P)→L2(S,calS,P){\textless}math alttext="\$H:L\_2(S,\{{\textbackslash}cal S\},P) {\textbackslash}rightarrow L\_2(S,\{{\textbackslash}cal S\},P)\$" overflow="scroll"{\textgreater} {\textless}mi{\textgreater}H{\textless}/mi{\textgreater} {\textless}mo{\textgreater}:{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater} {\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}S{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mrow{\textgreater}{\textless}mo rspace="thinmathspace" lspace="0em"{\textgreater}cal{\textless}/mo{\textgreater} {\textless}mi{\textgreater}S{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}P{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}mo{\textgreater}→{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater} {\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}S{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mrow{\textgreater}{\textless}mo rspace="thinmathspace" lspace="0em"{\textgreater}cal{\textless}/mo{\textgreater} {\textless}mi{\textgreater}S{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}P{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater} {\textless}/math{\textgreater} be a compact integral operator with a symmetric kernel h. LetXi,i∈NXi,i∈N{\textless}math alttext="\$\{X\_i,{\textbackslash} i{\textbackslash}in{\textbackslash}N\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}X{\textless}/mi{\textgreater} {\textless}mi{\textgreater}i{\textless}/mi{\textgreater}{\textless}/msub{\textgreater} {\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}i{\textless}/mi{\textgreater}{\textless}mo{\textgreater}∈{\textless}/mo{\textgreater}{\textless}mo rspace="thinmathspace" lspace="0em"{\textgreater}N{\textless}/mo{\textgreater}{\textless}/mrow{\textgreater} {\textless}/math{\textgreater} , be independent S-valued random variables with common probability law P. Consider the n×n matrixH˜nH˜n{\textless}math alttext="\$\{{\textbackslash}tilde \{H\}\_n\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mover{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}H{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}mo{\textgreater}˜{\textless}/mo{\textgreater}{\textless}/mover{\textgreater} {\textless}mi{\textgreater}n{\textless}/mi{\textgreater}{\textless}/msub{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} with entriesn−1h(Xi,Xj),1≤i,j≤nn-1h(Xi,Xj),1≤i,j≤n{\textless}math alttext="\$\{n{\textasciicircum}\{-1\}h(X\_i, X\_j),{\textbackslash} 1{\textbackslash}leq i,j{\textbackslash}leq n\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msup{\textgreater}{\textless}mi{\textgreater}n{\textless}/mi{\textgreater} {\textless}mrow{\textgreater}{\textless}mo rspace="0em" lspace="thinthinmathspace"{\textgreater}-{\textless}/mo{\textgreater} {\textless}mn{\textgreater}1{\textless}/mn{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msup{\textgreater} {\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}X{\textless}/mi{\textgreater} {\textless}mi{\textgreater}i{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}X{\textless}/mi{\textgreater} {\textless}mi{\textgreater}j{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mn{\textgreater}1{\textless}/mn{\textgreater}{\textless}mo{\textgreater}≤{\textless}/mo{\textgreater}{\textless}mi{\textgreater}i{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}j{\textless}/mi{\textgreater}{\textless}mo{\textgreater}≤{\textless}/mo{\textgreater}{\textless}mi{\textgreater}n{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater} {\textless}/math{\textgreater} (this is the matrix of an empirical version of the operator H with P replaced by the empirical measure Pn), and let Hn denote the modification ofH˜n,H˜n,{\textless}math alttext="\$\{{\textbackslash}tilde H\_n,\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mover{\textgreater}{\textless}mi{\textgreater}H{\textless}/mi{\textgreater}{\textless}mo{\textgreater}˜{\textless}/mo{\textgreater}{\textless}/mover{\textgreater} {\textless}mi{\textgreater}n{\textless}/mi{\textgreater}{\textless}/msub{\textgreater} {\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}/mrow{\textgreater} {\textless}/math{\textgreater} obtained by deleting its diagonal. It is proved that theℓ2ℓ2{\textless}math alttext="\$\{{\textbackslash}ell\_2\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mo{\textgreater}ℓ{\textless}/mo{\textgreater} {\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} distance between the ordered spectrum of Hn and the ordered spectrum of H tends to zero a.s. if and only if H is Hilbert-Schmidt. Rates of convergence and distributional limit theorems for the difference between the ordered spectra of the operators Hn (orH˜nH˜n{\textless}math alttext="\$\{{\textbackslash}tilde H\_n\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mover{\textgreater}{\textless}mi{\textgreater}H{\textless}/mi{\textgreater}{\textless}mo{\textgreater}˜{\textless}/mo{\textgreater}{\textless}/mover{\textgreater} {\textless}mi{\textgreater}n{\textless}/mi{\textgreater}{\textless}/msub{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} ) and H are also obtained under somewhat stronger conditions. These results apply in particular to the kernels of certain functionsH=varphi(L)H=varphi(L){\textless}math alttext="\$\{H={\textbackslash}varphi (L)\}\$" overflow="scroll"{\textgreater} {\textless}mrow{\textgreater}{\textless}mi{\textgreater}H{\textless}/mi{\textgreater} {\textless}mo{\textgreater}={\textless}/mo{\textgreater}{\textless}mo rspace="thinmathspace" lspace="0em"{\textgreater}varphi{\textless}/mo{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}/mrow{\textgreater} {\textless}/math{\textgreater} of partial differential operators L (heat kernels, Green functions).},
	number = {1},
	urldate = {2016-11-04},
	journal = {Bernoulli},
	author = {Koltchinskii, Vladimir and Giné, Evarist},
	month = feb,
	year = {2000},
	mrnumber = {MR2001e:47080},
	zmnumber = {0949.60078},
	keywords = {eigenvalues, heat kernels, integral operators, limit theorems, random matrices},
	pages = {113--167},
	file = {Snapshot:/home/jeremiah/Zotero/storage/DQ76PF2R/Koltchinskii and Giné - 2000 - Random matrix approximation of spectra of integral.html:text/html}
}

@article{roberts_geometric_1996,
	title = {Geometric convergence and central limit theorems for multidimensional {Hastings} and {Metropolis} algorithms},
	volume = {83},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org.ezp-prod1.hul.harvard.edu/content/83/1/95},
	doi = {10.1093/biomet/83.1.95},
	abstract = {We develop results on geometric ergodicity of Markov chains and apply these and other recent results in Markov chain theory to multidimensional Hastings and Metropolis algorithms. For those based on random walk candidate distributions, we find sufficient conditions for moments and moment generating functions to converge at a geometric rate to a prescribed distribution π. By phrasing the conditions in terms of the curvature of the densities we show that the results apply to all distributions with positive densities in a large class which encompasses many commonly-used statistical forms. From these results we develop central limit theorems for the Metropolis algorithm. Converse results, showing non-geometric convergence rates for chains where the rejection rate is not bounded away from unity, are also given; these show that the negative-definiteness property is not redundant.},
	language = {en},
	number = {1},
	urldate = {2016-10-31},
	journal = {Biometrika},
	author = {Roberts, G. O. and Tweedie, R. L.},
	month = mar,
	year = {1996},
	keywords = {Markov chain Monte Carlo, Geometric ergodicity, Gibbs sampling Hastings algorithm, Irreducible Markov process, Metropolis algorithm, posterior distribution},
	pages = {95--110},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/C57UBSR6/Roberts and Tweedie - 1996 - Geometric convergence and central limit theorems f.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/PPXH6NXK/95.html:text/html}
}

@article{roberts_exponential_1996,
	title = {Exponential convergence of {Langevin} distributions and their discrete approximations},
	volume = {2},
	issn = {1350-7265},
	url = {http://projecteuclid.org/euclid.bj/1178291835},
	abstract = {In this paper we consider a continuous-time method of approximating a given distribution ππ{\textless}math overflow="scroll"{\textgreater} {\textless}mi{\textgreater}π{\textless}/mi{\textgreater} {\textless}/math{\textgreater} using the Langevin diffusion dLt=dWt+12∇logπ(Lt)dtdLt=dWt+12∇logπ(Lt)dt{\textless}math overflow="scroll"{\textgreater} {\textless}mo{\textgreater}d{\textless}/mo{\textgreater} {\textless}msub{\textgreater}{\textless}mstyle fontweight="bold"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}/mstyle{\textgreater} {\textless}mi{\textgreater}t{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}={\textless}/mo{\textgreater}{\textless}mo{\textgreater}d{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mstyle fontweight="bold"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}W{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}/mstyle{\textgreater} {\textless}mi{\textgreater}t{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}+{\textless}/mo{\textgreater}{\textless}mfrac{\textgreater}{\textless}mrow{\textgreater}{\textless}mn{\textgreater}1{\textless}/mn{\textgreater} {\textless}/mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mn{\textgreater}2{\textless}/mn{\textgreater} {\textless}/mrow{\textgreater}{\textless}/mfrac{\textgreater}{\textless}mo{\textgreater}∇{\textless}/mo{\textgreater}{\textless}mo{\textgreater}log{\textless}/mo{\textgreater}{\textless}mi{\textgreater}π{\textless}/mi{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mstyle fontweight="bold"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}/mstyle{\textgreater} {\textless}mi{\textgreater}t{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}mo{\textgreater}d{\textless}/mo{\textgreater}{\textless}mi{\textgreater}t{\textless}/mi{\textgreater} {\textless}/math{\textgreater}. We find conditions under which this diffusion converges exponentially quickly to ππ{\textless}math overflow="scroll"{\textgreater} {\textless}mi{\textgreater}π{\textless}/mi{\textgreater} {\textless}/math{\textgreater} or does not: in one dimension, these are essentially that for distributions with exponential tails of the form π(x)∝exp(−γ∣∣x∣∣β)π(x)∝exp(-γ{\textbar}x{\textbar}β){\textless}math overflow="scroll"{\textgreater} {\textless}mi{\textgreater}π{\textless}/mi{\textgreater} {\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}x{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}mo{\textgreater}∝{\textless}/mo{\textgreater}{\textless}mo{\textgreater}exp{\textless}/mo{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mo{\textgreater}-{\textless}/mo{\textgreater}{\textless}mi{\textgreater}γ{\textless}/mi{\textgreater}{\textless}mo{\textgreater}{\textbar}{\textless}/mo{\textgreater}{\textless}mi{\textgreater}x{\textless}/mi{\textgreater}{\textless}msup{\textgreater}{\textless}mo{\textgreater}{\textbar}{\textless}/mo{\textgreater} {\textless}mrow{\textgreater}{\textless}mi{\textgreater}β{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}/msup{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater} {\textless}/math{\textgreater}, 0{\textless}β{\textless}∞0{\textless}β{\textless}∞{\textless}math overflow="scroll"{\textgreater} {\textless}mn{\textgreater}0{\textless}/mn{\textgreater} {\textless}mi{\textgreater}\&lt;{\textless}/mi{\textgreater}{\textless}mi{\textgreater}β{\textless}/mi{\textgreater}{\textless}mi{\textgreater}\&lt;{\textless}/mi{\textgreater}{\textless}mn{\textgreater}∞{\textless}/mn{\textgreater} {\textless}/math{\textgreater}, exponential convergence occurs if and only if β≥1β≥1{\textless}math overflow="scroll"{\textgreater} {\textless}mi{\textgreater}β{\textless}/mi{\textgreater} {\textless}mo{\textgreater}≥{\textless}/mo{\textgreater}{\textless}mn{\textgreater}1{\textless}/mn{\textgreater} {\textless}/math{\textgreater}. We then consider conditions under which the discrete approximations to the diffusion converge. We first show that even when the diffusion itself converges, naive discretizations need not do so. We then consider a 'Metropolis-adjusted' version of the algorithm, and find conditions under which this also converges at an exponential rate: perhaps surprisingly, even the Metropolized version need not converge exponentially fast even if the diffusion does. We briefly discuss a truncated form of the algorithm which, in practice, should avoid the difficulties of the other forms.},
	language = {EN},
	number = {4},
	urldate = {2016-10-31},
	journal = {Bernoulli},
	author = {Roberts, Gareth O. and Tweedie, Richard L.},
	month = dec,
	year = {1996},
	mrnumber = {MR1440273},
	zmnumber = {0870.60027},
	keywords = {Markov chain Monte Carlo, Geometric ergodicity, diffusions, discrete approximations, Hastings algorithms, irreducible Markov processes, Langevin models, Metropolis algorithms, posterior distributions},
	pages = {341--363},
	file = {Snapshot:/home/jeremiah/Zotero/storage/JG7T5GCI/1178291835.html:text/html}
}

@article{mengersen_rates_1996,
	title = {Rates of {Convergence} of the {Hastings} and {Metropolis} {Algorithms}},
	volume = {24},
	issn = {0090-5364},
	url = {http://www.jstor.org/stable/2242610},
	abstract = {We apply recent results in Markov chain theory to Hastings and Metropolis algorithms with either independent or symmetric candidate distributions, and provide necessary and sufficient conditions for the algorithms to converge at a geometric rate to a prescribed distribution π. In the independence case (in Rk) these indicate that geometric convergence essentially occurs if and only if the candidate density is bounded below by a multiple of π; in the symmetric case (in R only) we show geometric convergence essentially occurs if and only if π has geometric tails. We also evaluate recently developed computable bounds on the rates of convergence in this context: examples show that these theoretical bounds can be inherently extremely conservative, although when the chain is stochastically monotone the bounds may well be effective.},
	number = {1},
	urldate = {2016-10-31},
	journal = {The Annals of Statistics},
	author = {Mengersen, K. L. and Tweedie, R. L.},
	year = {1996},
	pages = {101--121}
}

@article{tang_deep_2012,
	title = {Deep {Mixtures} of {Factor} {Analysers}},
	url = {http://arxiv.org/abs/1206.4635},
	abstract = {An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets.},
	urldate = {2016-10-28},
	journal = {arXiv:1206.4635 [cs, stat]},
	author = {Tang, Yichuan and Salakhutdinov, Ruslan and Hinton, Geoffrey},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.4635},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: ICML2012},
	file = {arXiv\:1206.4635 PDF:/home/jeremiah/Zotero/storage/X9ZHCIHC/Tang et al. - 2012 - Deep Mixtures of Factor Analysers.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/NB9P6V6H/1206.html:text/html}
}

@inproceedings{gan_scalable_2015,
	title = {Scalable {Deep} {Poisson} {Factor} {Analysis} for {Topic} {Modeling}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2015_gan15},
	urldate = {2016-10-28},
	author = {Gan, Zhe and Chen, Changyou and Henao, Ricardo and Carlson, David and Carin, Lawrence},
	year = {2015},
	pages = {1823--1832},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/H9N5U4TD/Gan et al. - 2015 - Scalable Deep Poisson Factor Analysis for Topic Mo.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/ZFTE2Z6M/icml2015_gan15.html:text/html}
}

@article{ranganath_deep_2014,
	title = {Deep {Exponential} {Families}},
	url = {http://arxiv.org/abs/1411.2581},
	abstract = {We describe {\textbackslash}textit\{deep exponential families\} (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent "black box" variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show that going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models.},
	urldate = {2016-10-27},
	journal = {arXiv:1411.2581 [cs, stat]},
	author = {Ranganath, Rajesh and Tang, Linpeng and Charlin, Laurent and Blei, David M.},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.2581},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1411.2581 PDF:/home/jeremiah/Zotero/storage/CW27TA6K/Ranganath et al. - 2014 - Deep Exponential Families.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/NI7XWG8G/1411.html:text/html}
}

@article{holmes_curvature_2014,
	title = {Curvature and {Concentration} of {Hamiltonian} {Monte} {Carlo} in {High} {Dimensions}},
	url = {http://arxiv.org/abs/1407.1114},
	abstract = {In this article, we analyze Hamiltonian Monte Carlo (HMC) by placing it in the setting of Riemannian geometry using the Jacobi metric, so that each step corresponds to a geodesic on a suitable Riemannian manifold. We then combine the notion of curvature of a Markov chain due to Joulin and Ollivier with the classical sectional curvature from Riemannian geometry to derive error bounds for HMC in important cases, where we have positive curvature. These cases include several classical distributions such as multivariate Gaussians, and also distributions arising in the study of Bayesian image registration. The theoretical development suggests the sectional curvature as a new diagnostic tool for convergence for certain Markov chains.},
	urldate = {2016-10-27},
	journal = {arXiv:1407.1114 [math, stat]},
	author = {Holmes, Susan and Rubinstein-Salzedo, Simon and Seiler, Christof},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.1114},
	keywords = {Mathematics - Statistics Theory, Mathematics - Probability, Mathematics - Differential Geometry},
	annote = {Comment: Comments welcome},
	file = {arXiv\:1407.1114 PDF:/home/jeremiah/Zotero/storage/RT6CMUQJ/Holmes et al. - 2014 - Curvature and Concentration of Hamiltonian Monte C.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/ZPRECMU2/1407.html:text/html}
}

@article{livingstone_geometric_2016,
	title = {On the {Geometric} {Ergodicity} of {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1601.08057},
	abstract = {We establish general conditions under which Markov chains produced by the Hamiltonian Monte Carlo method will and will not be geometrically ergodic. We consider implementations with both position-independent and position-dependent integration times. In the former case we find that the conditions for geometric ergodicity are essentially a non-vanishing gradient of the log-density which asymptotically points towards the centre of the space and does not grow faster than linearly. In an idealised scenario in which the integration time is allowed to change in different regions of the space, we show that geometric ergodicity can be recovered for a much broader class of tail behaviours, leading to some guidelines for the choice of this free parameter in practice.},
	urldate = {2016-10-27},
	journal = {arXiv:1601.08057 [stat]},
	author = {Livingstone, Samuel and Betancourt, Michael and Byrne, Simon and Girolami, Mark},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.08057},
	keywords = {Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: 31 pages, 1 figure},
	file = {arXiv\:1601.08057 PDF:/home/jeremiah/Zotero/storage/TFW762AD/Livingstone et al. - 2016 - On the Geometric Ergodicity of Hamiltonian Monte C.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/3267W35D/1601.html:text/html}
}

@article{bolley_convergence_2011,
	title = {Convergence to equilibrium in {Wasserstein} distance for {Fokker}-{Planck} equations},
	volume = {222},
	issn = {01664328},
	url = {http://arxiv.org/abs/1110.3606},
	doi = {10.1016/j.bbr.2011.03.031},
	abstract = {We describe conditions on non-gradient drift diffusion Fokker-Planck equations for its solutions to converge to equilibrium with a uniform exponential rate in Wasserstein distance. This asymptotic behaviour is related to a functional inequality, which links the distance with its dissipation and ensures a spectral gap in Wasserstein distance. We give practical criteria for this inequality and compare it to classical ones. The key point is to quantify the contribution of the diffusion term to the rate of convergence, which to our knowledge is a novelty.},
	number = {1},
	urldate = {2016-10-27},
	journal = {Behavioural Brain Research},
	author = {Bolley, François and Gentil, Ivan and Guillin, Arnaud},
	month = sep,
	year = {2011},
	note = {arXiv: 1110.3606},
	keywords = {Mathematics - Probability},
	pages = {1--9},
	file = {arXiv\:1110.3606 PDF:/home/jeremiah/Zotero/storage/CXFKHAMM/Bolley et al. - 2011 - Convergence to equilibrium in Wasserstein distance.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/58ZANFKH/1110.html:text/html}
}

@article{leimkuhler_adaptive_2016,
	title = {Adaptive {Thermostats} for {Noisy} {Gradient} {Systems}},
	volume = {38},
	issn = {1064-8275, 1095-7197},
	url = {http://arxiv.org/abs/1505.06889},
	doi = {10.1137/15M102318X},
	abstract = {We study numerical methods for sampling probability measures in high dimension where the underlying model is only approximately identified with a gradient system. Extended stochastic dynamical methods are discussed which have application to multiscale models, nonequilibrium molecular dynamics, and Bayesian sampling techniques arising in emerging machine learning applications. In addition to providing a more comprehensive discussion of the foundations of these methods, we propose a new numerical method for the adaptive Langevin/stochastic gradient Nos{\textbackslash}'\{e\}--Hoover thermostat that achieves a dramatic improvement in numerical efficiency over the most popular stochastic gradient methods reported in the literature. We also demonstrate that the newly established method inherits a superconvergence property (fourth order convergence to the invariant measure for configurational quantities) recently demonstrated in the setting of Langevin dynamics. Our findings are verified by numerical experiments.},
	number = {2},
	urldate = {2016-10-27},
	journal = {SIAM Journal on Scientific Computing},
	author = {Leimkuhler, Benedict and Shang, Xiaocheng},
	month = jan,
	year = {2016},
	note = {arXiv: 1505.06889},
	keywords = {Condensed Matter - Statistical Mechanics, Physics - Computational Physics, Mathematics - Numerical Analysis, Physics - Chemical Physics},
	pages = {A712--A736},
	file = {arXiv\:1505.06889 PDF:/home/jeremiah/Zotero/storage/SK9ZXFTJ/Leimkuhler and Shang - 2016 - Adaptive Thermostats for Noisy Gradient Systems.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/XVBIJM6Z/1505.html:text/html}
}

@article{leimkuhler_adaptive_2016-1,
	title = {Adaptive {Thermostats} for {Noisy} {Gradient} {Systems}},
	volume = {38},
	issn = {1064-8275, 1095-7197},
	url = {http://arxiv.org/abs/1505.06889},
	doi = {10.1137/15M102318X},
	abstract = {We study numerical methods for sampling probability measures in high dimension where the underlying model is only approximately identified with a gradient system. Extended stochastic dynamical methods are discussed which have application to multiscale models, nonequilibrium molecular dynamics, and Bayesian sampling techniques arising in emerging machine learning applications. In addition to providing a more comprehensive discussion of the foundations of these methods, we propose a new numerical method for the adaptive Langevin/stochastic gradient Nos{\textbackslash}'\{e\}--Hoover thermostat that achieves a dramatic improvement in numerical efficiency over the most popular stochastic gradient methods reported in the literature. We also demonstrate that the newly established method inherits a superconvergence property (fourth order convergence to the invariant measure for configurational quantities) recently demonstrated in the setting of Langevin dynamics. Our findings are verified by numerical experiments.},
	number = {2},
	urldate = {2016-10-27},
	journal = {SIAM Journal on Scientific Computing},
	author = {Leimkuhler, Benedict and Shang, Xiaocheng},
	month = jan,
	year = {2016},
	note = {arXiv: 1505.06889},
	keywords = {Condensed Matter - Statistical Mechanics, Physics - Computational Physics, Mathematics - Numerical Analysis, Physics - Chemical Physics},
	pages = {A712--A736}
}

@inproceedings{chen_bridging_2016,
	title = {Bridging the {Gap} between {Stochastic} {Gradient} {MCMC} and {Stochastic} {Optimization}},
	url = {http://www.jmlr.org/proceedings/papers/v51/chen16c.html},
	urldate = {2016-10-27},
	author = {Chen, Changyou and Carlson, David and Gan, Zhe and Li, Chunyuan and Carin, Lawrence},
	year = {2016},
	pages = {1051--1060},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/RV5AJDQ7/Chen et al. - 2016 - Bridging the Gap between Stochastic Gradient MCMC .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/KWWJ296R/chen16c.html:text/html}
}

@inproceedings{chen_convergence_2015,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'15},
	title = {On the {Convergence} of {Stochastic} {Gradient} {MCMC} {Algorithms} with {High}-order {Integrators}},
	url = {http://dl.acm.org/citation.cfm?id=2969442.2969494},
	abstract = {Recent advances in Bayesian learning with large-scale data have witnessed emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC (SGHMC), and the stochastic gradient thermostat. While finite-time convergence properties of the SGLD with a 1st-order Euler integrator have recently been studied, corresponding theory for general SG-MCMCs has not been explored. In this paper we consider general SG-MCMCs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures. Our theoretical results show faster convergence rates and more accurate invariant measures for SG-MCMCs with higher-order integrators. For example, with the proposed efficient 2nd-order symmetric splitting integrator, the mean square error (MSE) of the posterior average for the SGHMC achieves an optimal convergence rate of L-4/5 at L iterations, compared to L-2/3 for the SGHMC and SGLD with 1st-order Euler integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence. Experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications.},
	urldate = {2016-10-27},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Chen, Changyou and Ding, Nan and Carin, Lawrence},
	year = {2015},
	pages = {2278--2286},
	file = {ACM Full Text PDF:/home/jeremiah/Zotero/storage/JP8VABCT/Chen et al. - 2015 - On the Convergence of Stochastic Gradient MCMC Alg.pdf:application/pdf}
}

@article{pennec_intrinsic_2006,
	title = {Intrinsic {Statistics} on {Riemannian} {Manifolds}: {Basic} {Tools} for {Geometric} {Measurements}},
	volume = {25},
	issn = {0924-9907, 1573-7683},
	shorttitle = {Intrinsic {Statistics} on {Riemannian} {Manifolds}},
	url = {http://link.springer.com/article/10.1007/s10851-006-6228-4},
	doi = {10.1007/s10851-006-6228-4},
	abstract = {In medical image analysis and high level computer vision, there is an intensive use of geometric features like orientations, lines, and geometric transformations ranging from simple ones (orientations, lines, rigid body or affine transformations, etc.) to very complex ones like curves, surfaces, or general diffeomorphic transformations. The measurement of such geometric primitives is generally noisy in real applications and we need to use statistics either to reduce the uncertainty (estimation), to compare observations, or to test hypotheses. Unfortunately, even simple geometric primitives often belong to manifolds that are not vector spaces. In previous works [1, 2], we investigated invariance requirements to build some statistical tools on transformation groups and homogeneous manifolds that avoids paradoxes. In this paper, we consider finite dimensional manifolds with a Riemannian metric as the basic structure. Based on this metric, we develop the notions of mean value and covariance matrix of a random element, normal law, Mahalanobis distance and χ2 law. We provide a new proof of the characterization of Riemannian centers of mass and an original gradient descent algorithm to efficiently compute them. The notion of Normal law we propose is based on the maximization of the entropy knowing the mean and covariance of the distribution. The resulting family of pdfs spans the whole range from uniform (on compact manifolds) to the point mass distribution. Moreover, we were able to provide tractable approximations (with their limits) for small variances which show that we can effectively implement and work with these definitions.},
	language = {en},
	number = {1},
	urldate = {2016-10-25},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Pennec, Xavier},
	month = jul,
	year = {2006},
	pages = {127},
	file = {Snapshot:/home/jeremiah/Zotero/storage/9WH8B6BZ/s10851-006-6228-4.html:text/html}
}

@article{gal_bayesian_2015,
	title = {Bayesian {Convolutional} {Neural} {Networks} with {Bernoulli} {Approximate} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1506.02158},
	abstract = {Convolutional neural networks (CNNs) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use CNNs with small data -- as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.},
	urldate = {2016-10-25},
	journal = {arXiv:1506.02158 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02158},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: 12 pages, 3 figures, ICLR format, updated with reviewer comments}
}

@article{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	urldate = {2016-10-25},
	journal = {arXiv:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)},
	file = {arXiv\:1505.05424 PDF:/home/jeremiah/Zotero/storage/HK75K7UK/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/X9W9RJ89/1505.html:text/html}
}

@article{shaham_provable_2016,
	title = {Provable approximation properties for deep neural networks},
	issn = {10635203},
	url = {http://arxiv.org/abs/1509.07385},
	doi = {10.1016/j.acha.2016.04.003},
	abstract = {We discuss approximation of functions using deep neural nets. Given a function \$f\$ on a \$d\$-dimensional manifold \${\textbackslash}Gamma {\textbackslash}subset {\textbackslash}mathbb\{R\}{\textasciicircum}m\$, we construct a sparsely-connected depth-4 neural network and bound its error in approximating \$f\$. The size of the network depends on dimension and curvature of the manifold \${\textbackslash}Gamma\$, the complexity of \$f\$, in terms of its wavelet description, and only weakly on the ambient dimension \$m\$. Essentially, our network computes wavelet functions, which are computed from Rectified Linear Units (ReLU)},
	urldate = {2016-10-25},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Shaham, Uri and Cloninger, Alexander and Coifman, Ronald R.},
	month = apr,
	year = {2016},
	note = {arXiv: 1509.07385},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: accepted for publication in Applied and Computational Harmonic Analysis},
	file = {arXiv\:1509.07385 PDF:/home/jeremiah/Zotero/storage/EQRN2AUM/Shaham et al. - 2016 - Provable approximation properties for deep neural .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/63T4ZU78/1509.html:text/html}
}

@article{mhaskar_learning_2016,
	title = {Learning {Functions}: {When} {Is} {Deep} {Better} {Than} {Shallow}},
	shorttitle = {Learning {Functions}},
	url = {http://arxiv.org/abs/1603.00988},
	abstract = {While the universal approximation property holds both for hierarchical and shallow networks, we prove that deep (hierarchical) networks can approximate the class of compositional functions with the same accuracy as shallow networks but with exponentially lower number of training parameters as well as VC-dimension. This theorem settles an old conjecture by Bengio on the role of depth in networks. We then define a general class of scalable, shift-invariant algorithms to show a simple and natural set of requirements that justify deep convolutional networks.},
	urldate = {2016-10-25},
	journal = {arXiv:1603.00988 [cs]},
	author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.00988},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1603.00988 PDF:/home/jeremiah/Zotero/storage/W3TTQTSB/Mhaskar et al. - 2016 - Learning Functions When Is Deep Better Than Shall.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/F5EWI25A/1603.html:text/html}
}

@article{barron_approximation_nodate,
	title = {Approximation and estimation bounds for artificial neural networks},
	volume = {14},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1007/BF00993164},
	doi = {10.1007/BF00993164},
	abstract = {For a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target functionf is shown to be bounded byO(c2fn)+O(ndNlogN)O(cf2n)+O(ndNlog⁡N)O{\textbackslash}left( \{{\textbackslash}frac\{\{{\textbackslash}mathop c{\textbackslash}nolimits\_f{\textasciicircum}2 \}\}\{n\}\} {\textbackslash}right) + O{\textbackslash}left( \{{\textbackslash}frac\{\{nd\}\}\{N\}{\textbackslash}log N\} {\textbackslash}right) wheren is the number of nodes,d is the input dimension of the function,N is the number of training observations, andCf is the first absolute moment of the Fourier magnitude distribution off. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. Withn ∼ Cf(N/(d logN))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to beO(Cf((d/N) logN)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case thatd is moderately large. Similar bounds are obtained when the number of nodesn is not preselected as a function ofCf (which is generally not knowna priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks.},
	language = {en},
	number = {1},
	urldate = {2016-10-25},
	journal = {Machine Learning},
	author = {Barron, Andrew R.},
	pages = {115--133},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/VSD8Q495/Barron - Approximation and estimation bounds for artificial.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/QI9P7H92/BF00993164.html:text/html}
}

@article{pennec_intrinsic_2006-1,
	title = {Intrinsic {Statistics} on {Riemannian} {Manifolds}: {Basic} {Tools} for {Geometric} {Measurements}},
	volume = {25},
	issn = {0924-9907, 1573-7683},
	shorttitle = {Intrinsic {Statistics} on {Riemannian} {Manifolds}},
	url = {http://link.springer.com/article/10.1007/s10851-006-6228-4},
	doi = {10.1007/s10851-006-6228-4},
	abstract = {In medical image analysis and high level computer vision, there is an intensive use of geometric features like orientations, lines, and geometric transformations ranging from simple ones (orientations, lines, rigid body or affine transformations, etc.) to very complex ones like curves, surfaces, or general diffeomorphic transformations. The measurement of such geometric primitives is generally noisy in real applications and we need to use statistics either to reduce the uncertainty (estimation), to compare observations, or to test hypotheses. Unfortunately, even simple geometric primitives often belong to manifolds that are not vector spaces. In previous works [1, 2], we investigated invariance requirements to build some statistical tools on transformation groups and homogeneous manifolds that avoids paradoxes. In this paper, we consider finite dimensional manifolds with a Riemannian metric as the basic structure. Based on this metric, we develop the notions of mean value and covariance matrix of a random element, normal law, Mahalanobis distance and χ2 law. We provide a new proof of the characterization of Riemannian centers of mass and an original gradient descent algorithm to efficiently compute them. The notion of Normal law we propose is based on the maximization of the entropy knowing the mean and covariance of the distribution. The resulting family of pdfs spans the whole range from uniform (on compact manifolds) to the point mass distribution. Moreover, we were able to provide tractable approximations (with their limits) for small variances which show that we can effectively implement and work with these definitions.},
	language = {en},
	number = {1},
	urldate = {2016-10-24},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Pennec, Xavier},
	month = jul,
	year = {2006},
	pages = {127},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/BS7E7R7J/Pennec - 2006 - Intrinsic Statistics on Riemannian Manifolds Basi.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/594WT24R/s10851-006-6228-4.html:text/html}
}

@article{he_learning_2004,
	title = {Learning an {Image} {Manifold} for {Retrieval}},
	url = {https://www.microsoft.com/en-us/research/publication/learning-an-image-manifold-for-retrieval/},
	abstract = {We consider the problem of learning a mapping function from low-level feature space to high-level semantic space. Under the assumption that the data lie on a submanifold embedded in a high dimensional Euclidean space, we propose a relevance feedback scheme which is naturally conducted only on the image manifold in question rather than the total …},
	urldate = {2016-10-24},
	journal = {Microsoft Research},
	author = {He, Xiaofei and Ma, Wei-Ying and Zhang, Hong-Jiang},
	month = oct,
	year = {2004},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/MPMWV2SI/He et al. - 2004 - Learning an Image Manifold for Retrieval.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/VQGZNG3G/learning-an-image-manifold-for-retrieval.html:text/html}
}

@article{nakajima_theoretical_2011,
	title = {Theoretical {Analysis} of {Bayesian} {Matrix} {Factorization}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v12/nakajima11a.html},
	number = {Sep},
	urldate = {2016-10-21},
	journal = {Journal of Machine Learning Research},
	author = {Nakajima, Shinichi and Sugiyama, Masashi},
	year = {2011},
	pages = {2583--2648},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/785D9AHT/Nakajima and Sugiyama - 2011 - Theoretical Analysis of Bayesian Matrix Factorizat.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/SQ69ASMZ/nakajima11a.html:text/html}
}

@article{mahendran_bayesian_2011,
	title = {Bayesian {Optimization} for {Adaptive} {MCMC}},
	url = {http://arxiv.org/abs/1110.6497},
	abstract = {This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained, discrete and densely connected probabilistic graphical models where, for each variation of the problem, one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.},
	urldate = {2016-10-21},
	journal = {arXiv:1110.6497 [stat]},
	author = {Mahendran, Nimalan and Wang, Ziyu and Hamze, Firas and de Freitas, Nando},
	month = oct,
	year = {2011},
	note = {arXiv: 1110.6497},
	keywords = {Statistics - Machine Learning, Statistics - Computation},
	annote = {Comment: This paper contains 12 pages and 6 figures. A similar version of this paper has been submitted to AISTATS 2012 and is currently under review},
	file = {arXiv\:1110.6497 PDF:/home/jeremiah/Zotero/storage/CCGAW96A/Mahendran et al. - 2011 - Bayesian Optimization for Adaptive MCMC.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/IHT9KB4T/1110.html:text/html}
}

@article{mahendran_bayesian_2011-1,
	title = {Bayesian {Optimization} for {Adaptive} {MCMC}},
	url = {http://arxiv.org/abs/1110.6497},
	abstract = {This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained, discrete and densely connected probabilistic graphical models where, for each variation of the problem, one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.},
	urldate = {2016-10-21},
	journal = {arXiv:1110.6497 [stat]},
	author = {Mahendran, Nimalan and Wang, Ziyu and Hamze, Firas and de Freitas, Nando},
	month = oct,
	year = {2011},
	note = {arXiv: 1110.6497},
	keywords = {Statistics - Machine Learning, Statistics - Computation},
	annote = {Comment: This paper contains 12 pages and 6 figures. A similar version of this paper has been submitted to AISTATS 2012 and is currently under review},
	file = {arXiv\:1110.6497 PDF:/home/jeremiah/Zotero/storage/NADDVQNC/Mahendran et al. - 2011 - Bayesian Optimization for Adaptive MCMC.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/G6BTQW4V/1110.html:text/html}
}

@article{duvenaud_structure_2013,
	title = {Structure {Discovery} in {Nonparametric} {Regression} through {Compositional} {Kernel} {Search}},
	url = {http://arxiv.org/abs/1302.4922},
	abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.},
	urldate = {2016-10-21},
	journal = {arXiv:1302.4922 [cs, stat]},
	author = {Duvenaud, David and Lloyd, James Robert and Grosse, Roger and Tenenbaum, Joshua B. and Ghahramani, Zoubin},
	month = feb,
	year = {2013},
	note = {arXiv: 1302.4922},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology, G.3, I.2.6},
	annote = {Comment: 9 pages, 7 figures, To appear in proceedings of the 2013 International Conference on Machine Learning},
	file = {arXiv\:1302.4922 PDF:/home/jeremiah/Zotero/storage/M7VXBZ7J/Duvenaud et al. - 2013 - Structure Discovery in Nonparametric Regression th.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/BZVQGTPC/1302.html:text/html}
}

@article{fefferman_testing_2013,
	title = {Testing the {Manifold} {Hypothesis}},
	url = {http://arxiv.org/abs/1310.0425},
	abstract = {The hypothesis that high dimensional data tend to lie in the vicinity of a low dimensional manifold is the basis of manifold learning. The goal of this paper is to develop an algorithm (with accompanying complexity guarantees) for fitting a manifold to an unknown probability distribution supported in a separable Hilbert space, only using i.i.d samples from that distribution. More precisely, our setting is the following. Suppose that data are drawn independently at random from a probability distribution \$P\$ supported on the unit ball of a separable Hilbert space \$H\$. Let \$G(d, V, {\textbackslash}tau)\$ be the set of submanifolds of the unit ball of \$H\$ whose volume is at most \$V\$ and reach (which is the supremum of all \$r\$ such that any point at a distance less than \$r\$ has a unique nearest point on the manifold) is at least \${\textbackslash}tau\$. Let \$L(M, P)\$ denote mean-squared distance of a random point from the probability distribution \$P\$ to \$M\$. We obtain an algorithm that tests the manifold hypothesis in the following sense. The algorithm takes i.i.d random samples from \$P\$ as input, and determines which of the following two is true (at least one must be): (a) There exists \$M {\textbackslash}in G(d, CV, {\textbackslash}frac\{{\textbackslash}tau\}\{C\})\$ such that \$L(M, P) {\textbackslash}leq C {\textbackslash}epsilon.\$ (b) There exists no \$M {\textbackslash}in G(d, V/C, C{\textbackslash}tau)\$ such that \$L(M, P) {\textbackslash}leq {\textbackslash}frac\{{\textbackslash}epsilon\}\{C\}.\$ The answer is correct with probability at least \$1-{\textbackslash}delta\$.},
	urldate = {2016-10-18},
	journal = {arXiv:1310.0425 [math, stat]},
	author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.0425},
	keywords = {Mathematics - Statistics Theory, Mathematics - Differential Geometry, 62G08, Mathematics - Classical Analysis and ODEs},
	annote = {Comment: 47 pages, 7 figures},
	file = {arXiv\:1310.0425 PDF:/home/jeremiah/Zotero/storage/T2W6F2U3/Fefferman et al. - 2013 - Testing the Manifold Hypothesis.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/ZGBNJS6Q/1310.html:text/html}
}

@article{gomez-pinilla_brain_2008,
	title = {Brain foods: the effects of nutrients on brain function},
	volume = {9},
	issn = {1471-003X},
	shorttitle = {Brain foods},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2805706/},
	doi = {10.1038/nrn2421},
	abstract = {It has long been suspected that the relative abundance of specific nutrients can affect cognitive processes and emotions. Newly described influences of dietary factors on neuronal function and synaptic plasticity have revealed some of the vital mechanisms that are responsible for the action of diet on brain health and mental function. Several gut hormones that can enter the brain, or that are produced in the brain itself, influence cognitive ability. In addition, well-established regulators of synaptic plasticity, such as brain-derived neurotrophic factor, can function as metabolic modulators, responding to peripheral signals such as food intake. Understanding the molecular basis of the effects of food on cognition will help us to determine how best to manipulate diet in order to increase the resistance of neurons to insults and promote mental fitness.},
	number = {7},
	urldate = {2016-06-23},
	journal = {Nature reviews. Neuroscience},
	author = {Gómez-Pinilla, Fernando},
	month = jul,
	year = {2008},
	pmid = {18568016},
	pmcid = {PMC2805706},
	pages = {568--578},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/382TFE9D/Gómez-Pinilla - 2008 - Brain foods the effects of nutrients on brain fun.pdf:application/pdf}
}

@article{abadi_tensorflow:_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {{TensorFlow}},
	url = {http://arxiv.org/abs/1605.08695},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	urldate = {2016-06-23},
	journal = {arXiv:1605.08695 [cs]},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = may,
	year = {2016},
	note = {arXiv: 1605.08695},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: 18 pages, 9 figures; v2 has a spelling correction in the metadata},
	file = {arXiv\:1605.08695 PDF:/home/jeremiah/Zotero/storage/QTCAGGM6/Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/Z4PUADVN/Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.html:text/html}
}

@article{klein_fast_2016,
	title = {Fast {Bayesian} {Optimization} of {Machine} {Learning} {Hyperparameters} on {Large} {Datasets}},
	url = {http://arxiv.org/abs/1605.07079},
	abstract = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. But it is still costly if each evaluation of the objective requires training and validating the algorithm being optimized, which, for large datasets, often takes hours, days, or even weeks. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed FABOLAS, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods.},
	urldate = {2016-06-23},
	journal = {arXiv:1605.07079 [cs, stat]},
	author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
	month = may,
	year = {2016},
	note = {arXiv: 1605.07079},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1605.07079 PDF:/home/jeremiah/Zotero/storage/W5HGVG57/Klein et al. - 2016 - Fast Bayesian Optimization of Machine Learning Hyp.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/CU2DFWHB/1605.html:text/html}
}

@article{machado_learning_2016,
	title = {Learning {Purposeful} {Behaviour} in the {Absence} of {Rewards}},
	url = {http://arxiv.org/abs/1605.07700},
	abstract = {Artificial intelligence is commonly defined as the ability to achieve goals in the world. In the reinforcement learning framework, goals are encoded as reward functions that guide agent behaviour, and the sum of observed rewards provide a notion of progress. However, some domains have no such reward signal, or have a reward signal so sparse as to appear absent. Without reward feedback, agent behaviour is typically random, often dithering aimlessly and lacking intentionality. In this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards. The algorithm proceeds by constructing temporally extended actions (options), through the identification of purposes that are "just out of reach" of the agent's current behaviour. These purposes establish intrinsic goals for the agent to learn, ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space. Moreover, the approach is particularly suited for settings where rewards are very sparse, and such behaviours can help in the exploration of the environment until reward is observed.},
	urldate = {2016-06-23},
	journal = {arXiv:1605.07700 [cs]},
	author = {Machado, Marlos C. and Bowling, Michael},
	month = may,
	year = {2016},
	note = {arXiv: 1605.07700},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Extended version of the paper presented at the workshop entitled Abstraction in Reinforcement Learning, at the 33rd International Conference on Machine Learning, New York, NY, USA, 2016},
	file = {arXiv\:1605.07700 PDF:/home/jeremiah/Zotero/storage/UVRVFM3M/Machado and Bowling - 2016 - Learning Purposeful Behaviour in the Absence of Re.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/EAK5QRN3/1605.html:text/html}
}

@article{koushik_understanding_2016,
	title = {Understanding {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1605.09081},
	abstract = {Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on a variety of machine learning tasks. However, their mathematical properties and behavior are quite poorly understood. There is some work, in the form of a framework, for analyzing the operations that they perform. The goal of this project is to present key results from this theory, and provide intuition for why CNNs work.},
	urldate = {2016-06-23},
	journal = {arXiv:1605.09081 [cs, stat]},
	author = {Koushik, Jayanth},
	month = may,
	year = {2016},
	note = {arXiv: 1605.09081},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Statistical Machine Learning Course Project at Carnegie Mellon University},
	file = {arXiv\:1605.09081 PDF:/home/jeremiah/Zotero/storage/CVAS6BBZ/Koushik - 2016 - Understanding Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/THE7ZKMZ/Koushik - 2016 - Understanding Convolutional Neural Networks.html:text/html}
}

@misc{noauthor_15_nodate,
	title = {15 {Jaw}-{Dropping} {Places} {Worth} {Visiting}},
	url = {http://mentalfloss.com/article/62175/15-jaw-dropping-places-worth-visiting},
	abstract = {As wild and vivid as your imagination might be, it likely doesn’t hold a candle to that of Mother Nature.},
	urldate = {2016-06-20},
	journal = {Mental Floss},
	file = {Snapshot:/home/jeremiah/Zotero/storage/96EFW4JU/15-jaw-dropping-places-worth-visiting.html:text/html}
}

@misc{bodybuildingcom_built_2013,
	title = {Built {By} {Science}: {Six}-{Week} {Muscle}-{Building} {Trainer}},
	shorttitle = {Built {By} {Science}},
	url = {http://www.bodybuilding.com/fun/built-by-science-six-week-muscle-building-trainer.html},
	abstract = {The body is a work of art. It's yours to create. Built by Science will teach you anatomy, biomechanics, and specific exercises so you can build a masterpiece physique.},
	urldate = {2016-06-19},
	journal = {Bodybuilding.com},
	author = {{Bodybuildingcom}},
	month = dec,
	year = {2013},
	file = {Snapshot:/home/jeremiah/Zotero/storage/XFAE7JN5/Bodybuildingcom - 2013 - Built By Science Six-Week Muscle-Building Trainer.html:text/html}
}

@article{journal_next_2016,
	chapter = {Page One},
	title = {The {Next} {Frontier} in {Nordic} {Vacationing}? {Norway} {Itself}},
	issn = {0099-9660},
	shorttitle = {The {Next} {Frontier} in {Nordic} {Vacationing}?},
	url = {http://www.wsj.com/articles/the-next-frontier-in-nordic-vacationing-norway-itself-1465826145},
	abstract = {A land with all the otherworldliness of Iceland and none of the crowds. All you need is the right guide.},
	urldate = {2016-06-16},
	journal = {Wall Street Journal},
	author = {Journal, Stan Parish {\textbar} Photographs by Marzena Skubatz for The Wall Street},
	month = jun,
	year = {2016},
	file = {Wall Street Journal Snapshot:/home/jeremiah/Zotero/storage/FZTVZUNQ/Journal - 2016 - The Next Frontier in Nordic Vacationing Norway It.html:text/html}
}

@misc{bodybuildingcom_built_2013-1,
	title = {Built {By} {Science}: {Six}-{Week} {Muscle}-{Building} {Trainer}},
	shorttitle = {Built {By} {Science}},
	url = {http://www.bodybuilding.com/fun/built-by-science-six-week-muscle-building-trainer.html},
	abstract = {The body is a work of art. It's yours to create. Built by Science will teach you anatomy, biomechanics, and specific exercises so you can build a masterpiece physique.},
	urldate = {2016-06-16},
	journal = {Bodybuilding.com},
	author = {{Bodybuildingcom}},
	month = dec,
	year = {2013},
	file = {Snapshot:/home/jeremiah/Zotero/storage/PD6X944M/Bodybuildingcom - 2013 - Built By Science Six-Week Muscle-Building Trainer.html:text/html}
}

@article{li_learning_2016,
	title = {Learning to {Optimize}},
	url = {http://arxiv.org/abs/1606.01885},
	abstract = {Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.},
	urldate = {2016-06-10},
	journal = {arXiv:1606.01885 [cs, math, stat]},
	author = {Li, Ke and Malik, Jitendra},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01885},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence, Mathematics - Optimization and Control},
	annote = {Comment: 9 pages, 3 figures}
}

@article{badre_rostrolateral_2012,
	title = {Rostrolateral prefrontal cortex and individual differences in uncertainty-driven exploration},
	volume = {73},
	issn = {1097-4199},
	doi = {10.1016/j.neuron.2011.12.025},
	abstract = {How do individuals decide to act based on a rewarding status quo versus an unexplored choice that might yield a better outcome? Recent evidence suggests that individuals may strategically explore as a function of the relative uncertainty about the expected value of options. However, the neural mechanisms supporting uncertainty-driven exploration remain underspecified. The present fMRI study scanned a reinforcement learning task in which participants stop a rotating clock hand in order to win points. Reward schedules were such that expected value could increase, decrease, or remain constant with respect to time. We fit several mathematical models to subject behavior to generate trial-by-trial estimates of exploration as a function of relative uncertainty. These estimates were used to analyze our fMRI data. Results indicate that rostrolateral prefrontal cortex tracks trial-by-trial changes in relative uncertainty, and this pattern distinguished individuals who rely on relative uncertainty for their exploratory decisions versus those who do not.},
	language = {eng},
	number = {3},
	journal = {Neuron},
	author = {Badre, David and Doll, Bradley B. and Long, Nicole M. and Frank, Michael J.},
	month = feb,
	year = {2012},
	pmid = {22325209},
	pmcid = {PMC3285405},
	keywords = {Computer Simulation, Humans, Female, Male, Magnetic Resonance Imaging, Adult, Uncertainty, Adolescent, Brain Mapping, Choice Behavior, Exploratory Behavior, Image Processing, Computer-Assisted, Individuality, Models, Neurological, Neuropsychological Tests, Oxygen, Prefrontal Cortex, Reaction Time, Reinforcement (Psychology), Young Adult},
	pages = {595--607}
}

@article{friston_free-energy_2010,
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	copyright = {© 2010 Nature Publishing Group},
	issn = {1471-003X},
	shorttitle = {The free-energy principle},
	url = {http://www.nature.com/nrn/journal/v11/n2/full/nrn2787.html},
	doi = {10.1038/nrn2787},
	abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories — optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.},
	language = {en},
	number = {2},
	urldate = {2016-05-24},
	journal = {Nature Reviews Neuroscience},
	author = {Friston, Karl},
	month = feb,
	year = {2010},
	pages = {127--138},
	file = {Snapshot:/home/jeremiah/Zotero/storage/M9ZGIZHU/nrn2787.html:text/html}
}

@article{lu_how_2014,
	title = {How to {Scale} {Up} {Kernel} {Methods} to {Be} {As} {Good} {As} {Deep} {Neural} {Nets}},
	url = {http://arxiv.org/abs/1411.4000},
	abstract = {The computational complexity of kernel methods has often been a major barrier for applying them to large-scale learning problems. We argue that this barrier can be effectively overcome. In particular, we develop methods to scale up kernel models to successfully tackle large-scale learning problems that are so far only approachable by deep learning architectures. Based on the seminal work by Rahimi and Recht on approximating kernel functions with features derived from random projections, we advance the state-of-the-art by proposing methods that can efficiently train models with hundreds of millions of parameters, and learn optimal representations from multiple kernels. We conduct extensive empirical studies on problems from image recognition and automatic speech recognition, and show that the performance of our kernel models matches that of well-engineered deep neural nets (DNNs). To the best of our knowledge, this is the first time that a direct comparison between these two methods on large-scale problems is reported. Our kernel methods have several appealing properties: training with convex optimization, cost for training a single model comparable to DNNs, and significantly reduced total cost due to fewer hyperparameters to tune for model selection. Our contrastive study between these two very different but equally competitive models sheds light on fundamental questions such as how to learn good representations.},
	urldate = {2016-05-24},
	journal = {arXiv:1411.4000 [cs, stat]},
	author = {Lu, Zhiyun and May, Avner and Liu, Kuan and Garakani, Alireza Bagheri and Guo, Dong and Bellet, Aurélien and Fan, Linxi and Collins, Michael and Kingsbury, Brian and Picheny, Michael and Sha, Fei},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4000},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1411.4000 PDF:/home/jeremiah/Zotero/storage/U65SPVXJ/Lu et al. - 2014 - How to Scale Up Kernel Methods to Be As Good As De.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/3773DQFQ/Lu et al. - 2014 - How to Scale Up Kernel Methods to Be As Good As De.html:text/html}
}

@article{hamid_compact_2013,
	title = {Compact {Random} {Feature} {Maps}},
	url = {http://arxiv.org/abs/1312.4626},
	abstract = {Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results.},
	urldate = {2016-05-24},
	journal = {arXiv:1312.4626 [cs, stat]},
	author = {Hamid, Raffay and Xiao, Ying and Gittens, Alex and DeCoste, Dennis},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.4626},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: 9 pages},
	file = {arXiv\:1312.4626 PDF:/home/jeremiah/Zotero/storage/B4V5SVGZ/Hamid et al. - 2013 - Compact Random Feature Maps.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WDBT88GP/1312.html:text/html}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2016-05-12},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv\:1412.6980 PDF:/home/jeremiah/Zotero/storage/IWESPPZK/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/3WTXRG8J/1412.html:text/html}
}

@article{zeiler_adadelta:_2012,
	title = {{ADADELTA}: {An} {Adaptive} {Learning} {Rate} {Method}},
	shorttitle = {{ADADELTA}},
	url = {http://arxiv.org/abs/1212.5701},
	abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
	urldate = {2016-05-12},
	journal = {arXiv:1212.5701 [cs]},
	author = {Zeiler, Matthew D.},
	month = dec,
	year = {2012},
	note = {arXiv: 1212.5701},
	keywords = {Computer Science - Learning},
	annote = {Comment: 6 pages},
	file = {arXiv\:1212.5701 PDF:/home/jeremiah/Zotero/storage/MI9TZUIE/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/AU63AZ8H/1212.html:text/html}
}

@article{duchi_adaptive_2011,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	number = {Jul},
	urldate = {2016-05-12},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/X4UIHEQT/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/WB63TZSE/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.html:text/html}
}

@article{aas_pair-copula_2009,
	title = {Pair-copula constructions of multiple dependence},
	volume = {44},
	issn = {0167-6687},
	url = {http://www.sciencedirect.com/science/article/pii/S0167668707000194},
	doi = {10.1016/j.insmatheco.2007.02.001},
	abstract = {Building on the work of Bedford, Cooke and Joe, we show how multivariate data, which exhibit complex patterns of dependence in the tails, can be modelled using a cascade of pair-copulae, acting on two variables at a time. We use the pair-copula decomposition of a general multivariate distribution and propose a method for performing inference. The model construction is hierarchical in nature, the various levels corresponding to the incorporation of more variables in the conditioning sets, using pair-copulae as simple building blocks. Pair-copula decomposed models also represent a very flexible way to construct higher-dimensional copulae. We apply the methodology to a financial data set. Our approach represents the first step towards the development of an unsupervised algorithm that explores the space of possible pair-copula models, that also can be applied to huge data sets automatically.},
	number = {2},
	urldate = {2016-05-06},
	journal = {Insurance: Mathematics and Economics},
	author = {Aas, Kjersti and Czado, Claudia and Frigessi, Arnoldo and Bakken, Henrik},
	month = apr,
	year = {2009},
	keywords = {Conditional distribution, Decomposition, Multivariate distribution, Pair-copulae, Vines},
	pages = {182--198},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/NAK4NXTN/S0167668707000194.html:text/html}
}

@article{giordano_covariance_2014,
	title = {Covariance {Matrices} for {Mean} {Field} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1410.6853},
	abstract = {Mean Field Variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is its (sometimes severe) underestimates of the uncertainty of model variables and lack of information about model variable covariance. We develop a fast, general methodology for exponential families that augments MFVB to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables. MFVB for exponential families defines a fixed-point equation in the means of the approximating posterior, and our approach yields a covariance estimate by perturbing this fixed point. Inspired by linear response theory, we call our method linear response variational Bayes (LRVB). We demonstrate the accuracy of our method on simulated data sets.},
	urldate = {2016-05-06},
	journal = {arXiv:1410.6853 [cs, stat]},
	author = {Giordano, Ryan and Broderick, Tamara},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.6853},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology},
	annote = {Comment: 14 pages, 2 figures},
	file = {arXiv\:1410.6853 PDF:/home/jeremiah/Zotero/storage/MXZ5SB6E/Giordano and Broderick - 2014 - Covariance Matrices for Mean Field Variational Bay.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/4H397AV5/1410.html:text/html}
}

@article{aas_models_2009,
	title = {Models for construction of multivariate dependence – a comparison study},
	volume = {15},
	issn = {1351-847X},
	url = {http://dx.doi.org/10.1080/13518470802588767},
	doi = {10.1080/13518470802588767},
	abstract = {A multivariate data set, which exhibit complex patterns of dependence, particularly in the tails, can be modelled using a cascade of lower-dimensional copulae. In this paper, we compare two such models that differ in their representation of the dependency structure, namely the nested Archimedean construction (NAC) and the pair-copula construction (PCC). The NAC is much more restrictive than the PCC in two respects. There are strong limitations on the degree of dependence in each level of the NAC, and all the bivariate copulas in this construction has to be Archimedean. Based on an empirical study with two different four-dimensional data sets; precipitation values and equity returns, we show that the PCC provides a better fit than the NAC and that it is computationally more efficient. Hence, we claim that the PCC is more suitable than the NAC for hich-dimensional modelling.},
	number = {7-8},
	urldate = {2016-05-06},
	journal = {The European Journal of Finance},
	author = {Aas, Kjersti and Berg, Daniel},
	month = dec,
	year = {2009},
	pages = {639--659},
	file = {Snapshot:/home/jeremiah/Zotero/storage/ISHA9ATV/13518470802588767.html:text/html}
}

@article{dissmann_selecting_2012,
	title = {Selecting and estimating regular vine copulae and application to financial returns},
	url = {http://arxiv.org/abs/1202.2002},
	abstract = {Regular vine distributions which constitute a flexible class of multivariate dependence models are discussed. Since multivariate copulae constructed through pair-copula decompositions were introduced to the statistical community, interest in these models has been growing steadily and they are finding successful applications in various fields. Research so far has however been concentrating on so-called canonical and D-vine copulae, which are more restrictive cases of regular vine copulae. It is shown how to evaluate the density of arbitrary regular vine specifications. This opens the vine copula methodology to the flexible modeling of complex dependencies even in larger dimensions. In this regard, a new automated model selection and estimation technique based on graph theoretical considerations is presented. This comprehensive search strategy is evaluated in a large simulation study and applied to a 16-dimensional financial data set of international equity, fixed income and commodity indices which were observed over the last decade, in particular during the recent financial crisis. The analysis provides economically well interpretable results and interesting insights into the dependence structure among these indices.},
	urldate = {2016-05-06},
	journal = {arXiv:1202.2002 [stat]},
	author = {Dissmann, Jeffrey and Brechmann, Eike Christian and Czado, Claudia and Kurowicka, Dorota},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.2002},
	keywords = {Statistics - Methodology}
}

@article{kulkarni_hierarchical_2016,
	title = {Hierarchical {Deep} {Reinforcement} {Learning}: {Integrating} {Temporal} {Abstraction} and {Intrinsic} {Motivation}},
	shorttitle = {Hierarchical {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1604.06057},
	abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete MDP with stochastic transitions, and (2) the classic ATARI game `Montezuma's Revenge'.},
	urldate = {2016-04-29},
	journal = {arXiv:1604.06057 [cs, stat]},
	author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06057},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 13 pages, 7 figures}
}

@article{noauthor_conditionalization_2012,
	title = {Conditionalization of {Copula}-{Based} {Models}},
	volume = {9},
	issn = {1545-8490},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/deca.1120.0243},
	doi = {10.1287/deca.1120.0243},
	abstract = {Copula models are becoming increasingly popular in engineering and financial applications. They provide a flexible way of constructing joint distributions with arbitrary one-dimensional margins and a wide variety of dependence structures. This paper studies different types of conditionalization of copula-based models. We conditionalize these models in the usual way on point values but also propose a different type of conditionalization on a new margin. This new type of conditionalization is motivated by experience with a model built for application in modeling risk in civil aviation. Changing one margin in a copula model is very easy; however, it is not equivalent to conditionalizing on a new margin. For this purpose, the technique presented by Holland and Wang [Holland PW, Wang YJ (1987) Dependence function for continuous bivariate densities. Commun. Statist. Theory Methods 16(3):867–876], called marginal replacement, is used. The main result of this paper allows a simplified way of conditionalizing on more than one univariate margin in a normal copula model.},
	number = {3},
	urldate = {2016-04-25},
	journal = {Decision Analysis},
	month = aug,
	year = {2012},
	pages = {219--230},
	file = {Snapshot:/home/jeremiah/Zotero/storage/6KUNNBSR/2012 - Conditionalization of Copula-Based Models.html:text/html}
}

@article{giordano_robust_2015,
	title = {Robust {Inference} with {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1512.02578},
	abstract = {In Bayesian analysis, the posterior follows from the data and a choice of a prior and a likelihood. One hopes that the posterior is robust to reasonable variation in the choice of prior and likelihood, since this choice is made by the modeler and is necessarily somewhat subjective. Despite the fundamental importance of the problem and a considerable body of literature, the tools of robust Bayes are not commonly used in practice. This is in large part due to the difficulty of calculating robustness measures from MCMC draws. Although methods for computing robustness measures from MCMC draws exist, they lack generality and often require additional coding or computation. In contrast to MCMC, variational Bayes (VB) techniques are readily amenable to robustness analysis. The derivative of a posterior expectation with respect to a prior or data perturbation is a measure of local robustness to the prior or likelihood. Because VB casts posterior inference as an optimization problem, its methodology is built on the ability to calculate derivatives of posterior quantities with respect to model parameters, even in very complex models. In the present work, we develop local prior robustness measures for mean-field variational Bayes(MFVB), a VB technique which imposes a particular factorization assumption on the variational posterior approximation. We start by outlining existing local prior measures of robustness. Next, we use these results to derive closed-form measures of the sensitivity of mean-field variational posterior approximation to prior specification. We demonstrate our method on a meta-analysis of randomized controlled interventions in access to microcredit in developing countries.},
	urldate = {2016-04-25},
	journal = {arXiv:1512.02578 [stat]},
	author = {Giordano, Ryan and Broderick, Tamara and Jordan, Michael},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.02578},
	keywords = {Statistics - Methodology},
	annote = {Comment: 16 pages},
	file = {arXiv\:1512.02578 PDF:/home/jeremiah/Zotero/storage/QJ85M3J8/Giordano et al. - 2015 - Robust Inference with Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/7AR89C6J/1512.html:text/html}
}

@article{kurowicka_sampling_2007,
	title = {Sampling algorithms for generating joint uniform distributions using the vine-copula method},
	volume = {51},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947306004269},
	doi = {10.1016/j.csda.2006.11.043},
	abstract = {An n - dimensional joint uniform distribution is defined as a distribution whose one-dimensional marginals are uniform on some interval I. This interval is taken to be [0,1] or, when more convenient [ - 1 2 , 1 2 ] . The specification of joint uniform distributions in a way which captures intuitive dependence structures and also enables sampling routines is considered. The question whether every n-dimensional correlation matrix can be realized by a joint uniform distribution remains open. It is known, however, that the rank correlation matrices realized by the joint normal family are sparse in the set of correlation matrices. A joint uniform distribution is obtained by specifying conditional rank correlations on a regular vine and a copula is chosen to realize the conditional bivariate distributions corresponding to the edges of the vine. In this way a distribution is sampled which corresponds exactly to the specification. The relation between conditional rank correlations on a vine and correlation matrix of corresponding distribution is complex, and depends on the copula used. Some results for the elliptical copulae are given.},
	number = {6},
	urldate = {2016-04-25},
	journal = {Computational Statistics \& Data Analysis},
	author = {Kurowicka, D. and Cooke, R. M.},
	month = mar,
	year = {2007},
	keywords = {Vines, Dependence modeling, Joint distribution, Joint uniform, Sampling},
	pages = {2889--2906},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/U7GVQ24F/S0167947306004269.html:text/html}
}

@article{wang_variational_2012,
	title = {Variational {Inference} in {Nonconjugate} {Models}},
	url = {http://arxiv.org/abs/1209.4360},
	abstract = {Mean-field variational methods are widely used for approximate posterior inference in many probabilistic models. In a typical application, mean-field methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest---like the correlated topic model and Bayesian logistic regression---are nonconjuate. In these models, mean-field methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconjugate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for specific models; and they work well on real-world datasets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression.},
	urldate = {2016-04-24},
	journal = {arXiv:1209.4360 [stat]},
	author = {Wang, Chong and Blei, David M.},
	month = sep,
	year = {2012},
	note = {arXiv: 1209.4360},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1209.4360 PDF:/home/jeremiah/Zotero/storage/CUMR8S95/Wang and Blei - 2012 - Variational Inference in Nonconjugate Models.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/PB5XBUWM/1209.html:text/html}
}

@article{rezende_stochastic_2014-1,
	title = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1401.4082},
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	urldate = {2016-04-22},
	journal = {arXiv:1401.4082 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	month = jan,
	year = {2014},
	note = {arXiv: 1401.4082},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Appears In Proceedings of the 31st International Conference on Machine Learning (ICML), JMLR: W{\textbackslash}\&CP volume 32, 2014},
	file = {arXiv\:1401.4082 PDF:/home/jeremiah/Zotero/storage/5DU4ZEDD/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/DC6FURTF/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.html:text/html}
}

@article{tran_copula_2015,
	title = {Copula variational inference},
	url = {http://arxiv.org/abs/1506.03159},
	abstract = {We develop a general variational inference method that preserves dependency among the latent variables. Our method uses copulas to augment the families of distributions used in mean-field and structured approximations. Copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior. With stochastic optimization, inference on the augmented distribution is scalable. Furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach. Copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables.},
	urldate = {2016-04-22},
	journal = {arXiv:1506.03159 [cs, stat]},
	author = {Tran, Dustin and Blei, David M. and Airoldi, Edoardo M.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03159},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Appears in Neural Information Processing Systems, 2015},
	file = {arXiv\:1506.03159 PDF:/home/jeremiah/Zotero/storage/P6ZHUKH3/Tran et al. - 2015 - Copula variational inference.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/5VHCNIAG/Tran et al. - 2015 - Copula variational inference.html:text/html}
}

@article{ranganath_black_2013,
	title = {Black {Box} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1401.0118},
	abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
	urldate = {2016-04-22},
	journal = {arXiv:1401.0118 [cs, stat]},
	author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
	month = dec,
	year = {2013},
	note = {arXiv: 1401.0118},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, Statistics - Methodology},
	file = {arXiv\:1401.0118 PDF:/home/jeremiah/Zotero/storage/U62F54MH/Ranganath et al. - 2013 - Black Box Variational Inference.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WSKDSXFE/Ranganath et al. - 2013 - Black Box Variational Inference.html:text/html}
}

@article{favaro_mcmc_2013,
	title = {{MCMC} for {Normalized} {Random} {Measure} {Mixture} {Models}},
	volume = {28},
	issn = {0883-4237},
	url = {http://arxiv.org/abs/1310.0595},
	doi = {10.1214/13-STS422},
	abstract = {This paper concerns the use of Markov chain Monte Carlo methods for posterior sampling in Bayesian nonparametric mixture models with normalized random measure priors. Making use of some recent posterior characterizations for the class of normalized random measures, we propose novel Markov chain Monte Carlo methods of both marginal type and conditional type. The proposed marginal samplers are generalizations of Neal's well-regarded Algorithm 8 for Dirichlet process mixture models, whereas the conditional sampler is a variation of those recently introduced in the literature. For both the marginal and conditional methods, we consider as a running example a mixture model with an underlying normalized generalized Gamma process prior, and describe comparative simulation results demonstrating the efficacies of the proposed methods.},
	number = {3},
	urldate = {2016-04-20},
	journal = {Statistical Science},
	author = {Favaro, Stefano and Teh, Yee Whye},
	month = aug,
	year = {2013},
	note = {arXiv: 1310.0595},
	keywords = {Statistics - Methodology},
	pages = {335--359},
	annote = {Comment: Published in at http://dx.doi.org/10.1214/13-STS422 the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv\:1310.0595 PDF:/home/jeremiah/Zotero/storage/N6GU4VAH/Favaro and Teh - 2013 - MCMC for Normalized Random Measure Mixture Models.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/D6KR9J76/Favaro and Teh - 2013 - MCMC for Normalized Random Measure Mixture Models.html:text/html}
}

@incollection{wang_truncation-free_2012,
	title = {Truncation-free {Online} {Variational} {Inference} for {Bayesian} {Nonparametric} {Models}},
	url = {http://papers.nips.cc/paper/4534-truncation-free-online-variational-inference-for-bayesian-nonparametric-models.pdf},
	urldate = {2016-04-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Chong and Blei, David M.},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {413--421},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/TQBZDGS2/Wang and Blei - 2012 - Truncation-free Online Variational Inference for B.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/67GZPJ73/4534-truncation-free-online-variational-inference-for-bayesian-nonparametric-models.html:text/html}
}

@article{ranganath_correlated_2015,
	title = {Correlated {Random} {Measures}},
	url = {http://arxiv.org/abs/1507.00720},
	abstract = {We develop correlated random measures, random measures where the atom weights can exhibit a flexible pattern of dependence, and use them to develop powerful hierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric models are usually built from completely random measures, a Poisson-process based construction in which the atom weights are independent. Completely random measures imply strong independence assumptions in the corresponding hierarchical model, and these assumptions are often misplaced in real-world settings. Correlated random measures address this limitation. They model correlation within the measure by using a Gaussian process in concert with the Poisson process. With correlated random measures, for example, we can develop a latent feature model for which we can infer both the properties of the latent features and their dependency pattern. We develop several other examples as well. We study a correlated random measure model of pairwise count data. We derive an efficient variational inference algorithm and show improved predictive performance on large data sets of documents, web clicks, and electronic health records.},
	urldate = {2016-04-18},
	journal = {arXiv:1507.00720 [stat]},
	author = {Ranganath, Rajesh and Blei, David},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.00720},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv\:1507.00720 PDF:/home/jeremiah/Zotero/storage/F9M3WJIX/Ranganath and Blei - 2015 - Correlated Random Measures.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/UUZK77PP/1507.html:text/html}
}

@article{tank_streaming_2014,
	title = {Streaming {Variational} {Inference} for {Bayesian} {Nonparametric} {Mixture} {Models}},
	url = {http://arxiv.org/abs/1412.0694},
	abstract = {In theory, Bayesian nonparametric (BNP) models are well suited to streaming data scenarios due to their ability to adapt model complexity with the observed data. Unfortunately, such benefits have not been fully realized in practice; existing inference algorithms are either not applicable to streaming applications or not extensible to BNP models. For the special case of Dirichlet processes, streaming inference has been considered. However, there is growing interest in more flexible BNP models building on the class of normalized random measures (NRMs). We work within this general framework and present a streaming variational inference algorithm for NRM mixture models. Our algorithm is based on assumed density filtering (ADF), leading straightforwardly to expectation propagation (EP) for large-scale batch inference as well. We demonstrate the efficacy of the algorithm on clustering documents in large, streaming text corpora.},
	urldate = {2016-04-18},
	journal = {arXiv:1412.0694 [stat]},
	author = {Tank, Alex and Foti, Nicholas J. and Fox, Emily B.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.0694},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1412.0694 PDF:/home/jeremiah/Zotero/storage/8TVGUXU3/Tank et al. - 2014 - Streaming Variational Inference for Bayesian Nonpa.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/ZWC2W88H/1412.html:text/html}
}

@inproceedings{ghahramani_variational_2000,
	title = {Variational {Inference} for {Bayesian} {Mixtures} of {Factor} {Analysers}},
	abstract = {We present an algorithm that infers the model structure of a mixture of factor analysers using an ecient and deterministic variational approximation to full Bayesian integration over model parameters. This procedure can automatically determine the optimal number of components and the local dimensionality of each component (i.e. the number of factors in each factor analyser). Alternatively it can be used to infer posterior distributions over number of components and dimensionalities. Since all parameters are integrated out the method is not prone to over tting. Using a stochastic procedure for adding components it is possible to perform the variational optimisation incrementally and to avoid local maxima. Results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples. By importance sampling from the variational approximation we show how to obtain unbiased estimates of the true evidence, the exa...},
	booktitle = {In {Advances} in {Neural} {Information} {Processing} {Systems} 12},
	publisher = {MIT Press},
	author = {Ghahramani, Zoubin and Beal, Matthew J.},
	year = {2000},
	pages = {449--455},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/7DRJRBQ8/Ghahramani and Beal - 2000 - Variational Inference for Bayesian Mixtures of Fac.pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/IDV26ZUT/summary.html:text/html}
}

@article{bhattacharya_sparse_2011,
	title = {Sparse {Bayesian} infinite factor models},
	volume = {98},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/content/98/2/291},
	doi = {10.1093/biomet/asr013},
	abstract = {We focus on sparse modelling of high-dimensional covariance matrices using Bayesian latent factor models. We propose a multiplicative gamma process shrinkage prior on the factor loadings which allows introduction of infinitely many factors, with the loadings increasingly shrunk towards zero as the column index increases. We use our prior on a parameter-expanded loading matrix to avoid the order dependence typical in factor analysis models and develop an efficient Gibbs sampler that scales well as data dimensionality increases. The gain in efficiency is achieved by the joint conjugacy property of the proposed prior, which allows block updating of the loadings matrix. We propose an adaptive Gibbs sampler for automatically truncating the infinite loading matrix through selection of the number of important factors. Theoretical results are provided on the support of the prior and truncation approximation bounds. A fast algorithm is proposed to produce approximate Bayes estimates. Latent factor regression methods are developed for prediction and variable selection in applications with high-dimensional correlated predictors. Operating characteristics are assessed through simulation studies, and the approach is applied to predict survival times from gene expression data.},
	language = {en},
	number = {2},
	urldate = {2016-04-18},
	journal = {Biometrika},
	author = {Bhattacharya, A. and Dunson, D. B.},
	month = jun,
	year = {2011},
	keywords = {Adaptive Gibbs sampling, Factor analysis, High-dimensional data, Multiplicative gamma process, Parameter expansion, Regularization, Shrinkage},
	pages = {291--306},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/KVJU8CGE/Bhattacharya and Dunson - 2011 - Sparse Bayesian infinite factor models.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/ISEH28AH/Bhattacharya and Dunson - 2011 - Sparse Bayesian infinite factor models.html:text/html}
}

@article{james_posterior_2009,
	title = {Posterior {Analysis} for {Normalized} {Random} {Measures} with {Independent} {Increments}},
	volume = {36},
	copyright = {© 2008 Board of the Foundation of the Scandinavian Journal of Statistics},
	issn = {1467-9469},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9469.2008.00609.x/abstract},
	doi = {10.1111/j.1467-9469.2008.00609.x},
	abstract = {Abstract.  One of the main research areas in Bayesian Nonparametrics is the proposal and study of priors which generalize the Dirichlet process. In this paper, we provide a comprehensive Bayesian non-parametric analysis of random probabilities which are obtained by normalizing random measures with independent increments (NRMI). Special cases of these priors have already shown to be useful for statistical applications such as mixture models and species sampling problems. However, in order to fully exploit these priors, the derivation of the posterior distribution of NRMIs is crucial: here we achieve this goal and, indeed, provide explicit and tractable expressions suitable for practical implementation. The posterior distribution of an NRMI turns out to be a mixture with respect to the distribution of a specific latent variable. The analysis is completed by the derivation of the corresponding predictive distributions and by a thorough investigation of the marginal structure. These results allow to derive a generalized Blackwell–MacQueen sampling scheme, which is then adapted to cover also mixture models driven by general NRMIs.},
	language = {en},
	number = {1},
	urldate = {2016-04-16},
	journal = {Scandinavian Journal of Statistics},
	author = {James, Lancelot F. and Lijoi, Antonio and Prünster, Igor},
	month = mar,
	year = {2009},
	keywords = {posterior distribution, Bayesian Nonparametrics, Dirichlet process, normalized random measure, Poisson random measure, predictive distribution},
	pages = {76--97},
	file = {Snapshot:/home/jeremiah/Zotero/storage/J88PXRUQ/James et al. - 2009 - Posterior Analysis for Normalized Random Measures .html:text/html}
}

@article{blei_variational_2006,
	title = {Variational inference for {Dirichlet} process mixtures},
	volume = {1},
	issn = {1936-0975, 1931-6690},
	url = {http://projecteuclid.org/euclid.ba/1340371077},
	doi = {10.1214/06-BA104},
	abstract = {Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem.},
	language = {EN},
	number = {1},
	urldate = {2016-04-16},
	journal = {Bayesian Analysis},
	author = {Blei, David M. and Jordan, Michael I.},
	month = mar,
	year = {2006},
	mrnumber = {MR2227367},
	keywords = {Bayesian computation, Dirichlet processes, hierarchical models, image processing, variational inference},
	pages = {121--143}
}

@article{blei_variational_2016,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation about the posterior. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in myriad applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this widely-used class of algorithms.},
	urldate = {2016-04-15},
	journal = {arXiv:1601.00670 [cs, stat]},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00670},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation},
	file = {arXiv\:1601.00670 PDF:/home/jeremiah/Zotero/storage/9W56NMDI/Blei et al. - 2016 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/U3QHVIV7/1601.html:text/html}
}

@article{ren_bayesian_2016,
	title = {Bayesian {Nonparametric} {Ordination} for the {Analysis} of {Microbial} {Communities}},
	url = {http://arxiv.org/abs/1601.05156},
	abstract = {Human microbiome studies use sequencing technologies to measure the abundance of bacterial species or Operational Taxonomic Units (OTUs) in samples of biological material. Typically the data are organized in contingency tables with OTU counts across heterogeneous biological samples. In the microbial ecology community, ordination methods are frequently used to investigate latent factors or clusters that capture and describe variations of OTU counts across biological samples. It remains important to evaluate how uncertainty in estimates of each biological sample's microbial distribution propagates to ordination analyses, including visualization of clusters and projections of biological samples on low dimensional spaces. We propose a Bayesian analysis for dependent distributions to endow frequently used ordinations with estimates of uncertainty. A Bayesian nonparametric prior for dependent normalized random measures is constructed, which is marginally equivalent to the normalized generalized Gamma process, a well-known prior for nonparametric analyses. In our prior the dependence and similarity between microbial distributions is represented by latent factors that concentrate in a low dimensional space. We use a shrinkage prior to tune the dimensionality of the latent factors. The resulting posterior samples of model parameters can be used to evaluate uncertainty in analyses routinely applied in microbiome studies. Specifically, by combining them with multivariate data analysis techniques we can visualize credible regions in ecological ordination plots. The characteristics of the proposed model are illustrated through a simulation study and applications in two microbiome datasets.},
	urldate = {2016-04-15},
	journal = {arXiv:1601.05156 [stat]},
	author = {Ren, Boyu and Bacallado, Sergio and Favaro, Stefano and Holmes, Susan and Trippa, Lorenzo},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.05156},
	keywords = {Statistics - Methodology, Statistics - Applications},
	file = {arXiv\:1601.05156 PDF:/home/jeremiah/Zotero/storage/IFVP9R4B/Ren et al. - 2016 - Bayesian Nonparametric Ordination for the Analysis.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/QR6RW5AI/1601.html:text/html}
}

@article{ishwaran_gibbs_2001,
	title = {Gibbs {Sampling} {Methods} for {Stick}-{Breaking} {Priors}},
	volume = {96},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1198/016214501750332758},
	doi = {10.1198/016214501750332758},
	abstract = {A rich and flexible class of random probability measures, which we call stick-breaking priors, can be constructed using a sequence of independent beta random variables. Examples of random measures that have this characterization include the Dirichlet process, its two-parameter extension, the two-parameter Poisson–Dirichlet process, finite dimensional Dirichlet priors, and beta two-parameter processes. The rich nature of stick-breaking priors offers Bayesians a useful class of priors for nonparametric problems, while the similar construction used in each prior can be exploited to develop a general computational procedure for fitting them. In this article we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors. The first type of Gibbs sampler, referred to as a Pólya urn Gibbs sampler, is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing. This method applies to stick-breaking priors with a known Pólya urn characterization, that is, priors with an explicit and simple prediction rule. Our second method, the blocked Gibbs sampler, is based on an entirely different approach that works by directly sampling values from the posterior of the random measure. The blocked Gibbs sampler can be viewed as a more general approach because it works without requiring an explicit prediction rule. We find that the blocked Gibbs avoids some of the limitations seen with the Pólya urn approach and should be simpler for nonexperts to use.},
	number = {453},
	urldate = {2016-04-15},
	journal = {Journal of the American Statistical Association},
	author = {Ishwaran, Hemant and James, Lancelot F.},
	month = mar,
	year = {2001},
	pages = {161--173},
	file = {Snapshot:/home/jeremiah/Zotero/storage/KGEDE9Z8/Ishwaran and James - 2001 - Gibbs Sampling Methods for Stick-Breaking Priors.html:text/html}
}

@article{voorman_inference_2014,
	title = {Inference in {High} {Dimensions} with the {Penalized} {Score} {Test}},
	url = {http://arxiv.org/abs/1401.2678},
	abstract = {In recent years, there has been considerable theoretical development regarding variable selection consistency of penalized regression techniques, such as the lasso. However, there has been relatively little work on quantifying the uncertainty in these selection procedures. In this paper, we propose a new method for inference in high dimensions using a score test based on penalized regression. In this test, we perform penalized regression of an outcome on all but a single feature, and test for correlation of the residuals with the held-out feature. This procedure is applied to each feature in turn. Interestingly, when an \${\textbackslash}ell\_1\$ penalty is used, the sparsity pattern of the lasso corresponds exactly to a decision based on the proposed test. Further, when an \${\textbackslash}ell\_2\$ penalty is used, the test corresponds precisely to a score test in a mixed effects model, in which the effects of all but one feature are assumed to be random. We formulate the hypothesis being tested as a compromise between the null hypotheses tested in simple linear regression on each feature and in multiple linear regression on all features, and develop reference distributions for some well-known penalties. We also examine the behavior of the test on real and simulated data.},
	urldate = {2016-04-11},
	journal = {arXiv:1401.2678 [stat]},
	author = {Voorman, Arend and Shojaie, Ali and Witten, Daniela},
	month = jan,
	year = {2014},
	note = {arXiv: 1401.2678},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: 32 pages, 5 figures},
	file = {arXiv\:1401.2678 PDF:/home/jeremiah/Zotero/storage/EMTR3NEU/Voorman et al. - 2014 - Inference in High Dimensions with the Penalized Sc.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/CSM452N3/1401.html:text/html}
}

@article{oliva_bayesian_2015,
	title = {Bayesian {Nonparametric} {Kernel}-{Learning}},
	url = {http://arxiv.org/abs/1506.08776},
	abstract = {Kernel methods are ubiquitous tools in machine learning. They have proven to be effective in many domains and tasks. Yet, kernel methods often require the user to select a predefined kernel to build an estimator with. However, there is often little reason for the a priori selection of a kernel. Even if a universal approximating kernel is selected, the quality of the finite sample estimator may be greatly effected by the choice of kernel. Furthermore, when directly applying kernel methods, one typically needs to compute a \$N {\textbackslash}times N\$ Gram matrix of pairwise kernel evaluations to work with a dataset of \$N\$ instances. The computation of this Gram matrix precludes the direct application of kernel methods on large datasets. In this paper we introduce Bayesian nonparmetric kernel (BaNK) learning, a generic, data-driven framework for scalable learning of kernels. We show that this framework can be used for performing both regression and classification tasks and scale to large datasets. Furthermore, we show that BaNK outperforms several other scalable approaches for kernel learning on a variety of real world datasets.},
	urldate = {2016-04-11},
	journal = {arXiv:1506.08776 [stat]},
	author = {Oliva, Junier and Dubey, Avinava and Poczos, Barnabas and Schneider, Jeff and Xing, Eric P.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.08776},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1506.08776 PDF:/home/jeremiah/Zotero/storage/QSAV4XMM/Oliva et al. - 2015 - Bayesian Nonparametric Kernel-Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/SHBGZJNN/Oliva et al. - 2015 - Bayesian Nonparametric Kernel-Learning.html:text/html}
}

@phdthesis{fox_bayesian_2009,
	type = {Thesis},
	title = {Bayesian nonparametric learning of complex dynamical phenomena},
	copyright = {http://dspace.mit.edu/handle/1721.1/7582},
	url = {http://dspace.mit.edu/handle/1721.1/55111},
	abstract = {The complexity of many dynamical phenomena precludes the use of linear models for which exact analytic techniques are available. However, inference on standard nonlinear models quickly becomes intractable. In some cases, Markov switching processes, with switches between a set of simpler models, are employed to describe the observed dynamics. Such models typically rely on pre-specifying the number of Markov modes. In this thesis, we instead take a Bayesian nonparametric approach in defining a prior on the model parameters that allows for flexibility in the complexity of the learned model and for development of efficient inference algorithms. We start by considering dynamical phenomena that can be well-modeled as a hidden discrete Markov process, but in which there is uncertainty about the cardinality of the state space. The standard finite state hidden Markov model (HMM) has been widely applied in speech recognition, digital communications, and bioinformatics, amongst other fields. Through the use of the hierarchical Dirichlet process (HDP), one can examine an HMM with an unbounded number of possible states. We revisit this HDPHMM and develop a generalization of the model, the sticky HDP-HMM, that allows more robust learning of smoothly varying state dynamics through a learned bias towards self-transitions. We show that this sticky HDP-HMM not only better segments data according to the underlying state sequence, but also improves the predictive performance of the learned model. Additionally, the sticky HDP-HMM enables learning more complex, multimodal emission distributions.},
	language = {eng},
	urldate = {2016-03-23},
	school = {Massachusetts Institute of Technology},
	author = {Fox, Emily Beth},
	year = {2009},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/4DWAM8MG/Fox - 2009 - Bayesian nonparametric learning of complex dynamic.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/XH5BDJXB/55111.html:text/html}
}

@article{teh_hierarchical_2006,
	title = {Hierarchical {Dirichlet} {Processes}},
	volume = {101},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/27639773},
	abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the "Chinese restaurant franchise." We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
	number = {476},
	urldate = {2016-03-23},
	journal = {Journal of the American Statistical Association},
	author = {Teh, Yee Whye and Jordan, Michael I. and Beal, Matthew J. and Blei, David M.},
	year = {2006},
	pages = {1566--1581}
}

@misc{center_for_history_and_new_media_zotero_nodate,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}},
	annote = {Welcome to Zotero!View the Quick Start Guide to learn how to begin collecting, managing, citing, and sharing your research sources.Thanks for installing Zotero.}
}

@misc{zhang_theory_2017,
	title = {Theory of {Deep} {Learning} {III}: {Generalization} {Properties} of {SGD}},
	shorttitle = {Theory of {Deep} {Learning} {III}},
	author = {Zhang, Chiyuan and Liao, Qianli and Rakhlin, Alexander and Sridharan, Karthik and Miranda, Brando and Golowich, Noah and Poggio, Tomaso},
	year = {2017},
	file = {Theory of Deep Learning III\: Generalization Properties of SGD | The Center for Brains, Minds & Machines:/home/jeremiah/Zotero/storage/X7HAI3HB/theory-deep-learning-iii-generalization-properties-sgd.html:text/html}
}

@misc{poggio_theory_2017,
	title = {Theory {II}: {Landscape} of the {Empirical} {Risk} in {Deep} {Learning}},
	shorttitle = {Theory {II}},
	author = {Poggio, Tomaso and Liao, Qianli and Liao, Qianli and Liao, Qianli},
	year = {2017},
	file = {Theory II\: Landscape of the Empirical Risk in Deep Learning | The Center for Brains, Minds & Machines:/home/jeremiah/Zotero/storage/5JCHW6SP/theory-ii-landscape-empirical-risk-deep-learning.html:text/html}
}

@article{neyshabur_search_2014,
	title = {In {Search} of the {Real} {Inductive} {Bias}: {On} the {Role} of {Implicit} {Regularization} in {Deep} {Learning}},
	shorttitle = {In {Search} of the {Real} {Inductive} {Bias}},
	url = {http://arxiv.org/abs/1412.6614},
	abstract = {We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.},
	urldate = {2017-05-22},
	journal = {arXiv:1412.6614 [cs, stat]},
	author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6614},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 9 pages, 2 figures},
	file = {arXiv\:1412.6614 PDF:/home/jeremiah/Zotero/storage/4KUW27Z6/Neyshabur et al. - 2014 - In Search of the Real Inductive Bias On the Role .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MJ83I66D/1412.html:text/html}
}

@article{cai_optimal_2013,
	title = {Optimal hypothesis testing for high dimensional covariance matrices},
	volume = {19},
	issn = {1350-7265},
	url = {http://projecteuclid.org/euclid.bj/1386078606},
	doi = {10.3150/12-BEJ455},
	abstract = {This paper considers testing a covariance matrix ΣΣ{\textbackslash}Sigma in the high dimensional setting where the dimension ppp can be comparable or much larger than the sample size nnn. The problem of testing the hypothesis H0:Σ=Σ0H0:Σ=Σ0H\_\{0\}:{\textbackslash}Sigma={\textbackslash}Sigma\_\{0\} for a given covariance matrix Σ0Σ0{\textbackslash}Sigma\_\{0\} is studied from a minimax point of view. We first characterize the boundary that separates the testable region from the non-testable region by the Frobenius norm when the ratio between the dimension ppp over the sample size nnn is bounded. A test based on a UUU-statistic is introduced and is shown to be rate optimal over this asymptotic regime. Furthermore, it is shown that the power of this test uniformly dominates that of the corrected likelihood ratio test (CLRT) over the entire asymptotic regime under which the CLRT is applicable. The power of the UUU-statistic based test is also analyzed when p/np/np/n is unbounded.},
	language = {EN},
	number = {5B},
	urldate = {2017-05-22},
	journal = {Bernoulli},
	author = {Cai, T. Tony and Ma, Zongming},
	month = nov,
	year = {2013},
	mrnumber = {MR3160557},
	zmnumber = {1281.62140},
	keywords = {power, High-dimensional data, correlation matrix, covariance matrix, likelihood ratio test, minimax hypothesis testing, testing covariance structure},
	pages = {2359--2388},
	file = {Snapshot:/home/jeremiah/Zotero/storage/EJ2XHJKF/1386078606.html:text/html}
}

@incollection{kanagawa_convergence_2016,
	title = {Convergence guarantees for kernel-based quadrature rules in misspecified settings},
	url = {http://papers.nips.cc/paper/6174-convergence-guarantees-for-kernel-based-quadrature-rules-in-misspecified-settings.pdf},
	urldate = {2017-05-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Kanagawa, Motonobu and Sriperumbudur, Bharath K. and Fukumizu, Kenji},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {3288--3296},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/PKAPNW6M/Kanagawa et al. - 2016 - Convergence guarantees for kernel-based quadrature.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/7EBZZHDR/6174-convergence-guarantees-for-kernel-based-quadrature-rules-in-misspecified-settings.html:text/html}
}

@article{gallant_testing_1977,
	title = {Testing a {Nonlinear} {Regression} {Specification}: {A} {Nonregular} {Case}},
	volume = {72},
	issn = {0162-1459},
	shorttitle = {Testing a {Nonlinear} {Regression} {Specification}},
	url = {http://www.jstor.org/stable/2286210},
	doi = {10.2307/2286210},
	abstract = {A statistical test of whether an additive nonlinear term in the response function should be omitted from a nonlinear regression specification is considered. The regularity conditions used to obtain the asymptotic distributions of the customary test statistics are violated when the null hypothesis of omission is true. Moreover, standard iterative algorithms are likely to perform poorly when the data support the null hypothesis. Methods designed to circumvent these mathematical and computational difficulties are described and illustrated.},
	number = {359},
	urldate = {2017-05-12},
	journal = {Journal of the American Statistical Association},
	author = {Gallant, A. Ronald},
	year = {1977},
	pages = {523--530}
}

@article{liu_hypothesis_2004,
	title = {Hypothesis testing in smoothing spline models},
	volume = {74},
	issn = {0094-9655},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00949650310001623416},
	doi = {10.1080/00949650310001623416},
	abstract = {Nonparametric regression models are often used to check or suggest a parametric model. Several methods have been proposed to test the hypothesis of a parametric regression function against an alternative smoothing spline model. Some tests such as the locally most powerful (LMP) test by Cox et al. (Cox, D., Koh, E., Wahba, G. and Yandell, B. (1988). Testing the (parametric) null model hypothesis in (semiparametric) partial and generalized spline models. Ann. Stat., 16, 113–119.), the generalized maximum likelihood (GML) ratio test and the generalized cross validation (GCV) test by Wahba (Wahba, G. (1990). Spline models for observational data. CBMS-NSF Regional Conference Series in Applied Mathematics, SIAM.) were developed from the corresponding Bayesian models. Their frequentist properties have not been studied. We conduct simulations to evaluate and compare finite sample performances. Simulation results show that the performances of these tests depend on the shape of the true function. The LMP and GML tests are more powerful for low frequency functions while the GCV test is more powerful for high frequency functions. For all test statistics, distributions under the null hypothesis are complicated. Computationally intensive Monte Carlo methods can be used to calculate null distributions. We also propose approximations to these null distributions and evaluate their performances by simulations. E-mail: yuedong@pstat.ucsb.edu E-mail: annaliu@pstat.ucsb.edu},
	number = {8},
	urldate = {2017-05-12},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Liu, Anna and Wang, Yuedong},
	month = aug,
	year = {2004},
	keywords = {Bayesian models for smoothing splines, Connections between linear mixed effects models and smoothing splines, F-test, GCV test, GML test, LMP test, Symmetrized Kullback-Leibler test},
	pages = {581--597},
	file = {Snapshot:/home/jeremiah/Zotero/storage/TVMTE4B7/00949650310001623416.html:text/html}
}

@article{lee_testing_1993,
	title = {Testing for neglected nonlinearity in time series models},
	volume = {56},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/030440769390122L},
	doi = {10.1016/0304-4076(93)90122-L},
	abstract = {In this paper a new test, the neural network test for neglected nonlinearity, is compared with the Keenan test, the Tsay test, the White dynamic information matrix test, the McLeod-Li test, the Ramsey RESET test, the Brock-Dechert-Scheinkman test, and the Bispectrum test. The neural network test is based on the approximating ability of neural network modeling techniques recently developed by cognitive scientists. This test is a Lagrange multiplier test that statistically determines whether adding ‘hidden units’ to the linear network would be advantageous. The performance of the tests is compared using a variety of nonlinear artificial series including bilinear, threshold autoregressive, and nonlinear moving average models, and the tests are applied to actual economic time series. The relative performance of the neural network test is encouraging. Our results suggest that it can play a valuable role in evaluating model adequacy. The neural network test has proper size and good power, and many of the economic series tested exhibit potential nonlinearities.},
	number = {3},
	urldate = {2017-05-12},
	journal = {Journal of Econometrics},
	author = {Lee, Tae-Hwy and White, Halbert and Granger, Clive W. J.},
	month = apr,
	year = {1993},
	pages = {269--290},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/X5BXWN83/030440769390122L.html:text/html}
}

@article{hamilton_parametric_2001,
	title = {A {Parametric} {Approach} to {Flexible} {Nonlinear} {Inference}},
	volume = {69},
	issn = {0012-9682},
	url = {http://www.jstor.org/stable/2692201},
	abstract = {This paper proposes a new framework for determining whether a given relationship is nonlinear, what the nonlinearity looks like, and whether it is adequately described by a particular parametric model. The paper studies a regression or forecasting model of the form {\textless}tex-math{\textgreater}\$y\_\{t\}={\textbackslash}mu (\{{\textbackslash}bf x\}\_\{t\})+{\textbackslash}varepsilon \_\{t\}\${\textless}/tex-math{\textgreater} where the functional form of μ(·) is unknown. We propose viewing μ(·) itself as the outcome of a random process. The paper introduces a new stationary random field m(·) that generalizes finite-differenced Brownian motion to a vector field and whose realizations could represent a broad class of possible forms for μ(·). We view the parameters that characterize the relation between a given realization of m(·) and the particular value of μ(·) for a given sample as population parameters to be estimated by maximum likelihood or Bayesian methods. We show that the resulting inference about the functional relation also yields consistent estimates for a broad class of deterministic functions μ(·). The paper further develops a new test of the null hypothesis of linearity based on the Lagrange multiplier principle and small-sample confidence intervals based on numerical Bayesian methods. An empirical application suggests that properly accounting for the nonlinearity of the inflation-unemployment trade-off may explain the previously reported uneven empirical success of the Phillips Curve.},
	number = {3},
	urldate = {2017-05-12},
	journal = {Econometrica},
	author = {Hamilton, James D.},
	year = {2001},
	pages = {537--573}
}

@article{lee_testing_1993-1,
	title = {Testing for neglected nonlinearity in time series models},
	volume = {56},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/030440769390122L},
	doi = {10.1016/0304-4076(93)90122-L},
	abstract = {In this paper a new test, the neural network test for neglected nonlinearity, is compared with the Keenan test, the Tsay test, the White dynamic information matrix test, the McLeod-Li test, the Ramsey RESET test, the Brock-Dechert-Scheinkman test, and the Bispectrum test. The neural network test is based on the approximating ability of neural network modeling techniques recently developed by cognitive scientists. This test is a Lagrange multiplier test that statistically determines whether adding ‘hidden units’ to the linear network would be advantageous. The performance of the tests is compared using a variety of nonlinear artificial series including bilinear, threshold autoregressive, and nonlinear moving average models, and the tests are applied to actual economic time series. The relative performance of the neural network test is encouraging. Our results suggest that it can play a valuable role in evaluating model adequacy. The neural network test has proper size and good power, and many of the economic series tested exhibit potential nonlinearities.},
	number = {3},
	urldate = {2017-05-12},
	journal = {Journal of Econometrics},
	author = {Lee, Tae-Hwy and White, Halbert and Granger, Clive W. J.},
	month = apr,
	year = {1993},
	pages = {269--290},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/MC5EEB4F/030440769390122L.html:text/html}
}

@article{carpentier_testing_2015,
	title = {Testing the regularity of a smooth signal},
	volume = {21},
	issn = {1350-7265},
	url = {http://projecteuclid.org.ezp-prod1.hul.harvard.edu/euclid.bj/1426597078},
	doi = {10.3150/13-BEJ575},
	abstract = {We develop a test to determine whether a function lying in a fixed L2L2L\_\{2\}-Sobolev-type ball of smoothness ttt, and generating a noisy signal, is in fact of a given smoothness s≥ts≥ts{\textbackslash}geq t or not. While it is impossible to construct a uniformly consistent test for this problem on every function of smoothness ttt, it becomes possible if we remove a sufficiently large region of the set of functions of smoothness ttt. The functions that we remove are functions of smoothness strictly smaller than sss, but that are very close to sss-smooth functions. A lower bound on the size of this region has been proved to be of order n−t/(2t+1/2)n−t/(2t+1/2)n{\textasciicircum}\{-t/(2t+1/2)\}, and in this paper, we provide a test that is consistent after the removal of a region of such a size. Even though the null hypothesis is composite, the size of the region we remove does not depend on the complexity of the null hypothesis.},
	language = {EN},
	number = {1},
	urldate = {2017-05-12},
	journal = {Bernoulli},
	author = {Carpentier, Alexandra},
	month = feb,
	year = {2015},
	mrnumber = {MR3322327},
	zmnumber = {1320.94021},
	keywords = {functional analysis, minimax bounds, non-parametric composite testing problem},
	pages = {465--488},
	file = {Snapshot:/home/jeremiah/Zotero/storage/3REWXSWE/1426597078.html:text/html}
}

@article{greene_testing_2010,
	title = {Testing hypotheses about interaction terms in nonlinear models},
	volume = {107},
	issn = {0165-1765},
	url = {http://www.sciencedirect.com/science/article/pii/S0165176510000777},
	doi = {10.1016/j.econlet.2010.02.014},
	abstract = {We examine the interaction effect in nonlinear models discussed by Ai and Norton (2003). Tests about partial effects and interaction terms are not necessarily informative in the context of the model. We suggest ways to examine the effects that do not involve statistical testing.},
	number = {2},
	urldate = {2017-05-12},
	journal = {Economics Letters},
	author = {Greene, William},
	month = may,
	year = {2010},
	keywords = {Interaction effect, Interaction term, Nonlinear models, Partial effect, Probit},
	pages = {291--296},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/IEBBI35D/Greene - 2010 - Testing hypotheses about interaction terms in nonl.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/TZGS9UNG/S0165176510000777.html:text/html}
}

@article{lee_robust_2015,
	title = {Robust hypothesis tests for {M}-estimators with possibly non-differentiable estimating functions},
	volume = {18},
	issn = {1368-423X},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/ectj.12041/abstract},
	doi = {10.1111/ectj.12041},
	abstract = {We propose a new robust hypothesis test for (possibly non-linear) constraints on M-estimators with possibly non-differentiable estimating functions. The proposed test employs a random normalizing matrix computed from recursive M-estimators to eliminate the nuisance parameters arising from the asymptotic covariance matrix. It does not require consistent estimation of any nuisance parameters, in contrast with the conventional heteroscedasticity-autocorrelation consistent (HAC)-type test and the Kiefer–Vogelsang–Bunzel (KVB)-type test. Our test reduces to the KVB-type test in simple location models with ordinary least-squares estimation, so the error in the rejection probability of our test in a Gaussian location model is Op(T−1logT). We discuss robust testing in quantile regression, and censored regression models in detail. In simulation studies, we find that our test has better size control and better finite sample power than the HAC-type and KVB-type tests.},
	language = {en},
	number = {1},
	urldate = {2017-05-12},
	journal = {The Econometrics Journal},
	author = {Lee, Wei-Ming and Hsu, Yu-Chin and Kuan, Chung-Ming},
	month = feb,
	year = {2015},
	keywords = {Censored regression, Generalized method of moments, KVB approach, M-estimator, Quantile regression, Robust hypothesis testing},
	pages = {95--116},
	file = {Snapshot:/home/jeremiah/Zotero/storage/NM33DK5B/abstract.html:text/html}
}

@article{cortes_two-stage_2010,
	title = {Two-{Stage} {Learning} {Kernel} {Algorithms}},
	url = {https://research.google.com/pubs/pub36468.html},
	urldate = {2017-05-12},
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	year = {2010},
	file = {Snapshot:/home/jeremiah/Zotero/storage/U4UWTERC/pub36468.html:text/html}
}

@article{du_sharing_2017,
	title = {Sharing deep generative representation for perceived image reconstruction from human brain activity},
	url = {http://arxiv.org/abs/1704.07575},
	abstract = {Decoding human brain activities via functional magnetic resonance imaging (fMRI) has gained increasing attention in recent years. While encouraging results have been reported in brain states classification tasks, reconstructing the details of human visual experience still remains difficult. Two main challenges that hinder the development of effective models are the perplexing fMRI measurement noise and the high dimensionality of limited data instances. Existing methods generally suffer from one or both of these issues and yield dissatisfactory results. In this paper, we tackle this problem by casting the reconstruction of visual stimulus as the Bayesian inference of missing view in a multiview latent variable model. Sharing a common latent representation, our joint generative model of external stimulus and brain response is not only "deep" in extracting nonlinear features from visual images, but also powerful in capturing correlations among voxel activities of fMRI recordings. The nonlinearity and deep structure endow our model with strong representation ability, while the correlations of voxel activities are critical for suppressing noise and improving prediction. We devise an efficient variational Bayesian method to infer the latent variables and the model parameters. To further improve the reconstruction accuracy, the latent representations of testing instances are enforced to be close to that of their neighbours from the training set via posterior regularization. Experiments on three fMRI recording datasets demonstrate that our approach can more accurately reconstruct visual stimuli.},
	urldate = {2017-05-10},
	journal = {arXiv:1704.07575 [cs, q-bio]},
	author = {Du, Changde and Du, Changying and He, Huiguang},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.07575},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	file = {arXiv\:1704.07575 PDF:/home/jeremiah/Zotero/storage/E2HV9SNP/Du et al. - 2017 - Sharing deep generative representation for perceiv.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/72QKTP97/1704.html:text/html}
}

@article{lin_nonparametric_2000,
	title = {Nonparametric {Function} {Estimation} for {Clustered} {Data} {When} the {Predictor} is {Measured} without/with {Error}},
	volume = {95},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2669396},
	doi = {10.2307/2669396},
	abstract = {We consider local polynomial kernel regression with a single covariate for clustered data using estimating equations. We assume that at most {\textless}latex{\textgreater}\$m {\textless} {\textbackslash}infty\${\textless}/latex{\textgreater} observations are available on each cluster. In the case of random regressors, with no measurement error in the predictor, we show that it is generally the best strategy to ignore entirely the correlation structure within each cluster and instead pretend that all observations are independent. In the further special case of longitudinal data on individuals with fixed common observation times, we show that equivalent to the pooled data approach is the strategy of fitting separate nonparametric regressions at each observation time and constructing an optimal weighted average. We also consider what happens when the predictor is measured with error. Using the SIMEX approach to correct for measurement error, we construct an asymptotic theory for both the pooled and the weighted average estimators. Surprisingly, for the same amount of smoothing, the weighted average estimators typically have smaller variances than the pooling strategy. We apply the proposed methods to analysis of the AIDS Costs and Services Utilization Survey.},
	number = {450},
	urldate = {2017-04-30},
	journal = {Journal of the American Statistical Association},
	author = {Lin, Xihong and Carroll, Raymond J.},
	year = {2000},
	pages = {520--534},
	file = {JSTOR Full Text PDF:/home/jeremiah/Zotero/storage/35GVMQXC/Lin and Carroll - 2000 - Nonparametric Function Estimation for Clustered Da.pdf:application/pdf}
}

@article{lin_nonparametric_2000-1,
	title = {Nonparametric {Function} {Estimation} for {Clustered} {Data} {When} the {Predictor} is {Measured} without/with {Error}},
	volume = {95},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2669396},
	doi = {10.2307/2669396},
	abstract = {We consider local polynomial kernel regression with a single covariate for clustered data using estimating equations. We assume that at most {\textless}latex{\textgreater}\$m {\textless} {\textbackslash}infty\${\textless}/latex{\textgreater} observations are available on each cluster. In the case of random regressors, with no measurement error in the predictor, we show that it is generally the best strategy to ignore entirely the correlation structure within each cluster and instead pretend that all observations are independent. In the further special case of longitudinal data on individuals with fixed common observation times, we show that equivalent to the pooled data approach is the strategy of fitting separate nonparametric regressions at each observation time and constructing an optimal weighted average. We also consider what happens when the predictor is measured with error. Using the SIMEX approach to correct for measurement error, we construct an asymptotic theory for both the pooled and the weighted average estimators. Surprisingly, for the same amount of smoothing, the weighted average estimators typically have smaller variances than the pooling strategy. We apply the proposed methods to analysis of the AIDS Costs and Services Utilization Survey.},
	number = {450},
	urldate = {2017-04-30},
	journal = {Journal of the American Statistical Association},
	author = {Lin, Xihong and Carroll, Raymond J.},
	year = {2000},
	pages = {520--534}
}

@inproceedings{genton_classes_2001,
	title = {Classes of {Kernels} for {Machine} {Learning}: {A} {Statistics} {Perspective}},
	shorttitle = {Classes of {Kernels} for {Machine} {Learning}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/Genton01},
	urldate = {2017-04-24},
	author = {Genton, Marc G.},
	year = {2001},
	pages = {299--312},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/2WIQUPM4/Genton - 2001 - Classes of Kernels for Machine Learning A Statist.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/KZEC5STM/Genton01.html:text/html}
}

@article{williams_computation_1998,
	title = {Computation with {Infinite} {Neural} {Networks}},
	volume = {10},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org.ezp-prod1.hul.harvard.edu/doi/10.1162/089976698300017412},
	doi = {10.1162/089976698300017412},
	number = {5},
	urldate = {2017-04-18},
	journal = {Neural Computation},
	author = {Williams, Christopher K. I.},
	month = jul,
	year = {1998},
	pages = {1203--1216},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/K9JQ7HKE/Williams - 1998 - Computation with Infinite Neural Networks.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/FTWEGXHJ/089976698300017412.html:text/html}
}

@article{cho_large-margin_2010,
	title = {Large-{Margin} {Classification} in {Infinite} {Neural} {Networks}},
	volume = {22},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org.ezp-prod1.hul.harvard.edu/doi/10.1162/NECO_a_00018},
	doi = {10.1162/NECO_a_00018},
	number = {10},
	urldate = {2017-04-18},
	journal = {Neural Computation},
	author = {Cho, Youngmin and Saul, Lawrence K.},
	month = jul,
	year = {2010},
	pages = {2678--2697},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/XFZ78JJW/Cho and Saul - 2010 - Large-Margin Classification in Infinite Neural Net.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/QTV958AS/NECO_a_00018.html:text/html}
}

@article{mitrovic_deep_2017,
	title = {Deep {Kernel} {Machines} via the {Kernel} {Reparametrization} {Trick}},
	url = {https://openreview.net/forum?id=Bkiqt3Ntg&noteId=Bkiqt3Ntg},
	urldate = {2017-04-18},
	author = {Mitrovic, Jovana and Sejdinovic, Dino and Teh, Yee Whye},
	month = feb,
	year = {2017},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/D5PFZNN6/Mitrovic et al. - 2017 - Deep Kernel Machines via the Kernel Reparametrizat.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/82U65WBV/forum.html:text/html}
}

@article{hazan_steps_2015,
	title = {Steps {Toward} {Deep} {Kernel} {Methods} from {Infinite} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1508.05133},
	abstract = {Contemporary deep neural networks exhibit impressive results on practical problems. These networks generalize well although their inherent capacity may extend significantly beyond the number of training examples. We analyze this behavior in the context of deep, infinite neural networks. We show that deep infinite layers are naturally aligned with Gaussian processes and kernel methods, and devise stochastic kernels that encode the information of these networks. We show that stability results apply despite the size, offering an explanation for their empirical success.},
	urldate = {2017-04-18},
	journal = {arXiv:1508.05133 [cs]},
	author = {Hazan, Tamir and Jaakkola, Tommi},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.05133},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1508.05133 PDF:/home/jeremiah/Zotero/storage/Q422I3E9/Hazan and Jaakkola - 2015 - Steps Toward Deep Kernel Methods from Infinite Neu.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/28PSSQA3/1508.html:text/html}
}

@article{montavon_kernel_2011,
	title = {Kernel {Analysis} of {Deep} {Networks}},
	volume = {12},
	url = {http://jmlr.csail.mit.edu/papers/v12/montavon11a.html},
	abstract = {When training deep networks it is common knowledge that an efficient and well generalizing representation of the problem is formed. In this paper we aim to elucidate what makes the emerging representation successful. We analyze the layer-wise evolution of the representation in a deep network by building a sequence of deeper and deeper kernels that subsume the mapping performed by more and more layers of the deep network and measuring how these increasingly complex kernels fit the learning problem. We observe that deep networks create increasingly better representations of the learning problem and that the structure of the deep network controls how fast the representation of the task is formed layer after layer.},
	urldate = {2017-04-18},
	journal = {Journal of Machine Learning Research},
	author = {Montavon, Grégoire and Braun, Mikio L. and Müller, Klaus-Robert},
	month = sep,
	year = {2011},
	pages = {2563−2581},
	file = {Kernel Analysis of Deep Networks:/home/jeremiah/Zotero/storage/AR93S3WU/Montavon et al. - 2011 - Kernel Analysis of Deep Networks.pdf:application/pdf}
}

@incollection{cho_kernel_2009,
	title = {Kernel {Methods} for {Deep} {Learning}},
	url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf},
	urldate = {2017-04-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Cho, Youngmin and Saul, Lawrence K.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {342--350},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/7DCF9723/Cho and Saul - 2009 - Kernel Methods for Deep Learning.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/W5HXRUSI/3628-kernel-methods-for-deep-learning.html:text/html}
}

@article{bzdok_classical_2016,
	title = {Classical {Statistics} and {Statistical} {Learning} in {Imaging} {Neuroscience}},
	url = {http://arxiv.org/abs/1603.01857},
	abstract = {Neuroimaging research has predominantly drawn conclusions based on classical statistics, including null-hypothesis testing, t-tests, and ANOVA. Throughout recent years, statistical learning methods enjoy increasing popularity, including cross-validation, pattern classification, and sparsity-inducing regression. These two methodological families used for neuroimaging data analysis can be viewed as two extremes of a continuum. Yet, they originated from different historical contexts, build on different theories, rest on different assumptions, evaluate different outcome metrics, and permit different conclusions. This paper portrays commonalities and differences between classical statistics and statistical learning with their relation to neuroimaging research. The conceptual implications are illustrated in three common analysis scenarios. It is thus tried to resolve possible confusion between classical hypothesis testing and data-guided model estimation by discussing their ramifications for the neuroimaging access to neurobiology.},
	urldate = {2017-04-17},
	journal = {arXiv:1603.01857 [q-bio, stat]},
	author = {Bzdok, Danilo},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.01857},
	keywords = {Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 61 pages},
	file = {arXiv\:1603.01857 PDF:/home/jeremiah/Zotero/storage/5I67UGPN/Bzdok - 2016 - Classical Statistics and Statistical Learning in I.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/B4PJ844W/1603.html:text/html}
}

@article{van_der_laan_super_2007-1,
	title = {Super learner},
	volume = {6},
	issn = {1544-6115},
	doi = {10.2202/1544-6115.1309},
	abstract = {When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox. A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression. Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners. Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner. This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners. In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions. This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.},
	language = {eng},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {van der Laan, Mark J. and Polley, Eric C. and Hubbard, Alan E.},
	year = {2007},
	pmid = {17910531},
	keywords = {Models, Statistical, Algorithms, Artificial Intelligence},
	pages = {Article25}
}

@article{pearce_penalized_2006,
	title = {Penalized {Splines} and {Reproducing} {Kernel} {Methods}},
	volume = {60},
	issn = {0003-1305},
	url = {http://dx.doi.org/10.1198/000313006X124541},
	doi = {10.1198/000313006X124541},
	abstract = {Two data analytic research areas—penalized splines and reproducing kernel methods—have become very vibrant since the mid-1990s. This article shows how the former can be embedded in the latter via theory for reproducing kernel Hilbert spaces. This connection facilitates cross-fertilization between the two bodies of research. In particular, connections between support vector machines and penalized splines are established. These allow for significant reductions in computational complexity, and easier incorporation of special structure such as additivity.},
	number = {3},
	urldate = {2017-04-13},
	journal = {The American Statistician},
	author = {Pearce, N. D. and Wand, M. P.},
	month = aug,
	year = {2006},
	keywords = {Kernel machines, Bioinformatics, Classification, Data mining, Generalized additive models, Machine learning, Mixed models, Reproducing kernel hilbert spaces, Semi-parametric regression, Statistical learning, Supervised learning, Support vector machines},
	pages = {233--240},
	file = {Snapshot:/home/jeremiah/Zotero/storage/9DBRCTFI/000313006X124541.html:text/html}
}

@article{chen_incorporating_2014,
	title = {Incorporating gene-environment interaction in testing for association with rare genetic variants},
	volume = {78},
	issn = {1423-0062},
	doi = {10.1159/000363347},
	abstract = {OBJECTIVES: The incorporation of gene-environment interactions could improve the ability to detect genetic associations with complex traits. For common genetic variants, single-marker interaction tests and joint tests of genetic main effects and gene-environment interaction have been well-established and used to identify novel association loci for complex diseases and continuous traits. For rare genetic variants, however, single-marker tests are severely underpowered due to the low minor allele frequency, and only a few gene-environment interaction tests have been developed. We aimed at developing powerful and computationally efficient tests for gene-environment interaction with rare variants.
METHODS: In this paper, we propose interaction and joint tests for testing gene-environment interaction of rare genetic variants. Our approach is a generalization of existing gene-environment interaction tests for multiple genetic variants under certain conditions.
RESULTS: We show in our simulation studies that our interaction and joint tests have correct type I errors, and that the joint test is a powerful approach for testing genetic association, allowing for gene-environment interaction. We also illustrate our approach in a real data example from the Framingham Heart Study.
CONCLUSION: Our approach can be applied to both binary and continuous traits, it is powerful and computationally efficient.},
	language = {eng},
	number = {2},
	journal = {Human Heredity},
	author = {Chen, Han and Meigs, James B. and Dupuis, Josée},
	year = {2014},
	pmid = {25060534},
	pmcid = {PMC4169076},
	keywords = {Computer Simulation, Humans, Models, Genetic, Gene-Environment Interaction, Genetic Variation, Body Mass Index, Blood Glucose, Genotype, Insulin},
	pages = {81--90}
}

@article{chui_deep_2016,
	title = {Deep nets for local manifold learning},
	url = {http://arxiv.org/abs/1607.07110},
	abstract = {The problem of extending a function \$f\$ defined on a training data \${\textbackslash}mathcal\{C\}\$ on an unknown manifold \${\textbackslash}mathbb\{X\}\$ to the entire manifold and a tubular neighborhood of this manifold is considered in this paper. For \${\textbackslash}mathbb\{X\}\$ embedded in a high dimensional ambient Euclidean space \${\textbackslash}mathbb\{R\}{\textasciicircum}D\$, a deep learning algorithm is developed for finding a local coordinate system for the manifold \{{\textbackslash}bf without eigen--decomposition\}, which reduces the problem to the classical problem of function approximation on a low dimensional cube. Deep nets (or multilayered neural networks) are proposed to accomplish this approximation scheme by using the training data. Our methods do not involve such optimization techniques as back--propagation, while assuring optimal (a priori) error bounds on the output in terms of the number of derivatives of the target function. In addition, these methods are universal, in that they do not require a prior knowledge of the smoothness of the target function, but adjust the accuracy of approximation locally and automatically, depending only upon the local smoothness of the target function. Our ideas are easily extended to solve both the pre--image problem and the out--of--sample extension problem, with a priori bounds on the growth of the function thus extended.},
	urldate = {2017-04-08},
	journal = {arXiv:1607.07110 [cs]},
	author = {Chui, Charles K. and Mhaskar, H. N.},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.07110},
	keywords = {Computer Science - Learning},
	annote = {Comment: Submitted on Sept. 17, 2015},
	file = {arXiv\:1607.07110 PDF:/home/jeremiah/Zotero/storage/WN9IIDSD/Chui and Mhaskar - 2016 - Deep nets for local manifold learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/B3I2M3UG/1607.html:text/html}
}

@article{basri_efficient_2016,
	title = {Efficient {Representation} of {Low}-{Dimensional} {Manifolds} using {Deep} {Networks}},
	url = {http://arxiv.org/abs/1602.04723},
	abstract = {We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space. We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data. We first show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. We then extend these results to more general manifolds.},
	urldate = {2017-04-08},
	journal = {arXiv:1602.04723 [cs, stat]},
	author = {Basri, Ronen and Jacobs, David},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.04723},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1602.04723 PDF:/home/jeremiah/Zotero/storage/PSNTUQU2/Basri and Jacobs - 2016 - Efficient Representation of Low-Dimensional Manifo.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/HE2ANAM6/1602.html:text/html}
}

@article{li_gene-centric_2012,
	title = {Gene-centric gene–gene interaction: {A} model-based kernel machine method},
	volume = {6},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Gene-centric gene–gene interaction},
	url = {http://projecteuclid.org/euclid.aoas/1346418577},
	doi = {10.1214/12-AOAS545},
	abstract = {Much of the natural variation for a complex trait can be explained by variation in DNA sequence levels. As part of sequence variation, gene–gene interaction has been ubiquitously observed in nature, where its role in shaping the development of an organism has been broadly recognized. The identification of interactions between genetic factors has been progressively pursued via statistical or machine learning approaches. A large body of currently adopted methods, either parametrically or nonparametrically, predominantly focus on pairwise single marker interaction analysis. As genes are the functional units in living organisms, analysis by focusing on a gene as a system could potentially yield more biologically meaningful results. In this work, we conceptually propose a gene-centric framework for genome-wide gene–gene interaction detection. We treat each gene as a testing unit and derive a model-based kernel machine method for two-dimensional genome-wide scanning of gene–gene interactions. In addition to the biological advantage, our method is statistically appealing because it reduces the number of hypotheses tested in a genome-wide scan. Extensive simulation studies are conducted to evaluate the performance of the method. The utility of the method is further demonstrated with applications to two real data sets. Our method provides a conceptual framework for the identification of gene–gene interactions which could shed novel light on the etiology of complex diseases.},
	language = {EN},
	number = {3},
	urldate = {2017-04-07},
	journal = {The Annals of Applied Statistics},
	author = {Li, Shaoyu and Cui, Yuehua},
	month = sep,
	year = {2012},
	mrnumber = {MR3012524},
	zmnumber = {06096525},
	keywords = {reproducing kernel Hilbert space, Quantitative traits, Allele matching kernel, association study, gene-clustered SNPs, genomic similarity},
	pages = {1134--1161},
	file = {Snapshot:/home/jeremiah/Zotero/storage/JJZ4S47G/1346418577.html:text/html}
}

@article{evgeniou_leave_2004,
	title = {Leave {One} {Out} {Error}, {Stability}, and {Generalization} of {Voting} {Combinations} of {Classifiers}},
	volume = {55},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/B:MACH.0000019805.88351.60},
	doi = {10.1023/B:MACH.0000019805.88351.60},
	abstract = {We study the leave-one-out and generalization errors of voting combinations of learning machines. A special case considered is a variant of bagging. We analyze in detail combinations of kernel machines, such as support vector machines, and present theoretical estimates of their leave-one-out error. We also derive novel bounds on the stability of combinations of any classifiers. These bounds can be used to formally show that, for example, bagging increases the stability of unstable learning machines. We report experiments supporting the theoretical findings.},
	language = {en},
	number = {1},
	urldate = {2017-04-07},
	journal = {Machine Learning},
	author = {Evgeniou, Theodoros and Pontil, Massimiliano and Elisseeff, André},
	month = apr,
	year = {2004},
	pages = {71--97},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/M3A9NRRM/Evgeniou et al. - 2004 - Leave One Out Error, Stability, and Generalization.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/SBBSPJES/BMACH.0000019805.88351.html:text/html}
}

@article{elisseeff_stability_2005,
	title = {Stability of {Randomized} {Learning} {Algorithms}},
	volume = {6},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1046920.1046923},
	abstract = {We extend existing theory on stability, namely how much changes in the training data influence the estimated models, and generalization performance of deterministic learning algorithms to the case of randomized algorithms. We give formal definitions of stability for randomized algorithms and prove non-asymptotic bounds on the difference between the empirical and expected error as well as the leave-one-out and expected error of such algorithms that depend on their random stability. The setup we develop for this purpose can be also used for generally studying randomized learning algorithms. We then use these general results to study the effects of bagging on the stability of a learning method and to prove non-asymptotic bounds on the predictive performance of bagging which have not been possible to prove with the existing theory of stability for deterministic learning algorithms.},
	urldate = {2017-04-06},
	journal = {J. Mach. Learn. Res.},
	author = {Elisseeff, Andre and Evgeniou, Theodoros and Pontil, Massimiliano},
	month = dec,
	year = {2005},
	pages = {55--79}
}

@incollection{elisseeff_leave-one-out_2002,
	title = {Leave-one-out {Error} and {Stability} of {Learning} {Algorithms} with {Applications}},
	booktitle = {Learning {Theory} and {Practice}},
	publisher = {IOS Press},
	author = {Elisseeff, A and Pontil, M},
	editor = {Suykens, J and Horvath, G and Basu, S and Micchelli, C and Vandewalle, J},
	year = {2002},
	keywords = {cj-bib}
}

@article{arias-castro_global_2011,
	title = {Global testing under sparse alternatives: {ANOVA}, multiple comparisons and the higher criticism},
	volume = {39},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Global testing under sparse alternatives},
	url = {http://projecteuclid.org/euclid.aos/1322663467},
	doi = {10.1214/11-AOS910},
	abstract = {Testing for the significance of a subset of regression coefficients in a linear model, a staple of statistical analysis, goes back at least to the work of Fisher who introduced the analysis of variance (ANOVA). We study this problem under the assumption that the coefficient vector is sparse, a common situation in modern high-dimensional settings. Suppose we have p covariates and that under the alternative, the response only depends upon the order of p1−α of those, 0 ≤ α ≤ 1. Under moderate sparsity levels, that is, 0 ≤ α ≤ 1/2, we show that ANOVA is essentially optimal under some conditions on the design. This is no longer the case under strong sparsity constraints, that is, α {\textgreater} 1/2. In such settings, a multiple comparison procedure is often preferred and we establish its optimality when α ≥ 3/4. However, these two very popular methods are suboptimal, and sometimes powerless, under moderately strong sparsity where 1/2 {\textless} α {\textless} 3/4. We suggest a method based on the higher criticism that is powerful in the whole range α {\textgreater} 1/2. This optimality property is true for a variety of designs, including the classical (balanced) multi-way designs and more modern “p {\textgreater} n” designs arising in genetics and signal processing. In addition to the standard fixed effects model, we establish similar results for a random effects model where the nonzero coefficients of the regression vector are normally distributed.},
	language = {EN},
	number = {5},
	urldate = {2017-04-06},
	journal = {The Annals of Statistics},
	author = {Arias-Castro, Ery and Candès, Emmanuel J. and Plan, Yaniv},
	month = oct,
	year = {2011},
	zmnumber = {1231.62136},
	keywords = {random matrices, analysis of variance, compressive sensing, Detecting a sparse signal, higher criticism, incoherence, minimax detection, suprema of Gaussian processes},
	pages = {2533--2556},
	file = {Snapshot:/home/jeremiah/Zotero/storage/UI3ZMBSG/1322663467.html:text/html}
}

@article{young_generalized_2011,
	title = {Generalized {Additive} {Models} and {Inflated} {Type} {I} {Error} {Rates} of {Smoother} {Significance} {Tests}},
	volume = {55},
	issn = {0167-9473},
	doi = {10.1016/j.csda.2010.05.004},
	abstract = {Generalized additive models (GAMs) have distinct advantages over generalized linear models as they allow investigators to make inferences about associations between outcomes and predictors without placing parametric restrictions on the associations. The variable of interest is often smoothed using a locally weighted regression (LOESS) and the optimal span (degree of smoothing) can be determined by minimizing the Akaike Information Criterion (AIC). A natural hypothesis when using GAMs is to test whether the smoothing term is necessary or if a simpler model would suffice. The statistic of interest is the difference in deviances between models including and excluding the smoothed term. As approximate chi-square tests of this hypothesis are known to be biased, permutation tests are a reasonable alternative. We compare the type I error rates of the chi-square test and of three permutation test methods using synthetic data generated under the null hypothesis. In each permutation method a distribution of differences in deviances is obtained from 999 permuted datasets and the null hypothesis is rejected if the observed statistic falls in the upper 5\% of the distribution. One test is a conditional permutation test using the optimal span size for the observed data; this span size is held constant for all permutations. This test is shown to have an inflated type I error rate. Alternatively, the span size can be fixed a priori such that the span selection technique is not reliant on the observed data. This test is shown to be unbiased; however, the choice of span size is not clear. A third method is an unconditional permutation test where the optimal span size is selected for observed and permuted datasets. This test is unbiased though computationally intensive.},
	language = {eng},
	number = {1},
	journal = {Computational Statistics \& Data Analysis},
	author = {Young, Robin L. and Weinberg, Janice and Vieira, Verónica and Ozonoff, Al and Webster, Thomas F.},
	month = jan,
	year = {2011},
	pmid = {20948974},
	pmcid = {PMC2952638},
	pages = {366--374}
}

@inproceedings{rakotomamonjy_frames_2005,
	title = {Frames, {Reproducing} {Kernels}, {Regularization} and {Learning}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/RakotomamonjyC05},
	urldate = {2017-04-05},
	author = {Rakotomamonjy, Alain and Canu, Stéphane},
	year = {2005},
	pages = {1485--1515},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/KIIUXDVX/Rakotomamonjy and Canu - 2005 - Frames, Reproducing Kernels, Regularization and Le.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/75FAGCGC/RakotomamonjyC05.html:text/html}
}

@article{canu_wavelet_nodate,
	title = {Wavelet {Kernels} and {RKHS}},
	url = {http://www.academia.edu/2884402/Wavelet_Kernels_and_RKHS},
	abstract = {Page 1. Wavelet Kernels and RKHS Alain Rakotomamonjy, Xavier Mary and Stéphane Canu alain.rakotomamonjy@insa-rouen.fr asi.insa-rouen.fr/˜arakotom . INSA Rouen -Département ASI Laboratoire PSI Wavelet Kernels and RKHS – p.1/17 Page 2.},
	urldate = {2017-04-05},
	journal = {Proc of Statistica l Lear ning: Theor y and Applica・tions. Par is},
	author = {Canu, Stéphane},
	file = {Snapshot:/home/jeremiah/Zotero/storage/524EMR5B/Wavelet_Kernels_and_RKHS.html:text/html}
}

@article{canu_functional_2009,
	title = {Functional learning through kernels},
	url = {http://arxiv.org/abs/0910.1013},
	abstract = {This paper reviews the functional aspects of statistical learning theory. The main point under consideration is the nature of the hypothesis set when no prior information is available but data. Within this framework we first discuss about the hypothesis set: it is a vectorial space, it is a set of pointwise defined functions, and the evaluation functional on this set is a continuous mapping. Based on these principles an original theory is developed generalizing the notion of reproduction kernel Hilbert space to non hilbertian sets. Then it is shown that the hypothesis set of any learning machine has to be a generalized reproducing set. Therefore, thanks to a general "representer theorem", the solution of the learning problem is still a linear combination of a kernel. Furthermore, a way to design these kernels is given. To illustrate this framework some examples of such reproducing sets and kernels are given.},
	urldate = {2017-04-05},
	journal = {arXiv:0910.1013 [stat]},
	author = {Canu, Stephane and Mary, Xavier and Rakotomamonjy, Alain},
	month = oct,
	year = {2009},
	note = {arXiv: 0910.1013},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:0910.1013 PDF:/home/jeremiah/Zotero/storage/VHQC9RMM/Canu et al. - 2009 - Functional learning through kernels.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/F4TURCZD/0910.html:text/html}
}

@article{durrande_detecting_2016,
	title = {Detecting periodicities with {Gaussian} processes},
	volume = {2},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-50},
	doi = {10.7717/peerj-cs.50},
	abstract = {We consider the problem of detecting and quantifying the periodic component of a function given noise-corrupted observations of a limited number of input/output tuples. Our approach is based on Gaussian process regression, which provides a flexible non-parametric framework for modelling periodic data. We introduce a novel decomposition of the covariance function as the sum of periodic and aperiodic kernels. This decomposition allows for the creation of sub-models which capture the periodic nature of the signal and its complement. To quantify the periodicity of the signal, we derive a periodicity ratio which reflects the uncertainty in the fitted sub-models. Although the method can be applied to many kernels, we give a special emphasis to the Matérn family, from the expression of the reproducing kernel Hilbert space inner product to the implementation of the associated periodic kernels in a Gaussian process toolkit. The proposed method is illustrated by considering the detection of periodically expressed genes in the arabidopsis genome.},
	language = {en},
	urldate = {2017-04-05},
	journal = {PeerJ Computer Science},
	author = {Durrande, Nicolas and Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
	month = apr,
	year = {2016},
	pages = {e50},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/6U6VMVPD/Durrande et al. - 2016 - Detecting periodicities with Gaussian processes.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/IGRQNDRH/cs-50.html:text/html}
}

@article{rakotomamonjy_non-parametric_2005,
	title = {Non-parametric regression with wavelet kernels},
	volume = {21},
	issn = {1526-4025},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/asmb.533/abstract},
	doi = {10.1002/asmb.533},
	abstract = {This paper introduces a method to construct a reproducing wavelet kernel Hilbert spaces for non-parametric regression estimation when the sampling points are not equally spaced. Another objective is to make high-dimensional wavelet estimation problems tractable. It then provides a theoretical foundation to build reproducing kernel from operators and a practical technique to obtain reproducing kernel Hilbert spaces spanned by a set of wavelets. A multiscale approximation technique that aims at taking advantage of the multiresolution structure of wavelets is also described. Examples on toy regression and a real-world problem illustrate the effectiveness of these wavelet kernels. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {2},
	urldate = {2017-04-05},
	journal = {Applied Stochastic Models in Business and Industry},
	author = {Rakotomamonjy, Alain and Mary, Xavier and Canu, Stéphane},
	month = mar,
	year = {2005},
	keywords = {wavelet, regression, regularization networks, reproducing kernel},
	pages = {153--163},
	file = {Snapshot:/home/jeremiah/Zotero/storage/3W4IV88D/abstract.html:text/html}
}

@article{taillefumier_discrete_2008,
	title = {A {Discrete} {Construction} for {Gaussian} {Markov} {Processes}},
	url = {http://arxiv.org/abs/0805.0048},
	abstract = {In the L{\textbackslash}'evy construction of Brownian motion, a Haar-derived basis of functions is used to form a finite-dimensional process \$W{\textasciicircum}\{N\}\$ and to define the Wiener process as the almost sure path-wise limit of \$W{\textasciicircum}\{N\}\$ when \$N\$ tends to infinity. We generalize such a construction to the class of centered Gaussian Markov processes \$X\$ which can be written \$X\_\{t\} = g(t) {\textbackslash}cdot {\textbackslash}int\_\{0\}{\textasciicircum}\{t\} f(t) dW\_\{t\}\$ with \$f\$ and \$g\$ being continuous functions. We build the finite-dimensional process \$X{\textasciicircum}\{N\}\$ so that it gives an exact representation of the conditional expectation of \$X\$ with respect to the filtration generated by \$\{{\textbackslash}lbrace X\_\{k/2{\textasciicircum}\{N\}\}{\textbackslash}rbrace\}\$ for \$0 {\textbackslash}leq k {\textbackslash}leq 2{\textasciicircum}\{N\}\$. Moreover, we prove that the process \$X{\textasciicircum}\{N\}\$ converges in distribution toward \$X\$.},
	urldate = {2017-04-05},
	journal = {arXiv:0805.0048 [math]},
	author = {Taillefumier, Thibaud},
	month = apr,
	year = {2008},
	note = {arXiv: 0805.0048},
	keywords = {Mathematics - Probability},
	file = {arXiv\:0805.0048 PDF:/home/jeremiah/Zotero/storage/7ZP2RQSQ/Taillefumier - 2008 - A Discrete Construction for Gaussian Markov Proces.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MRWVDXUB/0805.html:text/html}
}

@article{carvalho_high-dimensional_2008,
	title = {High-{Dimensional} {Sparse} {Factor} {Modeling}: {Applications} in {Gene} {Expression} {Genomics}},
	volume = {103},
	issn = {0162-1459},
	shorttitle = {High-{Dimensional} {Sparse} {Factor} {Modeling}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3017385/},
	doi = {10.1198/016214508000000869},
	abstract = {We describe studies in molecular profiling and biological pathway analysis that use sparse latent factor and regression models for microarray gene expression data. We discuss breast cancer applications and key aspects of the modeling and computational methodology. Our case studies aim to investigate and characterize heterogeneity of structure related to specific oncogenic pathways, as well as links between aggregate patterns in gene expression profiles and clinical biomarkers. Based on the metaphor of statistically derived “factors” as representing biological “subpathway” structure, we explore the decomposition of fitted sparse factor models into pathway subcomponents and investigate how these components overlay multiple aspects of known biological activity. Our methodology is based on sparsity modeling of multivariate regression, ANOVA, and latent factor models, as well as a class of models that combines all components. Hierarchical sparsity priors address questions of dimension reduction and multiple comparisons, as well as scalability of the methodology. The models include practically relevant non-Gaussian/nonparametric components for latent structure, underlying often quite complex non-Gaussianity in multivariate expression patterns. Model search and fitting are addressed through stochastic simulation and evolutionary stochastic search methods that are exemplified in the oncogenic pathway studies.  provides more details of the applications, as well as examples of the use of freely available software tools for implementing the methodology.},
	number = {484},
	urldate = {2017-03-31},
	journal = {Journal of the American Statistical Association},
	author = {Carvalho, Carlos M. and Chang, Jeffrey and Lucas, Joseph E. and Nevins, Joseph R. and Wang, Quanli and West, Mike},
	month = dec,
	year = {2008},
	pmid = {21218139},
	pmcid = {PMC3017385},
	pages = {1438--1456},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/6RWF8S4B/Carvalho et al. - 2008 - High-Dimensional Sparse Factor Modeling Applicati.pdf:application/pdf}
}

@article{owen_bi-cross-validation_2015,
	title = {Bi-cross-validation for factor analysis},
	url = {http://arxiv.org/abs/1503.03515},
	abstract = {Factor analysis is over a century old, but it is still problematic to choose the number of factors for a given data set. The scree test is popular but subjective. The best performing objective methods are recommended on the basis of simulations. We introduce a method based on bi-cross-validation, using randomly held-out submatrices of the data to choose the number of factors. We find it performs better than the leading methods of parallel analysis (PA) and Kaiser's rule. Our performance criterion is based on recovery of the underlying factor-loading (signal) matrix rather than identifying the true number of factors. Like previous comparisons, our work is simulation based. Recent advances in random matrix theory provide principled choices for the number of factors when the noise is homoscedastic, but not for the heteroscedastic case. The simulations we choose are designed using guidance from random matrix theory. In particular, we include factors too small to detect, factors large enough to detect but not large enough to improve the estimate, and two classes of factors large enough to be useful. Much of the advantage of bi-cross-validation comes from cases with factors large enough to detect but too small to be well estimated. We also find that a form of early stopping regularization improves the recovery of the signal matrix.},
	urldate = {2017-03-23},
	journal = {arXiv:1503.03515 [stat]},
	author = {Owen, A. B. and Wang, J.},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.03515},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1503.03515 PDF:/home/jeremiah/Zotero/storage/SGX6QKQS/Owen and Wang - 2015 - Bi-cross-validation for factor analysis.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/M6UD2ICX/1503.html:text/html}
}

@article{owhadi_separability_2017,
	title = {Separability of reproducing kernel spaces},
	volume = {145},
	issn = {0002-9939, 1088-6826},
	url = {http://www.ams.org/proc/2017-145-05/S0002-9939-2016-13354-6/},
	doi = {10.1090/proc/13354},
	abstract = {We demonstrate that a reproducing kernel Hilbert or Banach space of functions on a separable absolute Borel space or an analytic subset of a Polish space is separable if it possesses a Borel measurable feature map.},
	number = {5},
	urldate = {2017-03-22},
	journal = {Proceedings of the American Mathematical Society},
	author = {Owhadi, Houman and Scovel, Clint},
	year = {2017},
	pages = {2131--2138},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/F59PE68M/Owhadi and Scovel - 2017 - Separability of reproducing kernel spaces.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/PVEI8QD4/home.html:text/html}
}

@article{wood_p-values_2013,
	title = {On p-values for smooth components of an extended generalized additive model},
	volume = {100},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/100/1/221/192816/On-p-values-for-smooth-components-of-an-extended},
	doi = {10.1093/biomet/ass048},
	number = {1},
	urldate = {2017-03-21},
	journal = {Biometrika},
	author = {Wood, Simon N.},
	month = mar,
	year = {2013},
	pages = {221--228},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/6QHQXAST/Wood - 2013 - On p-values for smooth components of an extended g.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/MN8DJ56R/On-p-values-for-smooth-components-of-an-extended.html:text/html}
}

@article{wood_smoothing_2016,
	title = {Smoothing parameter and model selection for general smooth models},
	volume = {111},
	issn = {0162-1459, 1537-274X},
	url = {http://arxiv.org/abs/1511.03864},
	doi = {10.1080/01621459.2016.1180986},
	abstract = {This paper discusses a general framework for smoothing parameter estimation for models with regular likelihoods constructed in terms of unknown smooth functions of covariates. Gaussian random effects and parametric terms may also be present. By construction the method is numerically stable and convergent, and enables smoothing parameter uncertainty to be quantified. The latter enables us to fix a well known problem with AIC for such models. The smooth functions are represented by reduced rank spline like smoothers, with associated quadratic penalties measuring function smoothness. Model estimation is by penalized likelihood maximization, where the smoothing parameters controlling the extent of penalization are estimated by Laplace approximate marginal likelihood. The methods cover, for example, generalized additive models for non-exponential family responses (for example beta, ordered categorical, scaled t distribution, negative binomial and Tweedie distributions), generalized additive models for location scale and shape (for example two stage zero inflation models, and Gaussian location-scale models), Cox proportional hazards models and multivariate additive models. The framework reduces the implementation of new model classes to the coding of some standard derivatives of the log likelihood.},
	number = {516},
	urldate = {2017-03-21},
	journal = {Journal of the American Statistical Association},
	author = {Wood, Simon N. and Pya, Natalya and Säfken, Benjamin},
	month = oct,
	year = {2016},
	note = {arXiv: 1511.03864},
	keywords = {Statistics - Methodology},
	pages = {1548--1563},
	file = {arXiv\:1511.03864 PDF:/home/jeremiah/Zotero/storage/7SHU6JCU/Wood et al. - 2016 - Smoothing parameter and model selection for genera.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/R6TME3UK/1511.html:text/html}
}

@article{scheipl_size_2008,
	title = {Size and power of tests for a zero random effect variance or polynomial regression in additive and linear mixed models},
	volume = {52},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947307004306},
	doi = {10.1016/j.csda.2007.10.022},
	abstract = {Several tests for a zero random effect variance in linear mixed models are compared. This testing problem is non-regular because the tested parameter is on the boundary of the parameter space. Size and power of the different tests are investigated in an extensive simulation study that covers a variety of important settings. These include testing for polynomial regression versus a general smooth alternative using penalized splines. Among the test procedures considered, three are based on the restricted likelihood ratio test statistic (RLRT), while six are different extensions of the linear model F -test to the linear mixed model. Four of the tests with unknown null distributions are based on a parametric bootstrap, the other tests rely on approximate or asymptotic distributions. The parametric bootstrap-based tests all have a similar performance. Tests based on approximate F -distributions are usually the least powerful among the tests under consideration. The chi-square mixture approximation for the RLRT is confirmed to be conservative, with corresponding loss in power. A recently developed approximation to the distribution of the RLRT is identified as a rapid, powerful and reliable alternative to computationally intensive parametric bootstrap procedures. This novel method extends the exact distribution available for models with one random effect to models with several random effects.},
	number = {7},
	urldate = {2017-03-21},
	journal = {Computational Statistics \& Data Analysis},
	author = {Scheipl, Fabian and Greven, Sonja and Küchenhoff, Helmut},
	month = mar,
	year = {2008},
	pages = {3283--3299},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/DJU3ASAB/S0167947307004306.html:text/html}
}

@article{ranganath_operator_2016,
	title = {Operator {Variational} {Inference}},
	url = {http://arxiv.org/abs/1610.09033},
	abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
	urldate = {2017-03-16},
	journal = {arXiv:1610.09033 [cs, stat]},
	author = {Ranganath, Rajesh and Altosaar, Jaan and Tran, Dustin and Blei, David M.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.09033},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Appears in Neural Information Processing Systems, 2016},
	file = {arXiv\:1610.09033 PDF:/home/jeremiah/Zotero/storage/68TMJ2VE/Ranganath et al. - 2016 - Operator Variational Inference.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MP4CXDJD/1610.html:text/html}
}

@incollection{hofmann_learning_2000,
	title = {Learning the {Similarity} of {Documents}: {An} {Information}-{Geometric} {Approach} to {Document} {Retrieval} and {Categorization}},
	shorttitle = {Learning the {Similarity} of {Documents}},
	url = {http://papers.nips.cc/paper/1654-learning-the-similarity-of-documents-an-information-geometric-approach-to-document-retrieval-and-categorization.pdf},
	urldate = {2017-03-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 12},
	publisher = {MIT Press},
	author = {Hofmann, Thomas},
	editor = {Solla, S. A. and Leen, T. K. and Müller, K.},
	year = {2000},
	pages = {914--920},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/NCVA2E67/Hofmann - 2000 - Learning the Similarity of Documents An Informati.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/K8SX82WF/1654-learning-the-similarity-of-documents-an-information-geometric-approach-to-document-retriev.html:text/html}
}

@article{bhattacharya_simplex_2012,
	title = {Simplex {Factor} {Models} for {Multivariate} {Unordered} {Categorical} {Data}},
	volume = {107},
	issn = {0162-1459},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3728016/},
	doi = {10.1080/01621459.2011.646934},
	abstract = {Gaussian latent factor models are routinely used for modeling of dependence in continuous, binary, and ordered categorical data. For unordered categorical variables, Gaussian latent factor models lead to challenging computation and complex modeling structures. As an alternative, we propose a novel class of simplex factor models. In the single-factor case, the model treats the different categorical outcomes as independent with unknown marginals. The model can characterize flexible dependence structures parsimoniously with few factors, and as factors are added, any multivariate categorical data distribution can be accurately approximated. Using a Bayesian approach for computation and inferences, a Markov chain Monte Carlo (MCMC) algorithm is proposed that scales well with increasing dimension, with the number of factors treated as unknown. We develop an efficient proposal for updating the base probability vector in hierarchical Dirichlet models. Theoretical properties are described, and we evaluate the approach through simulation examples. Applications are described for modeling dependence in nucleotide sequences and prediction from high-dimensional categorical features.},
	number = {497},
	urldate = {2017-03-16},
	journal = {Journal of the American Statistical Association},
	author = {Bhattacharya, Anirban and Dunson, David B.},
	month = mar,
	year = {2012},
	pmid = {23908561},
	pmcid = {PMC3728016},
	pages = {362--377},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/M4T4R8JM/Bhattacharya and Dunson - 2012 - Simplex Factor Models for Multivariate Unordered C.pdf:application/pdf}
}

@article{bhattacharya_simplex_2012-1,
	title = {Simplex {Factor} {Models} for {Multivariate} {Unordered} {Categorical} {Data}},
	volume = {107},
	issn = {0162-1459},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3728016/},
	doi = {10.1080/01621459.2011.646934},
	abstract = {Gaussian latent factor models are routinely used for modeling of dependence in continuous, binary, and ordered categorical data. For unordered categorical variables, Gaussian latent factor models lead to challenging computation and complex modeling structures. As an alternative, we propose a novel class of simplex factor models. In the single-factor case, the model treats the different categorical outcomes as independent with unknown marginals. The model can characterize flexible dependence structures parsimoniously with few factors, and as factors are added, any multivariate categorical data distribution can be accurately approximated. Using a Bayesian approach for computation and inferences, a Markov chain Monte Carlo (MCMC) algorithm is proposed that scales well with increasing dimension, with the number of factors treated as unknown. We develop an efficient proposal for updating the base probability vector in hierarchical Dirichlet models. Theoretical properties are described, and we evaluate the approach through simulation examples. Applications are described for modeling dependence in nucleotide sequences and prediction from high-dimensional categorical features.},
	number = {497},
	urldate = {2017-03-16},
	journal = {Journal of the American Statistical Association},
	author = {Bhattacharya, Anirban and Dunson, David B.},
	month = mar,
	year = {2012},
	pmid = {23908561},
	pmcid = {PMC3728016},
	pages = {362--377},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/VHZZRMGN/Bhattacharya and Dunson - 2012 - Simplex Factor Models for Multivariate Unordered C.pdf:application/pdf}
}

@article{fu_robust_2016,
	title = {Robust {Volume} {Minimization}-{Based} {Matrix} {Factorization} for {Remote} {Sensing} and {Document} {Clustering}},
	volume = {64},
	issn = {1053-587X, 1941-0476},
	url = {http://arxiv.org/abs/1608.04290},
	doi = {10.1109/TSP.2016.2602800},
	abstract = {This paper considers {\textbackslash}emph\{volume minimization\} (VolMin)-based structured matrix factorization (SMF). VolMin is a factorization criterion that decomposes a given data matrix into a basis matrix times a structured coefficient matrix via finding the minimum-volume simplex that encloses all the columns of the data matrix. Recent work showed that VolMin guarantees the identifiability of the factor matrices under mild conditions that are realistic in a wide variety of applications. This paper focuses on both theoretical and practical aspects of VolMin. On the theory side, exact equivalence of two independently developed sufficient conditions for VolMin identifiability is proven here, thereby providing a more comprehensive understanding of this aspect of VolMin. On the algorithm side, computational complexity and sensitivity to outliers are two key challenges associated with real-world applications of VolMin. These are addressed here via a new VolMin algorithm that handles volume regularization in a computationally simple way, and automatically detects and \{iteratively downweights\} outliers, simultaneously. Simulations and real-data experiments using a remotely sensed hyperspectral image and the Reuters document corpus are employed to showcase the effectiveness of the proposed algorithm.},
	number = {23},
	urldate = {2017-03-16},
	journal = {IEEE Transactions on Signal Processing},
	author = {Fu, Xiao and Huang, Kejun and Yang, Bo and Ma, Wing-Kin and Sidiropoulos, Nicholas D.},
	month = dec,
	year = {2016},
	note = {arXiv: 1608.04290},
	keywords = {Statistics - Machine Learning},
	pages = {6254--6268},
	file = {arXiv\:1608.04290 PDF:/home/jeremiah/Zotero/storage/WVJ6E8FU/Fu et al. - 2016 - Robust Volume Minimization-Based Matrix Factorizat.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/294VC6MW/1608.html:text/html}
}

@article{conti_bayesian_2014,
	series = {Internally {Consistent} {Modeling}, {Aggregation}, {Inference} and {Policy}},
	title = {Bayesian exploratory factor analysis},
	volume = {183},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407614001493},
	doi = {10.1016/j.jeconom.2014.06.008},
	abstract = {This paper develops and applies a Bayesian approach to Exploratory Factor Analysis that improves on ad hoc classical approaches. Our framework relies on dedicated factor models and simultaneously determines the number of factors, the allocation of each measurement to a unique factor, and the corresponding factor loadings. Classical identification criteria are applied and integrated into our Bayesian procedure to generate models that are stable and clearly interpretable. A Monte Carlo study confirms the validity of the approach. The method is used to produce interpretable low dimensional aggregates from a high dimensional set of psychological measurements.},
	number = {1},
	urldate = {2017-03-16},
	journal = {Journal of Econometrics},
	author = {Conti, Gabriella and Frühwirth-Schnatter, Sylvia and Heckman, James J. and Piatek, Rémi},
	month = nov,
	year = {2014},
	keywords = {Bayesian factor models, Exploratory factor analysis, Identifiability, Marginal data augmentation, Model expansion, Model selection},
	pages = {31--57},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/XT2KGS7Z/Conti et al. - 2014 - Bayesian exploratory factor analysis.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/CB9Q4QJK/S0304407614001493.html:text/html}
}

@inproceedings{bernstein_manifold_2015-1,
	title = {Manifold {Learning} in {Regression} {Tasks}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-17091-6_36},
	doi = {10.1007/978-3-319-17091-6_36},
	abstract = {The paper presents a new geometrically motivated method for non-linear regression based on Manifold learning technique. The regression problem is to construct a predictive function which estimates an unknown smooth mapping f from q-dimensional inputs to m-dimensional outputs based on a training data set consisting of given ‘input-output’ pairs. The unknown mapping f determines q-dimensional manifold M(f) consisting of all the ‘input-output’ vectors which is embedded in (q+m)-dimensional space and covered by a single chart; the training data set determines a sample from this manifold. Modern Manifold Learning methods allow constructing the certain estimator M* from the manifold-valued sample which accurately approximates the manifold. The proposed method called Manifold Learning Regression (MLR) finds the predictive function fMLR to ensure an equality M(fMLR) = M*. The MLR simultaneously estimates the m×q Jacobian matrix of the mapping f.},
	language = {en},
	urldate = {2017-03-15},
	booktitle = {Statistical {Learning} and {Data} {Sciences}},
	publisher = {Springer, Cham},
	author = {Bernstein, Alexander and Kuleshov, Alexander and Yanovich, Yury},
	month = apr,
	year = {2015},
	pages = {414--423},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/2TBSW6VK/Bernstein et al. - 2015 - Manifold Learning in Regression Tasks.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/X8D9V84S/978-3-319-17091-6_36.html:text/html}
}

@inproceedings{yu_improved_2010,
	title = {Improved {Local} {Coordinate} {Coding} using {Local} {Tangents}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2010_YuZ10},
	urldate = {2017-03-15},
	author = {Yu, Kai and Zhang, Tong},
	year = {2010},
	pages = {1215--1222},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/8D8EKUVH/Yu and Zhang - 2010 - Improved Local Coordinate Coding using Local Tange.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/378FNT9M/icml2010_YuZ10.html:text/html}
}

@article{fishbaugh_geodesic_2013,
	title = {Geodesic image regression with a sparse parameterization of diffeomorphisms},
	volume = {8085},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4316381/},
	doi = {10.1007/978-3-642-40020-9_9},
	abstract = {Image regression allows for time-discrete imaging data to be modeled continuously, and is a crucial tool for conducting statistical analysis on longitudinal images. Geodesic models are particularly well suited for statistical analysis, as image evolution is fully characterized by a baseline image and initial momenta. However, existing geodesic image regression models are parameterized by a large number of initial momenta, equal to the number of image voxels. In this paper, we present a sparse geodesic image regression framework which greatly reduces the number of model parameters. We combine a control point formulation of deformations with a L1 penalty to select the most relevant subset of momenta. This way, the number of model parameters reflects the complexity of anatomical changes in time rather than the sampling of the image. We apply our method to both synthetic and real data and show that we can decrease the number of model parameters (from the number of voxels down to hundreds) with only minimal decrease in model accuracy. The reduction in model parameters has the potential to improve the power of ensuing statistical analysis, which faces the challenging problem of high dimensionality.},
	urldate = {2017-03-15},
	journal = {Geometric science of information : first international conference, GSI 2013, Paris, France, August 28-30, 2013 : proceedings / Frank Nielsen, Frederic Barbaresco (eds.). Geometric Science of Information (1st : 2013 : Paris, France)},
	author = {Fishbaugh, James and Prastawa, Marcel and Gerig, Guido and Durrleman, Stanley},
	year = {2013},
	pmid = {25664349},
	pmcid = {PMC4316381},
	pages = {95--102},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/BZCB37N5/Fishbaugh et al. - 2013 - Geodesic image regression with a sparse parameteri.pdf:application/pdf}
}

@inproceedings{zhang_bayesian_2013,
	title = {Bayesian {Estimation} of {Regularization} and {Atlas} {Building} in {Diffeomorphic} {Image} {Registration}},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-38868-2_4},
	doi = {10.1007/978-3-642-38868-2_4},
	abstract = {This paper presents a generative Bayesian model for diffeomorphic image registration and atlas building. We develop an atlas estimation procedure that simultaneously estimates the parameters controlling the smoothness of the diffeomorphic transformations. To achieve this, we introduce a Monte Carlo Expectation Maximization algorithm, where the expectation step is approximated via Hamiltonian Monte Carlo sampling on the manifold of diffeomorphisms. An added benefit of this stochastic approach is that it can successfully solve difficult registration problems involving large deformations, where direct geodesic optimization fails. Using synthetic data generated from the forward model with known parameters, we demonstrate the ability of our model to successfully recover the atlas and regularization parameters. We also demonstrate the effectiveness of the proposed method in the atlas estimation problem for 3D brain images.},
	language = {en},
	urldate = {2017-03-15},
	booktitle = {Information {Processing} in {Medical} {Imaging}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Zhang, Miaomiao and Singh, Nikhil and Fletcher, P. Thomas},
	month = jun,
	year = {2013},
	pages = {37--48},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/USFNMCKF/Zhang et al. - 2013 - Bayesian Estimation of Regularization and Atlas Bu.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/JN5WNIJA/978-3-642-38868-2_4.html:text/html}
}

@article{anderes_general_2012,
	title = {A general spline representation for nonparametric and semiparametric density estimates using diffeomorphisms},
	url = {http://arxiv.org/abs/1205.5314},
	abstract = {A theorem of McCann shows that for any two absolutely continuous probability measures on R{\textasciicircum}d there exists a monotone transformation sending one probability measure to the other. A consequence of this theorem, relevant to statistics, is that density estimation can be recast in terms of transformations. In particular, one can fix any absolutely continuous probability measure, call it P, and then reparameterize the whole class of absolutely continuous probability measures as monotone transformations from P. In this paper we utilize this reparameterization of densities, as monotone transformations from some P, to construct semiparametric and nonparametric density estimates. We focus our attention on classes of transformations, developed in the image processing and computational anatomy literature, which are smooth, invertible and which have attractive computational properties. The techniques developed for this class of transformations allow us to show that a penalized maximum likelihood estimate (PMLE) of a smooth transformation from P exists and has a finite dimensional characterization, similar to those results found in the spline literature. These results are derived utilizing an Euler-Lagrange characterization of the PMLE which also establishes a surprising connection to a generalization of Stein's lemma for characterizing the normal distribution.},
	urldate = {2017-03-15},
	journal = {arXiv:1205.5314 [stat]},
	author = {Anderes, Ethan and Coram, Marc},
	month = may,
	year = {2012},
	note = {arXiv: 1205.5314},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1205.5314 PDF:/home/jeremiah/Zotero/storage/I6CM9ZHT/Anderes and Coram - 2012 - A general spline representation for nonparametric .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/Z8UVFUHQ/1205.html:text/html}
}

@article{bengio_representation_2013,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	issn = {0162-8828},
	shorttitle = {Representation {Learning}},
	url = {http://dx.doi.org/10.1109/TPAMI.2013.50},
	doi = {10.1109/TPAMI.2013.50},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	number = {8},
	urldate = {2017-03-15},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = aug,
	year = {2013},
	keywords = {unsupervised learning, Machine learning, Abstracts, autoencoder, Boltzmann machine, Deep learning, Feature extraction, feature learning, Learning systems, Manifolds, neural nets, Neural networks, representation learning, Speech recognition},
	pages = {1798--1828}
}

@article{fan_testing_1994,
	title = {Testing the {Goodness} of {Fit} of a {Parametric} {Density} {Function} by {Kernel} {Method}},
	volume = {10},
	issn = {0266-4666},
	url = {http://www.jstor.org/stable/3532871},
	abstract = {Let F denote a distribution function defined on the probability space (Ω, F, P), which is absolutely continuous with respect to the Lebesgue measure in R$^{\textrm{d}}$ with probability density function f. Let f$_{\textrm{0}}$(·,β) be a parametric density function that depends on an unknown p × 1 vector β. In this paper, we consider tests of the goodness-of-fit of f$_{\textrm{0}}$(·,β) for f(·) for some β based on (i) the integrated squared difference between a kernel estimate of f(·) and the quasi-maximum likelihood estimate of f$_{\textrm{0}}$(·,β) denoted by I$_{\textrm{n}}$ and (ii) the integrated squared difference between a kernel estimate of f(·) and the corresponding kernel smoothed estimate of f$_{\textrm{0}}$(·,β) denoted by J$_{\textrm{n}}$. It is shown in this paper that the amount of smoothing applied to the data in constructing the kernel estimate of f(·) determines the form of the test statistic based on I$_{\textrm{n}}$. For each test developed, we also examine its asymptotic properties including consistency and the local power property. In particular, we show that tests developed in this paper, except the first one, are more powerful than the Kolmogorov-Smirnov test under the sequence of local alternatives introduced in Rosenblatt [12], although they are less powerful than the Kolmogorov-Smirnov test under the sequence of Pitman alternatives. A small simulation study is carried out to examine the finite sample performance of one of these tests.},
	number = {2},
	urldate = {2017-03-10},
	journal = {Econometric Theory},
	author = {Fan, Yanqin},
	year = {1994},
	pages = {316--356}
}

@article{smirnov_table_1948,
	title = {Table for {Estimating} the {Goodness} of {Fit} of {Empirical} {Distributions}},
	volume = {19},
	issn = {0003-4851, 2168-8990},
	url = {http://projecteuclid.org/euclid.aoms/1177730256},
	doi = {10.1214/aoms/1177730256},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	number = {2},
	urldate = {2017-03-10},
	journal = {The Annals of Mathematical Statistics},
	author = {Smirnov, N.},
	month = jun,
	year = {1948},
	mrnumber = {MR25109},
	zmnumber = {0031.37001},
	pages = {279--281},
	file = {Snapshot:/home/jeremiah/Zotero/storage/NIXNSXKB/1177730256.html:text/html}
}

@book{daniel_applied_1978,
	title = {Applied nonparametric statistics},
	abstract = {Introduction and review; Procedures that utilize data from a single sample; Procedures that utilize data from two independent samples; Procedures that utilize data from two related samples; Chi-square tests of independence and homogeneity; Procedures that utilize data from three or more independent samples; Procedures that utilize data from three or more related; Coodness-of-fit tests; Rank correlation and other measures of association; Simple linear regression analysis.},
	language = {en},
	publisher = {Houghton Mifflin},
	author = {Daniel, Wayne W.},
	year = {1978},
	note = {Google-Books-ID: yYEpAQAAMAAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Nonparametric statistics}
}

@article{baringhaus_consistent_1988,
	title = {A consistent test for multivariate normality based on the empirical characteristic function},
	volume = {35},
	issn = {0026-1335, 1435-926X},
	url = {https://link.springer.com/article/10.1007/BF02613322},
	doi = {10.1007/BF02613322},
	abstract = {LetX1,X2, …,Xn be independent identically distributed random vectors in IRd,d ⩾ 1, with sample meanX¯nX¯n{\textbackslash}bar X\_n and sample covariance matrixSn. We present a practicable and consistent test for the composite hypothesisHd: the law ofX1 is a non-degenerate normal distribution, based on a weighted integral of the squared modulus of the difference between the empirical characteristic function of the residualsSn−1/2(Xj −X¯nX¯n{\textbackslash}bar X\_n ) and its pointwise limit exp (−1/2{\textbar}t{\textbar}2) underHd. The limiting null distribution of the test statistic is obtained, and a table with critical values for various choices ofn andd based on extensive simulations is supplied.},
	language = {en},
	number = {1},
	urldate = {2017-03-10},
	journal = {Metrika},
	author = {Baringhaus, L. and Henze, N.},
	month = dec,
	year = {1988},
	pages = {339--348},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/GVC2CGK3/Baringhaus and Henze - 1988 - A consistent test for multivariate normality based.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/XZHC22EA/BF02613322.html:text/html}
}

@article{gyorfi_asymptotic_2002,
	title = {Asymptotic distributions for goodness-of-fit statistics in a sequence of multinomial models},
	volume = {56},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/S0167715201001729},
	doi = {10.1016/S0167-7152(01)00172-9},
	abstract = {We consider f-disparities Df(p̂n,pn) between discrete distributions pn=(pn1,…,pnkn) and their estimates p̂n=(p̂n1,…,p̂nkn) based on relative frequencies in an i.i.d. sample of size n, where f : (0,∞)→R is twice continuously differentiable in a neighborhood of 1 with f″(1)≠0. We derive asymptotic distributions of the disparity statistics n Df(p̂n,pn) under certain assumptions about pn and the second derivatives f″ in a neighborhood of 1. These assumptions are weaker than those known from the literature.},
	number = {1},
	urldate = {2017-03-10},
	journal = {Statistics \& Probability Letters},
	author = {Györfi, L. and Vajda, I.},
	month = jan,
	year = {2002},
	keywords = {Asymptotic distributions, Disparity statistics, Goodness-of-fit statistics, Goodness-of-fit tests},
	pages = {57--67},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/GWT2SNHR/S0167715201001729.html:text/html}
}

@article{barron_uniformly_1989,
	title = {Uniformly {Powerful} {Goodness} of {Fit} {Tests}},
	volume = {17},
	issn = {0090-5364, 2168-8966},
	url = {http://projecteuclid.org/euclid.aos/1176347005},
	doi = {10.1214/aos/1176347005},
	abstract = {The simple hypothesis is tested that the distribution of independent random variables X1,X2,⋯,XnX1,X2,⋯,XnX\_1, X\_2, {\textbackslash}cdots, X\_n is a given probability measure P0P0P\_0. Let πnπn{\textbackslash}pi\_n be any sequence of partitions. The alternative hypothesis is the set of probability measures PPP with ∑A∈πn{\textbar}P(A)−P0(A){\textbar}≥δ∑A∈πn{\textbar}P(A)−P0(A){\textbar}≥δ{\textbackslash}sum\_\{A{\textbackslash}in {\textbackslash}pi\_n\}{\textbar}P(A) - P\_0(A){\textbar} {\textbackslash}geq {\textbackslash}delta, where δ{\textgreater}0δ{\textgreater}0{\textbackslash}delta {\textgreater} 0. Note the dependence of this set of alternatives on the sample size. It is shown that if the effective cardinality of the partitions is of the same order as the sample size, then sequences of tests exist with uniformly exponentially small probabilities of error. Conversely, if the effective cardinality is of larger order than the sample size, then no such sequence of tests exists. The effective cardinality is the number of sets in the partition which exhaust all but a negligible portion of the probability under the null hypothesis.},
	language = {EN},
	number = {1},
	urldate = {2017-03-10},
	journal = {The Annals of Statistics},
	author = {Barron, Andrew R.},
	month = mar,
	year = {1989},
	mrnumber = {MR981439},
	zmnumber = {0674.62032},
	keywords = {exponential bounds, goodness of fit, Kullback-Leibler divergence, large deviation inequalities, Uniformly consistent tests, variation distance},
	pages = {107--124},
	file = {Snapshot:/home/jeremiah/Zotero/storage/4PGNRHFK/1176347005.html:text/html}
}

@techreport{gelman_adaptively_2007,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Adaptively {Scaling} the {Metropolis} {Algorithm} {Using} {Expected} {Squared} {Jumped} {Distance}},
	url = {https://papers.ssrn.com/abstract=1010403},
	abstract = {A good choice of the proposal distribution is crucial for the rapid convergence of the Metropolis algorithm. In this paper, given a family of parametric Markovi},
	number = {ID 1010403},
	urldate = {2017-03-09},
	institution = {Social Science Research Network},
	author = {Gelman, Andrew and Pasarica, Cristian},
	year = {2007},
	keywords = {Markov chain Monte Carlo, Metropolis algorithm, Bayesian computation, acceptance rates, autocorrelation, iterative simulation, multiple importance sampling},
	file = {Snapshot:/home/jeremiah/Zotero/storage/99N388B8/papers.html:text/html}
}

@techreport{woodard_detecting_2007,
	title = {Detecting poor convergence of posterior samplers due to multimodality},
	abstract = {Computation in Bayesian statistical models is often performed using iterative sampling techniques such as Markov chain Monte Carlo (MCMC), in which case the convergence of the sampler to the posterior distribution is assessed using a set of standard diagnostics. We show that this approach may be insufficient when the posterior distribution is multimodal–that lack of convergence due to posterior multimodality can be undetected using the standard convergence diagnostics, including the Gelman-Rubin diagnostic that was introduced for exactly this problem. We show that the poor convergence can be detected using a validation technique that was originally proposed for detecting coding errors in MCMC software (Cook, Gelman and Rubin 2006). The validation method can succeed where convergence diagnostics fail, because it evaluates the convergence of the sampling algorithm for many data sets drawn from the model, rather than for the particular data set under consideration. We show that for several models (including some instances of the popular stochastic search variable selection technique by George and McCulloch 1993) one mode of the posterior distribution has a much smaller “basin of attraction ” than another mode. The narrower mode is extremely difficult to detect, both for a Gibbs sampler and for the Gelman-Rubin diagnostic applied to that sampler. Failure to diagnose that there is an undetected narrow mode then leads to overestimation of the posterior variance.},
	author = {Woodard, Dawn B.},
	year = {2007},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/3SHKJZKI/Woodard - 2007 - Detecting poor convergence of posterior samplers d.pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/2BF7VJK7/summary.html:text/html}
}

@article{kass_markov_1998,
	title = {Markov {Chain} {Monte} {Carlo} in {Practice}: {A} {Roundtable} {Discussion}},
	volume = {52},
	issn = {0003-1305},
	shorttitle = {Markov {Chain} {Monte} {Carlo} in {Practice}},
	url = {http://www.jstor.org/stable/2685466},
	doi = {10.2307/2685466},
	abstract = {Markov chain Monte Carlo (MCMC) methods make possible the use of flexible Bayesian models that would otherwise be computationally infeasible. In recent years, a great variety of such applications have been described in the literature. Applied statisticians who are new to these methods may have several questions and concerns, however: How much effort and expertise are needed to design and use a Markov chain sampler? How much confidence can one have in the answers that MCMC produces? How does the use of MCMC affect the rest of the model-building process? At the Joint Statistical Meetings in August, 1996, a panel of experienced MCMC users discussed these and other issues, as well as various "tricks of the trade". This article is an edited recreation of that discussion. Its purpose is to offer advice and guidance to novice users of MCMC-and to not-so-novice users as well. Topics include building confidence in simulation results, methods for speeding and assessing convergence, estimating standard errors, identification of models for which good MCMC algorithms exist, and the current state of software development.},
	number = {2},
	urldate = {2017-03-09},
	journal = {The American Statistician},
	author = {Kass, Robert E. and Carlin, Bradley P. and Gelman, Andrew and Neal, Radford M.},
	year = {1998},
	pages = {93--100}
}

@article{gelman_inference_1992,
	title = {Inference from {Iterative} {Simulation} {Using} {Multiple} {Sequences}},
	volume = {7},
	issn = {0883-4237, 2168-8745},
	url = {http://projecteuclid.org/euclid.ss/1177011136},
	doi = {10.1214/ss/1177011136},
	abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
	language = {EN},
	number = {4},
	urldate = {2017-03-09},
	journal = {Statistical Science},
	author = {Gelman, Andrew and Rubin, Donald B.},
	month = nov,
	year = {1992},
	keywords = {Bayesian inference, EM, Gibbs sampler, Metropolis algorithm, convergence of stochastic processes, ECM, importance sampling, multiple imputation, random-effects model, SIR},
	pages = {457--472},
	file = {Snapshot:/home/jeremiah/Zotero/storage/KHGUFRKG/1177011136.html:text/html}
}

@article{gelman_inference_1992-1,
	title = {Inference from {Iterative} {Simulation} {Using} {Multiple} {Sequences}},
	volume = {7},
	issn = {0883-4237, 2168-8745},
	url = {http://projecteuclid.org/euclid.ss/1177011136},
	doi = {10.1214/ss/1177011136},
	abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
	language = {EN},
	number = {4},
	urldate = {2017-03-09},
	journal = {Statistical Science},
	author = {Gelman, Andrew and Rubin, Donald B.},
	month = nov,
	year = {1992},
	keywords = {Bayesian inference, EM, Gibbs sampler, Metropolis algorithm, convergence of stochastic processes, ECM, importance sampling, multiple imputation, random-effects model, SIR},
	pages = {457--472},
	file = {Snapshot:/home/jeremiah/Zotero/storage/APDCV7FG/1177011136.html:text/html}
}

@inproceedings{geweke_evaluating_1992,
	title = {Evaluating the {Accuracy} of {Sampling}-{Based} {Approaches} to the {Calculation} of {Posterior} {Moments}},
	abstract = {Data augmentation and Gibbs sampling are two closely related, sampling-based approaches to the calculation of posterior moments. The fact that each produces a sample whose constituents are neither independent nor identically distributed complicates the assessment of convergence and numerical accuracy of the approximations to the expected value of functions of interest under the posterior. In this paper methods from spectral analysis are used to evaluate numerical accuracy formally and construct diagnostics for convergence. These methods are illustrated in the normal linear model with informative priors, and in the Tobit-censored regression model.},
	booktitle = {In {Bayesian} {Statistics}},
	publisher = {University Press},
	author = {Geweke, John},
	year = {1992},
	pages = {169--193},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/UH5ZGS4A/Geweke - 1992 - Evaluating the Accuracy of Sampling-Based Approach.pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/P38NB78W/summary.html:text/html}
}

@unpublished{chauveau_selection_2006-1,
	title = {Selection of a {MCMC} simulation strategy via an entropy convergence criterion},
	url = {https://hal.archives-ouvertes.fr/hal-00019174},
	abstract = {In MCMC methods, such as the Metropolis-Hastings (MH) algorithm, the Gibbs sampler, or recent adaptive methods, many different strategies can be proposed, often associated in practice to unknown rates of convergence. In this paper we propose a simulation-based methodology to compare these rates of convergence, grounded on an entropy criterion computed from parallel (i.i.d.) simulated Markov chains coming from each candidate strategy. Our criterion determines on the very first iterations the best strategy among the candidates. Theoretically, we give for the MH algorithm general conditions under which its successive densities satisfy adequate smoothness and tail properties, so that this entropy criterion can be estimated consistently using kernel density estimate and Monte Carlo integration. Simulated examples are provided to illustrate this convergence criterion.},
	urldate = {2017-03-09},
	author = {Chauveau, Didier and Vandekerkhove, Pierre},
	year = {2006},
	keywords = {Markov chain Monte Carlo, Entropy, Kullback divergence, Metropolis-Hastings algorithm, nonparametric statistic, proposal distribution},
	annote = {working paper or preprint},
	file = {HAL PDF Full Text:/home/jeremiah/Zotero/storage/53V78NQV/Chauveau and Vandekerkhove - 2006 - Selection of a MCMC simulation strategy via an ent.pdf:application/pdf}
}

@article{geweke_getting_2004-1,
	title = {Getting {It} {Right}: {Joint} {Distribution} {Tests} of {Posterior} {Simulators}},
	volume = {99},
	issn = {0162-1459},
	shorttitle = {Getting {It} {Right}},
	url = {http://www.jstor.org/stable/27590449},
	abstract = {Analytical or coding errors in posterior simulators can produce reasonable but incorrect approximations of posterior moments. This article develops simple tests of posterior simulators that detect both kinds of errors, and uses them to detect and correct errors in two previously published articles. The tests exploit the fact that a Bayesian model specifies the joint distribution of observables (data) and unobservables (parameters). There are two joint distribution simulators. The marginal-conditional simulator draws unobservables from the prior and then observables conditional on unobservables. The successive-conditional simulator alternates between the posterior simulator and an observables simulator. Formal comparison of moment approximations of the two simulators reveals existing analytical or coding errors in the posterior simulator.},
	number = {467},
	urldate = {2017-03-09},
	journal = {Journal of the American Statistical Association},
	author = {Geweke, John},
	year = {2004},
	pages = {799--804}
}

@article{cook_validation_2006,
	title = {Validation of {Software} for {Bayesian} {Models} {Using} {Posterior} {Quantiles}},
	volume = {15},
	issn = {1061-8600},
	url = {http://www.jstor.org/stable/27594203},
	abstract = {This article presents a simulation-based method designed to establish the computational correctness of software developed to fit a specific Bayesian model, capitalizing on properties of Bayesian posterior distributions. We illustrate the validation technique with two examples. The validation method is shown to find errors in software when they exist and, moreover, the validation output can be informative about the nature and location of such errors. We also compare our method with that of an earlier approach.},
	number = {3},
	urldate = {2017-03-09},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Cook, Samantha R. and Gelman, Andrew and Rubin, Donald B.},
	year = {2006},
	pages = {675--692}
}

@article{brooks_general_1998,
	title = {General {Methods} for {Monitoring} {Convergence} of {Iterative} {Simulations}},
	volume = {7},
	issn = {1061-8600},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.1998.10474787},
	doi = {10.1080/10618600.1998.10474787},
	abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
	number = {4},
	urldate = {2017-03-09},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Brooks, Stephen P. and Gelman, Andrew},
	month = dec,
	year = {1998},
	keywords = {Markov chain Monte Carlo, Convergence diagnosis, Inference},
	pages = {434--455},
	file = {Snapshot:/home/jeremiah/Zotero/storage/WZCD6S83/10618600.1998.html:text/html}
}

@incollection{shirley_inference_2011,
	series = {Chapman \& {Hall}/{CRC} {Handbooks} of {Modern} {Statistical} {Methods}},
	title = {Inference from {Simulations} and {Monitoring} {Convergence}},
	isbn = {978-1-4200-7941-8},
	url = {http://www.crcnetbase.com/doi/abs/10.1201/b10905-7},
	urldate = {2017-03-09},
	booktitle = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	publisher = {Chapman and Hall/CRC},
	author = {Shirley, Kenneth},
	month = may,
	year = {2011},
	doi = {10.1201/b10905-7},
	doi = {10.1201/b10905-7},
	file = {Snapshot:/home/jeremiah/Zotero/storage/BZB8ZV3T/b10905-7.html:text/html}
}

@article{cowles_markov_1996,
	title = {Markov {Chain} {Monte} {Carlo} {Convergence} {Diagnostics}: {A} {Comparative} {Review}},
	volume = {91},
	issn = {0162-1459},
	shorttitle = {Markov {Chain} {Monte} {Carlo} {Convergence} {Diagnostics}},
	url = {http://www.jstor.org/stable/2291683},
	doi = {10.2307/2291683},
	abstract = {A critical issue for users of Markov chain Monte Carlo (MCMC) methods in applications is how to determine when it is safe to stop sampling and use the samples to estimate characteristics of the distribution of interest. Research into methods of computing theoretical convergence bounds holds promise for the future but to date has yielded relatively little of practical use in applied work. Consequently, most MCMC users address the convergence problem by applying diagnostic tools to the output produced by running their samplers. After giving a brief overview of the area, we provide an expository review of 13 convergence diagnostics, describing the theoretical basis and practical implementation of each. We then compare their performance in two simple models and conclude that all of the methods can fail to detect the sorts of convergence failure that they were designed to identify. We thus recommend a combination of strategies aimed at evaluating and accelerating MCMC sampler convergence, including applying diagnostic procedures to a small number of parallel chains, monitoring autocorrelations and cross-correlations, and modifying parametrizations or sampling algorithms appropriately. We emphasize, however, that it is not possible to say with certainty that a finite sample from an MCMC algorithm is representative of an underlying stationary distribution.},
	number = {434},
	urldate = {2017-03-09},
	journal = {Journal of the American Statistical Association},
	author = {Cowles, Mary Kathryn and Carlin, Bradley P.},
	year = {1996},
	pages = {883--904}
}

@inproceedings{knowles_infinite_2007,
	title = {Infinite {Sparse} {Factor} {Analysis} and {Infinite} {Independent} {Components} {Analysis}},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-74494-8_48},
	doi = {10.1007/978-3-540-74494-8_48},
	abstract = {A nonparametric Bayesian extension of Independent Components Analysis (ICA) is proposed where observed data Y is modelled as a linear superposition, G, of a potentially infinite number of hidden sources, X. Whether a given source is active for a specific data point is specified by an infinite binary matrix, Z. The resulting sparse representation allows increased data reduction compared to standard ICA. We define a prior on Z using the Indian Buffet Process (IBP). We describe four variants of the model, with Gaussian or Laplacian priors on X and the one or two-parameter IBPs. We demonstrate Bayesian inference under these models using a Markov Chain Monte Carlo (MCMC) algorithm on synthetic and gene expression data and compare to standard ICA algorithms.},
	language = {en},
	urldate = {2017-03-08},
	booktitle = {Independent {Component} {Analysis} and {Signal} {Separation}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Knowles, David and Ghahramani, Zoubin},
	month = sep,
	year = {2007},
	pages = {381--388},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/38GHPXK5/Knowles and Ghahramani - 2007 - Infinite Sparse Factor Analysis and Infinite Indep.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/UF4KUWF8/978-3-540-74494-8_48.html:text/html}
}

@article{bhattacharya_sparse_2011-1,
	title = {Sparse {Bayesian} infinite factor models},
	volume = {98},
	issn = {0006-3444},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3419391/},
	doi = {10.1093/biomet/asr013},
	abstract = {We focus on sparse modelling of high-dimensional covariance matrices using Bayesian latent factor models. We propose a multiplicative gamma process shrinkage prior on the factor loadings which allows introduction of infinitely many factors, with the loadings increasingly shrunk towards zero as the column index increases. We use our prior on a parameter-expanded loading matrix to avoid the order dependence typical in factor analysis models and develop an efficient Gibbs sampler that scales well as data dimensionality increases. The gain in efficiency is achieved by the joint conjugacy property of the proposed prior, which allows block updating of the loadings matrix. We propose an adaptive Gibbs sampler for automatically truncating the infinite loading matrix through selection of the number of important factors. Theoretical results are provided on the support of the prior and truncation approximation bounds. A fast algorithm is proposed to produce approximate Bayes estimates. Latent factor regression methods are developed for prediction and variable selection in applications with high-dimensional correlated predictors. Operating characteristics are assessed through simulation studies, and the approach is applied to predict survival times from gene expression data.},
	number = {2},
	urldate = {2017-03-08},
	journal = {Biometrika},
	author = {Bhattacharya, A. and Dunson, D. B.},
	month = jun,
	year = {2011},
	pmid = {23049129},
	pmcid = {PMC3419391},
	pages = {291--306},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/HPF6PKVW/Bhattacharya and Dunson - 2011 - Sparse Bayesian infinite factor models.pdf:application/pdf}
}

@incollection{ghahramani_infinite_2006,
	title = {Infinite latent feature models and the {Indian} buffet process},
	url = {http://papers.nips.cc/paper/2882-infinite-latent-feature-models-and-the-indian-buffet-process.pdf},
	urldate = {2017-03-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 18},
	publisher = {MIT Press},
	author = {Ghahramani, Zoubin and Griffiths, Thomas L.},
	editor = {Weiss, Y. and Schölkopf, P. B. and Platt, J. C.},
	year = {2006},
	pages = {475--482},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/6HEP7S29/Ghahramani and Griffiths - 2006 - Infinite latent feature models and the Indian buff.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/8BI843I9/2882-infinite-latent-feature-models-and-the-indian-buffet-process.html:text/html}
}

@article{ishwaran_spike_2005,
	title = {Spike and slab variable selection: {Frequentist} and {Bayesian} strategies},
	volume = {33},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Spike and slab variable selection},
	url = {http://projecteuclid.org/euclid.aos/1117114335},
	doi = {10.1214/009053604000001147},
	abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure’s ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.},
	number = {2},
	urldate = {2017-03-08},
	journal = {The Annals of Statistics},
	author = {Ishwaran, Hemant and Rao, J. Sunil},
	month = apr,
	year = {2005},
	mrnumber = {MR2163158},
	zmnumber = {1068.62079},
	keywords = {Shrinkage, Generalized ridge regression, hypervariance, model averaging, model uncertainty, ordinary least squares, penalization, rescaling, stochastic variable selection, Zcut},
	pages = {730--773},
	file = {Snapshot:/home/jeremiah/Zotero/storage/RU3EN2RQ/1117114335.html:text/html}
}

@article{rockova_fast_2016,
	title = {Fast {Bayesian} {Factor} {Analysis} via {Automatic} {Rotations} to {Sparsity}},
	volume = {111},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2015.1100620},
	doi = {10.1080/01621459.2015.1100620},
	abstract = {Rotational post hoc transformations have traditionally played a key role in enhancing the interpretability of factor analysis. Regularization methods also serve to achieve this goal by prioritizing sparse loading matrices. In this work, we bridge these two paradigms with a unifying Bayesian framework. Our approach deploys intermediate factor rotations throughout the learning process, greatly enhancing the effectiveness of sparsity inducing priors. These automatic rotations to sparsity are embedded within a PXL-EM algorithm, a Bayesian variant of parameter-expanded EM for posterior mode detection. By iterating between soft-thresholding of small factor loadings and transformations of the factor basis, we obtain (a) dramatic accelerations, (b) robustness against poor initializations, and (c) better oriented sparse solutions. To avoid the prespecification of the factor cardinality, we extend the loading matrix to have infinitely many columns with the Indian buffet process (IBP) prior. The factor dimensionality is learned from the posterior, which is shown to concentrate on sparse matrices. Our deployment of PXL-EM performs a dynamic posterior exploration, outputting a solution path indexed by a sequence of spike-and-slab priors. For accurate recovery of the factor loadings, we deploy the spike-and-slab LASSO prior, a two-component refinement of the Laplace prior. A companion criterion, motivated as an integral lower bound, is provided to effectively select the best recovery. The potential of the proposed procedure is demonstrated on both simulated and real high-dimensional data, which would render posterior simulation impractical. Supplementary materials for this article are available online.},
	number = {516},
	urldate = {2017-03-08},
	journal = {Journal of the American Statistical Association},
	author = {Ročková, Veronika and George, Edward I.},
	month = oct,
	year = {2016},
	keywords = {Parameter expansion, EM algorithm, Factor rotations, Sparsity, Spike-and-Slab LASSO},
	pages = {1608--1622},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/SXVMM48J/Ročková and George - 2016 - Fast Bayesian Factor Analysis via Automatic Rotati.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/4FCSBD6X/01621459.2015.html:text/html}
}

@article{wold_cross-validatory_1978,
	title = {Cross-{Validatory} {Estimation} of the {Number} of {Components} in {Factor} and {Principal} {Components} {Models}},
	volume = {20},
	issn = {0040-1706},
	url = {http://www.jstor.org/stable/1267639},
	doi = {10.2307/1267639},
	abstract = {By means of factor analysis (FA) or principal components analysis (PCA) a matrix Y with the elements y$_{\textrm{ik}}$ is approximated by the model y$_{\textrm{ik}}$=α $_{\textrm{i}}$+∑$_{\textrm{a=1}}$$^{\textrm{A}}$β $_{\textrm{ia}}$θ $_{\textrm{ak}}$+ε $_{\textrm{ik}}$. Here the parameters α, β and Θ express the systematic part of the data y$_{\textrm{ik}}$, "signal," and the residuals ε $_{\textrm{ik}}$ express the "random" part, "noise." When applying FA or PCA to a matrix of real data obtained, for example, by characterizing N chemical mixtures by M measured variables, one major problem is the estimation of the rank A of the matrix Y, i. e. the estimation of how much of the data y$_{\textrm{ik}}$ is "signal" and how much is "noise." Cross validation can be used to approach this problem. The matrix Y is partitioned and the rank A is determined so as to maximize the predictive properties of model (1) when the parameters are estimated on one part of the matrix Y and the prediction tested on another part of the matrix Y.},
	number = {4},
	urldate = {2017-03-07},
	journal = {Technometrics},
	author = {Wold, Svante},
	year = {1978},
	pages = {397--405}
}

@article{ghosh_default_2009,
	title = {Default {Prior} {Distributions} and {Efficient} {Posterior} {Computation} in {Bayesian} {Factor} {Analysis}},
	volume = {18},
	issn = {1061-8600},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3755784/},
	doi = {10.1198/jcgs.2009.07145},
	abstract = {Factor analytic models are widely used in social sciences. These models have also proven useful for sparse modeling of the covariance structure in multidimensional data. Normal prior distributions for factor loadings and inverse gamma prior distributions for residual variances are a popular choice because of their conditionally conjugate form. However, such prior distributions require elicitation of many hyperparameters and tend to result in poorly behaved Gibbs samplers. In addition, one must choose an informative specification, as high variance prior distributions face problems due to impropriety of the posterior distribution. This article proposes a default, heavy-tailed prior distribution specification, which is induced through parameter expansion while facilitating efficient posterior computation. We also develop an approach to allow uncertainty in the number of factors. The methods are illustrated through simulated examples and epidemiology and toxicology applications. Data sets and computer code used in this article are available online.},
	number = {2},
	urldate = {2017-03-07},
	journal = {Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America},
	author = {Ghosh, Joyee and Dunson, David B.},
	month = jun,
	year = {2009},
	pmid = {23997568},
	pmcid = {PMC3755784},
	pages = {306--320},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/ECCXZU5M/Ghosh and Dunson - 2009 - Default Prior Distributions and Efficient Posterio.pdf:application/pdf}
}

@article{lam_factor_2012,
	title = {Factor modeling for high-dimensional time series: {Inference} for the number of factors},
	volume = {40},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Factor modeling for high-dimensional time series},
	url = {http://projecteuclid.org/euclid.aos/1337268209},
	doi = {10.1214/12-AOS970},
	abstract = {This paper deals with the factor modeling for high-dimensional time series based on a dimension-reduction viewpoint. Under stationary settings, the inference is simple in the sense that both the number of factors and the factor loadings are estimated in terms of an eigenanalysis for a nonnegative definite matrix, and is therefore applicable when the dimension of time series is on the order of a few thousands. Asymptotic properties of the proposed method are investigated under two settings: (i) the sample size goes to infinity while the dimension of time series is fixed; and (ii) both the sample size and the dimension of time series go to infinity together. In particular, our estimators for zero-eigenvalues enjoy faster convergence (or slower divergence) rates, hence making the estimation for the number of factors easier. In particular, when the sample size and the dimension of time series go to infinity together, the estimators for the eigenvalues are no longer consistent. However, our estimator for the number of the factors, which is based on the ratios of the estimated eigenvalues, still works fine. Furthermore, this estimation shows the so-called “blessing of dimensionality” property in the sense that the performance of the estimation may improve when the dimension of time series increases. A two-step procedure is investigated when the factors are of different degrees of strength. Numerical illustration with both simulated and real data is also reported.},
	language = {EN},
	number = {2},
	urldate = {2017-03-07},
	journal = {The Annals of Statistics},
	author = {Lam, Clifford and Yao, Qiwei},
	month = apr,
	year = {2012},
	mrnumber = {MR2933663},
	zmnumber = {1273.62214},
	keywords = {Autocovariance matrices, blessing of dimensionality, eigenanalysis, fast convergence rates, multivariate time series, ratio-based estimator, strength of factors, white noise},
	pages = {694--726},
	file = {Snapshot:/home/jeremiah/Zotero/storage/9ZGITTKZ/1337268209.html:text/html}
}

@article{lam_estimation_nodate,
	title = {Estimation of latent factors for high-dimensional time series},
	url = {http://www.academia.edu/8584369/Estimation_of_latent_factors_for_high-dimensional_time_series},
	abstract = {Estimation of latent factors for high-dimensional time series},
	urldate = {2017-03-07},
	author = {Lam, Clifford},
	file = {Snapshot:/home/jeremiah/Zotero/storage/N3ZNMGDQ/Estimation_of_latent_factors_for_high-dimensional_time_series.html:text/html}
}

@article{kucukelbir_automatic_2015,
	title = {Automatic {Variational} {Inference} in {Stan}},
	url = {http://arxiv.org/abs/1506.03431},
	abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
	urldate = {2017-03-07},
	journal = {arXiv:1506.03431 [stat]},
	author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03431},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1506.03431 PDF:/home/jeremiah/Zotero/storage/XWEBIQVK/Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/CP2TPEGD/1506.html:text/html}
}

@book{barber_bayesian_2011,
	address = {New York, NY, USA},
	title = {Bayesian {Time} {Series} {Models}},
	isbn = {978-0-521-19676-5},
	abstract = {'What's going to happen next?' Time series data hold the answers, and Bayesian methods represent the cutting edge in learning what they have to say. This ambitious book is the first unified treatment of the emerging knowledge-base in Bayesian time series techniques. Exploiting the unifying framework of probabilistic graphical models, the book covers approximation schemes, both Monte Carlo and deterministic, and introduces switching, multi-object, non-parametric and agent-based models in a variety of application environments. It demonstrates that the basic framework supports the rapid creation of models tailored to specific applications and gives insight into the computational complexity of their implementation. The authors span traditional disciplines such as statistics and engineering and the more recently established areas of machine learning and pattern recognition. Readers with a basic understanding of applied probability, but no experience with time series analysis, are guided from fundamental concepts to the state-of-the-art in research and practice.},
	publisher = {Cambridge University Press},
	author = {Barber, David and Cemgil, A. Taylan and Chiappa, Silvia},
	year = {2011}
}

@inproceedings{schmidt_bayesian_2009,
	title = {Bayesian {Non}-negative {Matrix} {Factorization}},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-00599-2_68},
	doi = {10.1007/978-3-642-00599-2_68},
	abstract = {We present a Bayesian treatment of non-negative matrix factorization (NMF), based on a normal likelihood and exponential priors, and derive an efficient Gibbs sampler to approximate the posterior density of the NMF factors. On a chemical brain imaging data set, we show that this improves interpretability by providing uncertainty estimates. We discuss how the Gibbs sampler can be used for model order selection by estimating the marginal likelihood, and compare with the Bayesian information criterion. For computing the maximum a posteriori estimate we present an iterated conditional modes algorithm that rivals existing state-of-the-art NMF algorithms on an image feature extraction problem.},
	language = {en},
	urldate = {2017-03-07},
	booktitle = {Independent {Component} {Analysis} and {Signal} {Separation}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Schmidt, Mikkel N. and Winther, Ole and Hansen, Lars Kai},
	month = mar,
	year = {2009},
	pages = {540--547},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/URARQ26N/Schmidt et al. - 2009 - Bayesian Non-negative Matrix Factorization.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/U6M9MTFT/978-3-642-00599-2_68.html:text/html}
}

@inproceedings{zhong_reversible_2009,
	title = {Reversible {Jump} {MCMC} for {Non}-{Negative} {Matrix} {Factorization}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/AISTATS09_ZhongG},
	urldate = {2017-03-07},
	author = {Zhong, Mingjun and Girolami, Mark},
	year = {2009},
	pages = {663--670},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/4H6C6C7R/Zhong and Girolami - 2009 - Reversible Jump MCMC for Non-Negative Matrix Facto.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/98K3K247/AISTATS09_ZhongG.html:text/html}
}

@phdthesis{paquet_bayesian_nodate,
	title = {Bayesian inference for latent variable models},
	url = {/paper/Bayesian-inference-for-latent-variable-models-Paquet/32cdfe4f396150800f9061a3ee2e8a07cdbdf6f9},
	abstract = {Bayes’ theorem is the cornerstone of statistical inference. It provides the tools for dealing with
knowledge in an uncertain world, allowing us to explain observed phenomena through the re-
finement of belief in model parameters. At the heart of this elegant framework lie intractable
integrals, whether in computing an average over some posterior distribution, or in determining
the normalizing constant of a distribution. This thesis examines both deterministic and
stochastic methods in which these integrals can be treated. Of particular interest shall be parametric
models where the parameter space can be extended with additional latent variables to
get distributions that are easier to handle algorithmically.},
	urldate = {2017-03-07},
	author = {Paquet, Ulrich},
	file = {Snapshot:/home/jeremiah/Zotero/storage/ISNPIJEH/32cdfe4f396150800f9061a3ee2e8a07cdbdf6f9.html:text/html}
}

@phdthesis{shakir_generalised_nodate,
	title = {Generalised {Bayesian} matrix factorisation models},
	url = {/paper/Generalised-Bayesian-matrix-factorisation-models-Mohamed/250b96d6efb481d4888c6ac87bc74187af2f02c1},
	abstract = {Semantic Scholar extracted view of \&quot;Generalised Bayesian matrix factorisation models\&quot; by Shakir Mohamed},
	urldate = {2017-03-07},
	author = {Shakir, Mohamed},
	file = {Snapshot:/home/jeremiah/Zotero/storage/X922S8TG/250b96d6efb481d4888c6ac87bc74187af2f02c1.html:text/html}
}

@article{cemgil_bayesian_2009,
	title = {Bayesian {Inference} for {Nonnegative} {Matrix} {Factorisation} {Models}},
	volume = {2009},
	issn = {1687-5265},
	url = {https://www.hindawi.com/journals/cin/2009/785152/abs/},
	doi = {10.1155/2009/785152},
	abstract = {We describe nonnegative matrix factorisation (NMF) with a Kullback-Leibler (KL) error measure in a statistical framework, with a hierarchical generative model consisting of an observation and a prior component. Omitting the prior leads to the standard KL-NMF algorithms as special cases, where maximum likelihood parameter estimation is carried out via the Expectation-Maximisation (EM) algorithm. Starting from this view, we develop full Bayesian inference via variational Bayes or Monte Carlo. Our construction retains conjugacy and enables us to develop more powerful models while retaining attractive features of standard NMF such as monotonic convergence and easy implementation. We illustrate our approach on model order selection and image reconstruction.},
	language = {en},
	urldate = {2017-03-07},
	journal = {Computational Intelligence and Neuroscience},
	author = {Cemgil, Ali Taylan},
	month = may,
	year = {2009},
	pmid = {19536273},
	pages = {e785152},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/JNGJHM3Q/Cemgil - 2009 - Bayesian Inference for Nonnegative Matrix Factoris.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/XVRWTAK7/785152.html:application/xhtml+xml}
}

@inproceedings{collins_generalization_2001-1,
	title = {A generalization of principal component analysis to the exponential family},
	abstract = {Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data. 1},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Collins, Michael and Dasgupta, Sanjoy and Schapire, Robert E.},
	year = {2001},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/QV82SFD9/Collins et al. - 2001 - A generalization of principal component analysis t.pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/P849DKCW/summary.html:text/html}
}

@article{zhang_semi-separable_2014,
	title = {Semi-{Separable} {Hamiltonian} {Monte} {Carlo} for {Inference} in {Bayesian} {Hierarchical} {Models}},
	url = {http://arxiv.org/abs/1406.3843},
	abstract = {Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.},
	urldate = {2017-03-07},
	journal = {arXiv:1406.3843 [cs, stat]},
	author = {Zhang, Yichuan and Sutton, Charles},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.3843},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Computation},
	file = {arXiv\:1406.3843 PDF:/home/jeremiah/Zotero/storage/FVRDNUQK/Zhang and Sutton - 2014 - Semi-Separable Hamiltonian Monte Carlo for Inferen.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WDIFMEXN/1406.html:text/html}
}

@incollection{mohamed_bayesian_2009-1,
	title = {Bayesian {Exponential} {Family} {PCA}},
	url = {http://papers.nips.cc/paper/3532-bayesian-exponential-family-pca.pdf},
	urldate = {2017-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 21},
	publisher = {Curran Associates, Inc.},
	author = {Mohamed, Shakir and Ghahramani, Zoubin and Heller, Katherine A},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
	pages = {1089--1096},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/6JWTC5U7/Mohamed et al. - 2009 - Bayesian Exponential Family PCA.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/X3M33J22/3532-bayesian-exponential-family-pca.html:text/html}
}

@inproceedings{salakhutdinov_bayesian_2008-1,
	address = {New York, NY, USA},
	series = {{ICML} '08},
	title = {Bayesian {Probabilistic} {Matrix} {Factorization} {Using} {Markov} {Chain} {Monte} {Carlo}},
	isbn = {978-1-60558-205-4},
	url = {http://doi.acm.org/10.1145/1390156.1390267},
	doi = {10.1145/1390156.1390267},
	abstract = {Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction accuracy than PMF models trained using MAP estimation.},
	urldate = {2017-03-07},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Salakhutdinov, Ruslan and Mnih, Andriy},
	year = {2008},
	pages = {880--887},
	file = {ACM Full Text PDF:/home/jeremiah/Zotero/storage/PM3P8469/Salakhutdinov and Mnih - 2008 - Bayesian Probabilistic Matrix Factorization Using .pdf:application/pdf}
}

@article{ahn_large-scale_2015-1,
	title = {Large-{Scale} {Distributed} {Bayesian} {Matrix} {Factorization} using {Stochastic} {Gradient} {MCMC}},
	url = {http://arxiv.org/abs/1503.01596},
	abstract = {Despite having various attractive qualities such as high prediction accuracy and the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix Factorization has not been widely adopted because of the prohibitive cost of inference. In this paper, we propose a scalable distributed Bayesian matrix factorization algorithm using stochastic gradient MCMC. Our algorithm, based on Distributed Stochastic Gradient Langevin Dynamics, can not only match the prediction accuracy of standard MCMC methods like Gibbs sampling, but at the same time is as fast and simple as stochastic gradient descent. In our experiments, we show that our algorithm can achieve the same level of prediction accuracy as Gibbs sampling an order of magnitude faster. We also show that our method reduces the prediction error as fast as distributed stochastic gradient descent, achieving a 4.1\% improvement in RMSE for the Netflix dataset and an 1.8\% for the Yahoo music dataset.},
	urldate = {2017-03-07},
	journal = {arXiv:1503.01596 [cs, stat]},
	author = {Ahn, Sungjin and Korattikara, Anoop and Liu, Nathan and Rajan, Suju and Welling, Max},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.01596},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1503.01596 PDF:/home/jeremiah/Zotero/storage/7HF4JNA5/Ahn et al. - 2015 - Large-Scale Distributed Bayesian Matrix Factorizat.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/5XQXMEW3/1503.html:text/html}
}

@article{liang_beta_2014,
	title = {Beta {Process} {Non}-negative {Matrix} {Factorization} with {Stochastic} {Structured} {Mean}-{Field} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1411.1804},
	abstract = {Beta process is the standard nonparametric Bayesian prior for latent factor model. In this paper, we derive a structured mean-field variational inference algorithm for a beta process non-negative matrix factorization (NMF) model with Poisson likelihood. Unlike the linear Gaussian model, which is well-studied in the nonparametric Bayesian literature, NMF model with beta process prior does not enjoy the conjugacy. We leverage the recently developed stochastic structured mean-field variational inference to relax the conjugacy constraint and restore the dependencies among the latent variables in the approximating variational distribution. Preliminary results on both synthetic and real examples demonstrate that the proposed inference algorithm can reasonably recover the hidden structure of the data.},
	urldate = {2017-03-07},
	journal = {arXiv:1411.1804 [cs, stat]},
	author = {Liang, Dawen and Hoffman, Matthew D.},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1804},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: 6 pages, 1 figure},
	file = {arXiv\:1411.1804 PDF:/home/jeremiah/Zotero/storage/ZDKAK6ZJ/Liang and Hoffman - 2014 - Beta Process Non-negative Matrix Factorization wit.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/M79PTF3V/1411.html:text/html}
}

@inproceedings{zhai_chinese_2016,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Chinese {Image} {Text} {Recognition} with {BLSTM}-{CTC}: {A} {Segmentation}-{Free} {Method}},
	isbn = {978-981-10-3004-8 978-981-10-3005-5},
	shorttitle = {Chinese {Image} {Text} {Recognition} with {BLSTM}-{CTC}},
	url = {https://link-springer-com.ezp-prod1.hul.harvard.edu/chapter/10.1007/978-981-10-3005-5_43},
	doi = {10.1007/978-981-10-3005-5_43},
	abstract = {This paper presents BLSTM-CTC (bidirectional LSTM-Connectionist Temporal Classification), a novel scheme to tackle the Chinese image text recognition problem. Different from traditional methods that perform the recognition on the single character level, the input of BLSTM-CTC is an image text composed of a line of characters and the output is a recognized text sequence, where the recognition is carried out on the whole image text level. To train a neural network for this challenging task, we collect over 2 million news titles from which we generate over 1 million noisy image texts, covering almost the vast majority of common Chinese characters. With these training data, a RNN training procedure is conducted to learn the recognizer. We also carry out some adaptations on the neural network to make it suitable for real scenarios. Experiments on text images from 13 TV channels demonstrate the effectiveness of the proposed pipeline. The results all outperform those of a baseline system.},
	language = {en},
	booktitle = {Pattern {Recognition}},
	publisher = {Springer, Singapore},
	author = {Zhai, Chuanlei and Chen, Zhineng and Li, Jie and Xu, Bo},
	month = nov,
	year = {2016},
	pages = {525--536},
	file = {10.1007_978-981-10-3005-5_43.pdf:/home/jeremiah/Zotero/storage/LLUPECVE/10.1007_978-981-10-3005-5_43.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/VNHW3EGY/978-981-10-3005-5_43.html:text/html}
}

@article{sabour_dynamic_2017,
	title = {Dynamic {Routing} {Between} {Capsules}},
	url = {http://arxiv.org/abs/1710.09829},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	urldate = {2017-10-28},
	journal = {arXiv:1710.09829 [cs]},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.09829},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1710.09829 PDF:/home/jeremiah/Zotero/storage/N2MDYKRN/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/UFEY7UWQ/1710.html:text/html}
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2017-10-28},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv\:1503.02531 PDF:/home/jeremiah/Zotero/storage/WKU9273Z/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/QVBXMK5N/1503.html:text/html}
}

@article{ranganath_operator_2016-1,
	title = {Operator {Variational} {Inference}},
	url = {http://arxiv.org/abs/1610.09033},
	abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
	urldate = {2017-10-27},
	journal = {arXiv:1610.09033 [cs, stat]},
	author = {Ranganath, Rajesh and Altosaar, Jaan and Tran, Dustin and Blei, David M.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.09033},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Appears in Neural Information Processing Systems, 2016},
	file = {arXiv\:1610.09033 PDF:/home/jeremiah/Zotero/storage/7AH7WX35/Ranganath et al. - 2016 - Operator Variational Inference.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/R7IIKGNG/1610.html:text/html}
}

@article{gorham_measuring_2017,
	title = {Measuring {Sample} {Quality} with {Kernels}},
	url = {http://arxiv.org/abs/1703.01717},
	abstract = {Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein's method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.},
	urldate = {2017-10-27},
	journal = {arXiv:1703.01717 [cs, stat]},
	author = {Gorham, Jackson and Mackey, Lester},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.01717},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1703.01717 PDF:/home/jeremiah/Zotero/storage/NPCV3JXV/Gorham and Mackey - 2017 - Measuring Sample Quality with Kernels.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/DKEH7DSJ/1703.html:text/html}
}

@article{bodenham_comparison_2016,
	title = {A comparison of efficient approximations for a weighted sum of chi-squared random variables},
	volume = {26},
	issn = {0960-3174, 1573-1375},
	url = {https://link.springer.com/article/10.1007/s11222-015-9583-4},
	doi = {10.1007/s11222-015-9583-4},
	abstract = {In many applications, the cumulative distribution function (cdf) FQNFQNF\_\{Q\_N\} of a positively weighted sum of N i.i.d. chi-squared random variables QNQNQ\_N is required. Although there is no known closed-form solution for FQNFQNF\_\{Q\_N\}, there are many good approximations. When computational efficiency is not an issue, Imhof’s method provides a good solution. However, when both the accuracy of the approximation and the speed of its computation are a concern, there is no clear preferred choice. Previous comparisons between approximate methods could be considered insufficient. Furthermore, in streaming data applications where the computation needs to be both sequential and efficient, only a few of the available methods may be suitable. Streaming data problems are becoming ubiquitous and provide the motivation for this paper. We develop a framework to enable a much more extensive comparison between approximate methods for computing the cdf of weighted sums of an arbitrary random variable. Utilising this framework, a new and comprehensive analysis of four efficient approximate methods for computing FQNFQNF\_\{Q\_N\} is performed. This analysis procedure is much more thorough and statistically valid than previous approaches described in the literature. A surprising result of this analysis is that the accuracy of these approximate methods increases with N.},
	language = {en},
	number = {4},
	journal = {Statistics and Computing},
	author = {Bodenham, Dean A. and Adams, Niall M.},
	month = jul,
	year = {2016},
	pages = {917--928},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/YTJVJAEE/Bodenham and Adams - 2016 - A comparison of efficient approximations for a wei.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/LHD6L4YT/s11222-015-9583-4.html:text/html}
}

@article{gleason_contaminated_2014,
	title = {Contaminated {Turmeric} {Is} a {Potential} {Source} of {Lead} {Exposure} for {Children} in {Rural} {Bangladesh}},
	volume = {2014},
	doi = {10.1155/2014/730636},
	abstract = {Background. During the conduct of a cohort study intended to study the associations between mixed metal exposures and child health outcomes, we found that 78\% of 309 children aged 20–40 months evaluated in the Munshiganj District of Bangladesh had blood lead concentrations ≥5 íµí¼g/dL and 27\% had concentrations ≥10 íµí¼g/dL. Hypothesis. Environmental sources such as spices (e.g., turmeric, which has already faced recalls in Bangladesh due to high lead levels) may be a potential route of lead exposure. Methods. We conducted visits to the homes of 28 children randomly selected from among high and low blood lead concentration groups. During the visits, we administered a structured questionnaire and obtained soil, dust, rice, and spice samples. We obtained water samples from community water sources, as well as environmental samples from neighborhood businesses. Results. Lead concentrations in many turmeric samples were elevated, with lead concentrations as high as 483 ppm. Analyses showed high bioaccessibility of lead. Conclusions. Contamination of turmeric powder is a potentially important source of lead exposure in this population.},
	journal = {Journal of Environmental and Public Health},
	author = {Gleason, Kelsey and P. Shine, James and Shobnam, Nadia and B. Rokoff, Lisa and Sultana Suchanda, Hafiza and Ibne Hasan, Md and Mostofa, Md and Amarasiriwardena, Chitra and Quamruzzaman, Quazi and Rahman, Mahmuder and Kile, Molly and Bellinger, David and Christiani, David and O. Wright, Robert and Mazumdar, Maitreyi},
	month = aug,
	year = {2014},
	file = {Snapshot:/home/jeremiah/Zotero/storage/5VADB7V2/265684343_Research_Article_Contaminated_Turmeric_Is_a_Potential_Source_of_Lead_Exposure_for_Chil.pdf:application/pdf}
}

@article{lukic_stochastic_2001,
	title = {Stochastic {Processes} with {Sample} {Paths} in {Reproducing} {Kernel} {Hilbert} {Spaces}},
	volume = {353},
	issn = {0002-9947},
	url = {http://www.jstor.org/stable/2693779},
	abstract = {A theorem of M. F. Driscoll says that, under certain restrictions, the probability that a given Gaussian process has its sample paths almost surely in a given reproducing kernel Hilbert space (RKHS) is either 0 or 1. Driscoll also found a necessary and sufficient condition for that probability to be 1. Doing away with Driscoll's restrictions, R. Fortet generalized his condition and named it nuclear dominance. He stated a theorem claiming nuclear dominance to be necessary and sufficient for the existence of a process (not necessarily Gaussian) having its sample paths in a given RKHS. This theorem - specifically the necessity of the condition - turns out to be incorrect, as we will show via counterexamples. On the other hand, a weaker sufficient condition is available. Using Fortet's tools along with some new ones, we correct Fortet's theorem and then find the generalization of Driscoll's result. The key idea is that of a random element in a RKHS whose values are sample paths of a stochastic process. As in Fortet's work, we make almost no assumptions about the reproducing kernels we use, and we demonstrate the extent to which one may dispense with the Gaussian assumption.},
	number = {10},
	urldate = {2017-10-26},
	journal = {Transactions of the American Mathematical Society},
	author = {Lukić, Milan N. and Beder, Jay H.},
	year = {2001},
	pages = {3945--3969}
}

@article{liu_robust_2017,
	title = {Robust {Hypothesis} {Test} for {Nonlinear} {Effect} with {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1710.01406},
	abstract = {This work constructs a hypothesis test for detecting whether an data-generating function \$h: R{\textasciicircum}p {\textbackslash}rightarrow R\$ belongs to a specific reproducing kernel Hilbert space H0 , where the structure of \$H\_0\$ is only partially known. Utilizing the theory of reproducing kernels, we reduce this hypothesis to a simple one-sided score test for a scalar parameter, develop a testing procedure that is robust against the mis-specification of kernel functions, and also propose an ensemble-based estimator for the null model to guarantee test performance in small samples. To demonstrate the utility of the proposed method, we apply our test to the problem of detecting nonlinear interaction between groups of continuous features. We evaluate the finite-sample performance of our test under different data-generating functions and estimation strategies for the null model. Our results reveal interesting connections between notions in machine learning (model underfit/overfit) and those in statisticalinference (i.e. Type I error/power of hypothesis test), and also highlight unexpected consequences of common model estimating strategies (e.g. estimating kernel hyperparameters using maximum likelihood estimation) on model inference.},
	urldate = {2017-10-26},
	journal = {arXiv:1710.01406 [stat]},
	author = {Liu, Jeremiah Zhe and Coull, Brent},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.01406},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1710.01406 PDF:/home/jeremiah/Zotero/storage/Q2VJ548N/Liu and Coull - 2017 - Robust Hypothesis Test for Nonlinear Effect with G.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/IXLZT2B3/1710.html:text/html}
}

@book{mcculloch_generalized_2011,
	title = {Generalized, {Linear}, and {Mixed} {Models}},
	isbn = {978-1-118-20996-7},
	abstract = {An accessible and self-contained introduction to statistical models-now in a modernized new edition  Generalized, Linear, and Mixed Models, Second Edition provides an up-to-date treatment of the essential techniques for developing and applying a wide variety of statistical models. The book presents thorough and unified coverage of the theory behind generalized, linear, and mixed models and highlights their similarities and differences in various construction, application, and computational aspects.  A clear introduction to the basic ideas of fixed effects models, random effects models, and mixed models is maintained throughout, and each chapter illustrates how these models are applicable in a wide array of contexts. In addition, a discussion of general methods for the analysis of such models is presented with an emphasis on the method of maximum likelihood for the estimation of parameters. The authors also provide comprehensive coverage of the latest statistical models for correlated, non-normally distributed data. Thoroughly updated to reflect the latest developments in the field, the Second Edition features:  A new chapter that covers omitted covariates, incorrect random effects distribution, correlation of covariates and random effects, and robust variance estimation A new chapter that treats shared random effects models, latent class models, and properties of models A revised chapter on longitudinal data, which now includes a discussion of generalized linear models, modern advances in longitudinal data analysis, and the use between and within covariate decompositions Expanded coverage of marginal versus conditional models Numerous new and updated examples   With its accessible style and wealth of illustrative exercises, Generalized, Linear, and Mixed Models, Second Edition is an ideal book for courses on generalized linear and mixed models at the upper-undergraduate and beginning-graduate levels. It also serves as a valuable reference for applied statisticians, industrial practitioners, and researchers.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {McCulloch, Charles E. and Searle, Shayle R. and Neuhaus, John M.},
	month = sep,
	year = {2011},
	note = {Google-Books-ID: kyvgyK\_sBlkC},
	keywords = {Mathematics / Probability \& Statistics / Stochastic Processes, Social Science / Statistics}
}

@article{harville_maximum_1977-1,
	title = {Maximum {Likelihood} {Approaches} to {Variance} {Component} {Estimation} and to {Related} {Problems}},
	volume = {72},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2286796},
	doi = {10.2307/2286796},
	abstract = {Recent developments promise to increase greatly the popularity of maximum likelihood (ML) as a technique for estimating variance components. Patterson and Thompson (1971) proposed a restricted maximum likelihood (REML) approach which takes into account the loss in degrees of freedom resulting from estimating fixed effects. Miller (1973) developed a satisfactory asymptotic theory for ML estimators of variance components. There are many iterative algorithms that can be considered for computing the ML or REML estimates. The computations on each iteration of these algorithms are those associated with computing estimates of fixed and random effects for given values of the variance components.},
	number = {358},
	urldate = {2017-10-26},
	journal = {Journal of the American Statistical Association},
	author = {Harville, David A.},
	year = {1977},
	pages = {320--338}
}

@article{lamotte_direct_2007,
	title = {A direct derivation of the {REML} likelihood function},
	volume = {48},
	issn = {0932-5026, 1613-9798},
	url = {https://link-springer-com.ezp-prod1.hul.harvard.edu/article/10.1007/s00362-006-0335-6},
	doi = {10.1007/s00362-006-0335-6},
	abstract = {The original derivation of the widely cited form of the REML likelihood function for mixed linear models is difficult and indirect. This paper derives it directly using familiar operations with matrices and determinants.},
	language = {en},
	number = {2},
	journal = {Statistical Papers},
	author = {LaMotte, Lynn Roy},
	month = apr,
	year = {2007},
	pages = {321--327},
	file = {Snapshot:/home/jeremiah/Zotero/storage/R3FWH5JD/s00362-006-0335-6.html:text/html}
}

@article{psych_screening_1990,
	title = {Screening for depression during pregnancy with the edinburgh depression scale ({EDDS})},
	volume = {8},
	issn = {0264-6838},
	url = {http://dx.doi.org/10.1080/02646839008403615},
	doi = {10.1080/02646839008403615},
	abstract = {One hundred women attending a maternity hospital antenatal clinic who were between 28 and 34 weeks gestation completed the EPDS and were then interviewed using a standardized psychiatric interview. EPDS scores were compared with RDC diagnosis of major and minor depression and with total weighted score derived from the interview. The EPDS identified all women with RDC major depression but was less effective in detecting those with RDC minor depression.},
	number = {2},
	urldate = {2017-10-26},
	journal = {Journal of Reproductive and Infant Psychology},
	author = {Psych, Declan Murray MRC and John L. Cox MA, DM, FRC Psych, FRCP (Ed)},
	month = apr,
	year = {1990},
	pages = {99--107},
	file = {Snapshot:/home/jeremiah/Zotero/storage/4XPSNQNK/02646839008403615.html:text/html}
}

@article{lamotte_direct_2007-1,
	title = {A direct derivation of the {REML} likelihood function},
	volume = {48},
	issn = {0932-5026, 1613-9798},
	url = {https://link.springer.com/article/10.1007/s00362-006-0335-6},
	doi = {10.1007/s00362-006-0335-6},
	abstract = {The original derivation of the widely cited form of the REML likelihood function for mixed linear models is difficult and indirect. This paper derives it directly using familiar operations with matrices and determinants.},
	language = {en},
	number = {2},
	journal = {Statistical Papers},
	author = {LaMotte, Lynn Roy},
	month = apr,
	year = {2007},
	pages = {321--327},
	file = {Snapshot:/home/jeremiah/Zotero/storage/KFBVJN4D/s00362-006-0335-6.html:text/html}
}

@article{kile_prospective_2014,
	title = {A prospective cohort study of the association between drinking water arsenic exposure and self-reported maternal health symptoms during pregnancy in {Bangladesh}},
	volume = {13},
	issn = {1476-069X},
	url = {https://doi.org/10.1186/1476-069X-13-29},
	doi = {10.1186/1476-069X-13-29},
	abstract = {Arsenic, a common groundwater pollutant, is associated with adverse reproductive health but few studies have examined its effect on maternal health.},
	urldate = {2017-10-26},
	journal = {Environmental Health},
	author = {Kile, Molly L. and Rodrigues, Ema G. and Mazumdar, Maitreyi and Dobson, Christine B. and Diao, Nancy and Golam, Mostofa and Quamruzzaman, Quazi and Rahman, Mahmudar and Christiani, David C.},
	month = apr,
	year = {2014},
	keywords = {Arsenic, Cramping, Environmental health, Maternal health, Nausea, Reproductive health, Vomiting},
	pages = {29},
	annote = {Pages 29 in PDF},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/CAU64UTQ/Kile et al. - 2014 - A prospective cohort study of the association betw.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/PVF8BQDE/1476-069X-13-29.html:text/html}
}

@article{black_iron_2004,
	title = {Iron and zinc supplementation promote motor development and exploratory behavior among {Bangladeshi} infants},
	volume = {80},
	issn = {0002-9165, 1938-3207},
	url = {http://ajcn.nutrition.org/content/80/4/903},
	abstract = {Background:Iron and zinc deficiency are prevalent during infancy in low-income countries.
Objectives: The objectives were to examine whether a weekly supplement of iron, zinc, iron+zinc, or a micronutrient mix (MM) of 16 vitamins and minerals would alter infant development and behavior.
Design: The participants were 221 infants from rural Bangladesh at risk of micronutrient deficiencies. Development and behavior were evaluated at 6 and 12 mo of age by using the Bayley Scales of Infant Development II and the Home Observation Measurement of Environment (HOME) scale. In this double-blind trial, the infants were randomly assigned to 1 of 5 treatment conditions: iron (20 mg), zinc (20 mg), iron+zinc, MM (16 vitamins and minerals, including iron and zinc), or riboflavin weekly from 6 to 12 mo. Multivariate analyses were conducted to examine the change in development and behavior for each supplementation group, with control for maternal education, HOME score, months breastfed, anemia, growth at 6 mo, and change in growth from 6 to 12 mo.
Results: Iron and zinc administered together and with other micronutrients had a beneficial effect on infant motor development. Iron and zinc administered individually and in combination had a beneficial effect on orientation-engagement. Two-thirds of the infants were mildly anemic, no treatment effects on hemoglobin concentration were observed, and hemoglobin was not associated with measures of development or behavior.
Conclusion: The beneficial effects of weekly iron and zinc supplementation on motor development and orientation-engagement suggest that infants benefit from these minerals when administered together.},
	language = {en},
	number = {4},
	journal = {The American Journal of Clinical Nutrition},
	author = {Black, Maureen M. and Baqui, Abdullah H. and Zaman, K. and Persson, Lars Ake and Arifeen, Shams El and Le, Katherine and McNary, Scot W. and Parveen, Monowara and Hamadani, Jena D. and Black, Robert E.},
	month = oct,
	year = {2004},
	pmid = {15447897},
	keywords = {Bangladesh, functional isolation, infant development, Iron, micronutrient supplementation, zinc},
	pages = {903--910},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/3MN4FADW/Black et al. - 2004 - Iron and zinc supplementation promote motor develo.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/C5LLLPMK/903.html:text/html}
}

@article{claus_henn_chemical_2014,
	title = {Chemical mixtures and children's health},
	volume = {26},
	issn = {1531-698X},
	doi = {10.1097/MOP.0000000000000067},
	abstract = {PURPOSE OF REVIEW: Humans are routinely exposed to multiple chemicals simultaneously or sequentially. There is evidence that the toxicity of individual chemicals may depend on the presence of other chemicals. Studies on chemical mixtures are limited, however, because of the lack of sufficient exposure data, limited statistical power, and difficulty in the interpretation of multidimensional interactions. This review summarizes the recent literature examining chemical mixtures and pediatric health outcomes, with an emphasis on metal mixtures.
RECENT FINDINGS: Several studies report significant interactions between metals in relation to pediatric health outcomes. Two prospective studies found interactive effects of early-life lead and manganese exposures on cognition. In two different cohorts, interactions between lead and cadmium exposures were reported on reproductive hormone levels and on neurodevelopment. Effects of lead exposure on impulsive behavior and cognition were modified by mercury exposure in studies from Canada and Denmark. However, there is little consistency related to exposure indicators and statistical approaches for evaluating interaction.
SUMMARY: Several studies suggest that metals interact to cause health effects that are different from exposure to each metal alone. Despite the nearly infinite number of possible chemical combinations, mixtures research represents real-life exposure scenarios and warrants more attention, particularly in the context of uniquely vulnerable children.},
	language = {eng},
	number = {2},
	journal = {Current Opinion in Pediatrics},
	author = {Claus Henn, Birgit and Coull, Brent A. and Wright, Robert O.},
	month = apr,
	year = {2014},
	pmid = {24535499},
	pmcid = {PMC4043217},
	keywords = {Humans, Female, Pregnancy, Risk Factors, Environmental Exposure, Child, Prospective Studies, Arsenic, Cadmium, Child Welfare, Child, Preschool, Congenital Abnormalities, Environmental Pollutants, Infant, Lead, Manganese, Metals, Population Surveillance, Prenatal Exposure Delayed Effects, Prevalence},
	pages = {223--229}
}

@article{gul_minimax_2015,
	title = {Minimax {Robust} {Hypothesis} {Testing}},
	url = {http://arxiv.org/abs/1502.00647},
	abstract = {The minimax robust hypothesis testing problem for the case where the nominal probability distributions are subject to both modeling errors and outliers is studied in twofold. First, a robust hypothesis testing scheme based on a relative entropy distance is designed. This approach provides robustness with respect to modeling errors and is a generalization of a previous work proposed by Levy. Then, it is shown that this scheme can be combined with Huber's robust test through a composite uncertainty class, for which the existence of a saddle value condition is also proven. The composite version of the robust hypothesis testing scheme as well as the individual robust tests are extended to fixed sample size and sequential probability ratio tests. The composite model is shown to extend to robust estimation problems as well. Simulation results are provided to validate the proposed assertions.},
	urldate = {2017-10-20},
	journal = {arXiv:1502.00647 [cs, math]},
	author = {Gül, Gökhan and Zoubir, Abdelhak M.},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.00647},
	keywords = {Computer Science - Information Theory},
	file = {arXiv\:1502.00647 PDF:/home/jeremiah/Zotero/storage/M98NZNTH/Gül and Zoubir - 2015 - Minimax Robust Hypothesis Testing.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/CINSGWNA/1502.html:text/html}
}

@article{psych_screening_1990-1,
	title = {Screening for depression during pregnancy with the edinburgh depression scale ({EDDS})},
	volume = {8},
	issn = {0264-6838},
	url = {http://dx.doi.org/10.1080/02646839008403615},
	doi = {10.1080/02646839008403615},
	abstract = {One hundred women attending a maternity hospital antenatal clinic who were between 28 and 34 weeks gestation completed the EPDS and were then interviewed using a standardized psychiatric interview. EPDS scores were compared with RDC diagnosis of major and minor depression and with total weighted score derived from the interview. The EPDS identified all women with RDC major depression but was less effective in detecting those with RDC minor depression.},
	number = {2},
	urldate = {2017-10-17},
	journal = {Journal of Reproductive and Infant Psychology},
	author = {Psych, Declan Murray MRC and John L. Cox MA, DM, FRC Psych, FRCP (Ed)},
	month = apr,
	year = {1990},
	pages = {99--107},
	file = {Snapshot:/home/jeremiah/Zotero/storage/CCM5SGEU/02646839008403615.html:text/html}
}

@book{raven_manual_1998,
	edition = {1998 ed},
	title = {Manual for {Raven}'s progressive matrices and vocabulary scales},
	isbn = {978-1-85639-052-1},
	url = {http://trove.nla.gov.au/work/8583649},
	abstract = {In 8 libraries. Psychological tests.; Psychology - Testing.; Intellect - Testing.},
	language = {English},
	urldate = {2017-10-17},
	publisher = {Oxford : Oxford Psychologists},
	author = {Raven, John and Raven, J. C. and Court, John Hugh},
	year = {1998},
	annote = { Previous ed.: 1995 },
	file = {Trove thumbnail image:/home/jeremiah/Zotero/storage/MA2DLUCT/missing.gif:image/gif}
}

@article{lin_validation_2017,
	title = {Validation of a {Dish}-{Based} {Semiquantitative} {Food} {Questionnaire} in {Rural} {Bangladesh}},
	volume = {9},
	issn = {2072-6643},
	doi = {10.3390/nu9010049},
	abstract = {A locally validated tool was needed to evaluate long-term dietary intake in rural Bangladesh. We assessed the validity of a 42-item dish-based semi-quantitative food frequency questionnaire (FFQ) using two 3-day food diaries (FDs). We selected a random subset of 47 families (190 participants) from a longitudinal arsenic biomonitoring study in Bangladesh to administer the FFQ. Two 3-day FDs were completed by the female head of the households and we used an adult male equivalent method to estimate the FD for the other participants. Food and nutrient intakes measured by FFQ and FD were compared using Pearson's and Spearman's correlation, paired t-test, percent difference, cross-classification, weighted Kappa, and Bland-Altman analysis. Results showed good validity for total energy intake (paired t-test, p {\textless} 0.05; percent difference {\textless}10\%), with no presence of proportional bias (Bland-Altman correlation, p {\textgreater} 0.05). After energy-adjustment and de-attenuation for within-person variation, macronutrient intakes had excellent correlations ranging from 0.55 to 0.70. Validity for micronutrients was mixed. High intraclass correlation coefficients (ICCs) were found for most nutrients between the two seasons, except vitamin A. This dish-based FFQ provided adequate validity to assess and rank long-term dietary intake in rural Bangladesh for most food groups and nutrients, and should be useful for studying dietary-disease relationships.},
	language = {eng},
	number = {1},
	journal = {Nutrients},
	author = {Lin, Pi-I. D. and Bromage, Sabri and Mostofa, Md Golam and Allen, Joseph and Oken, Emily and Kile, Molly L. and Christiani, David C.},
	month = jan,
	year = {2017},
	pmid = {28075369},
	pmcid = {PMC5295093},
	keywords = {Humans, Female, Male, Body Mass Index, Reproducibility of Results, Child, Longitudinal Studies, Models, Theoretical, Adult, Middle Aged, Adolescent, Young Adult, Bangladesh, adult male equivalent, Diet Records, duplicate food sample, Energy Intake, food diary, food frequency questionnaire, Micronutrients, Nutrition Assessment, Rural Population, Seasons, Socioeconomic Factors, Surveys and Questionnaires, validation study, Vitamin A}
}

@article{valeri_joint_2017,
	title = {The {Joint} {Effect} of {Prenatal} {Exposure} to {Metal} {Mixtures} on {Neurodevelopmental} {Outcomes} at 20-40 {Months} of {Age}: {Evidence} from {Rural} {Bangladesh}},
	volume = {125},
	issn = {1552-9924},
	shorttitle = {The {Joint} {Effect} of {Prenatal} {Exposure} to {Metal} {Mixtures} on {Neurodevelopmental} {Outcomes} at 20-40 {Months} of {Age}},
	doi = {10.1289/EHP614},
	abstract = {BACKGROUND: Exposure to chemical mixtures is recognized as the real-life scenario in all populations, needing new statistical methods that can assess their complex effects.
OBJECTIVES: We aimed to assess the joint effect of in utero exposure to arsenic, manganese, and lead on children's neurodevelopment.
METHODS: We employed a novel statistical approach, Bayesian kernel machine regression (BKMR), to study the joint effect of coexposure to arsenic, manganese, and lead on neurodevelopment using an adapted Bayley Scale of Infant and Toddler Development™. Third Edition, in 825 mother-child pairs recruited into a prospective birth cohort from two clinics in the Pabna and Sirajdikhan districts of Bangladesh. Metals were measured in cord blood using inductively coupled plasma-mass spectrometry.
RESULTS: Analyses were stratified by clinic due to differences in exposure profiles. In the Pabna district, which displayed high manganese levels [interquartile range (IQR): 4.8, ], we found a statistically significant negative effect of the mixture of arsenic, lead, and manganese on cognitive score when cord blood metals concentrations were all above the 60th percentile (, , ) compared to the median (, , ). Evidence of a nonlinear effect of manganese was found. A change in log manganese from the 25th to the 75th percentile when arsenic and manganese were at the median was associated with a decrease in cognitive score of (, ) standard deviations. Our study suggests that arsenic might be a potentiator of manganese toxicity.
CONCLUSIONS: Employing a novel statistical method for the study of the health effects of chemical mixtures, we found evidence of neurotoxicity of the mixture, as well as potential synergism between arsenic and manganese. https://doi.org/10.1289/EHP614.},
	language = {eng},
	number = {6},
	journal = {Environmental Health Perspectives},
	author = {Valeri, Linda and Mazumdar, Maitreyi M. and Bobb, Jennifer F. and Claus Henn, Birgit and Rodrigues, Ema and Sharif, Omar I. A. and Kile, Molly L. and Quamruzzaman, Quazi and Afroz, Sakila and Golam, Mostafa and Amarasiriwardena, Citra and Bellinger, David C. and Christiani, David C. and Coull, Brent A. and Wright, Robert O.},
	month = jun,
	year = {2017},
	pmid = {28669934},
	keywords = {Humans, Female, Pregnancy, Male, Bayes Theorem, Environmental Exposure, Bangladesh, Child, Preschool, Environmental Pollutants, Infant, Metals, Prenatal Exposure Delayed Effects, Maternal Exposure},
	pages = {067015}
}

@inproceedings{turhan_generating_2017,
	title = {Generating word images using deep generative adversarial networks},
	doi = {10.1109/SIU.2017.7960464},
	abstract = {As one of the most important research topic of nowadays, deep learning attracts researchers' attention with applications of convolutional (CNNs) and recurrent neural networks (RNNs). By pioneers of the deep learning community, generative adversarial training, which has been working for especially last two years, is defined as the most exciting topic of computer vision for the last 10 years. With the influence of these views, a new training approach is proposed to combine generative adversarial network (GAN) architecture with a cascading training. Using CVL database, text images can be generated in a short training time as a different application from the existing GAN examples.},
	booktitle = {2017 25th {Signal} {Processing} and {Communications} {Applications} {Conference} ({SIU})},
	author = {Turhan, C. G. and Bilge, H. Ş},
	month = may,
	year = {2017},
	keywords = {learning (artificial intelligence), Training, Machine learning, adversarial training, cascading training, CNN, convolution, Convolutional codes, convolutional neural networks, Databases, deep generative adversarial networks, deep learning, document image processing, Gallium nitride, GAN architecture, generative adversarial network, generative model, Information processing, recurrent neural nets, recurrent neural networks, Recurrent neural networks, RNN, text image generator, word images generation, word processing},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/SCP98WI9/7960464.html:text/html}
}

@article{liu_gram-ctc:_2017,
	title = {Gram-{CTC}: {Automatic} {Unit} {Selection} and {Target} {Decomposition} for {Sequence} {Labelling}},
	shorttitle = {Gram-{CTC}},
	url = {http://arxiv.org/abs/1703.00096},
	abstract = {Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.},
	urldate = {2017-10-17},
	journal = {arXiv:1703.00096 [cs]},
	author = {Liu, Hairong and Zhu, Zhenyao and Li, Xiangang and Satheesh, Sanjeev},
	month = feb,
	year = {2017},
	note = {arXiv: 1703.00096},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published at ICML 2017},
	file = {arXiv\:1703.00096 PDF:/home/jeremiah/Zotero/storage/A672FRP4/Liu et al. - 2017 - Gram-CTC Automatic Unit Selection and Target Deco.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/FZGHUEVC/1703.html:text/html}
}

@article{campbell_automated_2017,
	title = {Automated {Scalable} {Bayesian} {Inference} via {Hilbert} {Coresets}},
	url = {http://arxiv.org/abs/1710.05053},
	abstract = {The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern datasets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the dataset itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms---one based on importance sampling, and one based on the Frank-Wolfe algorithm---along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic datasets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.},
	urldate = {2017-10-17},
	journal = {arXiv:1710.05053 [cs, stat]},
	author = {Campbell, Trevor and Broderick, Tamara},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05053},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation},
	file = {arXiv\:1710.05053 PDF:/home/jeremiah/Zotero/storage/D7Z3J5NN/Campbell and Broderick - 2017 - Automated Scalable Bayesian Inference via Hilbert .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/95RNNPRB/1710.html:text/html}
}

@article{ghosh_model_2017,
	title = {Model {Selection} in {Bayesian} {Neural} {Networks} via {Horseshoe} {Priors}},
	url = {http://arxiv.org/abs/1705.10388},
	abstract = {Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties. However, model selection---even choosing the number of nodes---remains an open question. In this work, we apply a horseshoe prior over node pre-activations of a Bayesian neural network, which effectively turns off nodes that do not help explain the data. We demonstrate that our prior prevents the BNN from under-fitting even when the number of nodes required is grossly over-estimated. Moreover, this model selection over the number of nodes doesn't come at the expense of predictive or computational performance; in fact, we learn smaller networks with comparable predictive performance to current approaches.},
	urldate = {2017-10-16},
	journal = {arXiv:1705.10388 [stat]},
	author = {Ghosh, Soumya and Doshi-Velez, Finale},
	month = may,
	year = {2017},
	note = {arXiv: 1705.10388},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1705.10388 PDF:/home/jeremiah/Zotero/storage/F9NJM766/Ghosh and Doshi-Velez - 2017 - Model Selection in Bayesian Neural Networks via Ho.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/JEXE5HTD/1705.html:text/html}
}

@article{lin_validation_2017-1,
	title = {Validation of a {Dish}-{Based} {Semiquantitative} {Food} {Questionnaire} in {Rural} {Bangladesh}},
	volume = {9},
	issn = {2072-6643},
	doi = {10.3390/nu9010049},
	abstract = {A locally validated tool was needed to evaluate long-term dietary intake in rural Bangladesh. We assessed the validity of a 42-item dish-based semi-quantitative food frequency questionnaire (FFQ) using two 3-day food diaries (FDs). We selected a random subset of 47 families (190 participants) from a longitudinal arsenic biomonitoring study in Bangladesh to administer the FFQ. Two 3-day FDs were completed by the female head of the households and we used an adult male equivalent method to estimate the FD for the other participants. Food and nutrient intakes measured by FFQ and FD were compared using Pearson's and Spearman's correlation, paired t-test, percent difference, cross-classification, weighted Kappa, and Bland-Altman analysis. Results showed good validity for total energy intake (paired t-test, p {\textless} 0.05; percent difference {\textless}10\%), with no presence of proportional bias (Bland-Altman correlation, p {\textgreater} 0.05). After energy-adjustment and de-attenuation for within-person variation, macronutrient intakes had excellent correlations ranging from 0.55 to 0.70. Validity for micronutrients was mixed. High intraclass correlation coefficients (ICCs) were found for most nutrients between the two seasons, except vitamin A. This dish-based FFQ provided adequate validity to assess and rank long-term dietary intake in rural Bangladesh for most food groups and nutrients, and should be useful for studying dietary-disease relationships.},
	language = {eng},
	number = {1},
	journal = {Nutrients},
	author = {Lin, Pi-I. D. and Bromage, Sabri and Mostofa, Md Golam and Allen, Joseph and Oken, Emily and Kile, Molly L. and Christiani, David C.},
	month = jan,
	year = {2017},
	pmid = {28075369},
	pmcid = {PMC5295093},
	keywords = {Humans, Female, Male, Body Mass Index, Reproducibility of Results, Child, Longitudinal Studies, Models, Theoretical, Adult, Middle Aged, Adolescent, Young Adult, Bangladesh, adult male equivalent, Diet Records, duplicate food sample, Energy Intake, food diary, food frequency questionnaire, Micronutrients, Nutrition Assessment, Rural Population, Seasons, Socioeconomic Factors, Surveys and Questionnaires, validation study, Vitamin A}
}

@article{lin_associations_2017,
	title = {Associations between {Diet} and {Toenail} {Arsenic} {Concentration} among {Pregnant} {Women} in {Bangladesh}: {A} {Prospective} {Study}},
	volume = {9},
	issn = {2072-6643},
	shorttitle = {Associations between {Diet} and {Toenail} {Arsenic} {Concentration} among {Pregnant} {Women} in {Bangladesh}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5409759/},
	doi = {10.3390/nu9040420},
	abstract = {This prospective study evaluated the relationship between long-term dietary habits and total arsenic (As) concentration in toenail clippings in a cohort of 1616 pregnant women in the Bangladeshi administrative regions of Sirajdikhan and Pabna Sadar. Diet was assessed at Gestation Week 28 and at Postpartum Month 1, using a locally-validated dish-based semi-quantitative food-frequency questionnaire. Toenail As concentration was analyzed by microwave-assisted acid digestion and inductively coupled plasma mass spectrometry. Associations between natural log-transformed consumption of individual food items and temporally matched natural log-transformed toenail As concentration were quantified using general linear models that accounted for As concentration in the primary drinking water source and other potential confounders. The analysis was stratified by As in drinking water (≤50 μg/L versus {\textgreater}50 μg/L) and the time of dietary assessment (Gestation Week 28 versus Postpartum Week 1). Interestingly, toenail As was not significantly associated with consumption of plain rice as hypothesized. However, toenail As was positively associated with consumption of several vegetable, fish and meat items and was negatively associated with consumption of rice, cereal, fruits, and milk based food items. Further studies in pregnant women are needed to compare As metabolism at different levels of As exposure and the interaction between dietary composition and As absorption.},
	number = {4},
	urldate = {2017-10-16},
	journal = {Nutrients},
	author = {Lin, Pi-I. D. and Bromage, Sabri and Mostofa, Md. Golam and Allen, Joseph and Oken, Emily and Kile, Molly L. and Christiani, David C.},
	month = apr,
	year = {2017},
	pmid = {28441747},
	pmcid = {PMC5409759},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/2VF9V4PK/Lin et al. - 2017 - Associations between Diet and Toenail Arsenic Conc.pdf:application/pdf}
}

@article{vehtari_bayesian_2016,
	title = {Bayesian {Leave}-{One}-{Out} {Cross}-{Validation} {Approximations} for {Gaussian} {Latent} {Variable} {Models}},
	volume = {17},
	url = {http://jmlr.org/papers/v17/14-540.html},
	number = {103},
	journal = {Journal of Machine Learning Research},
	author = {Vehtari, Aki and Mononen, Tommi and Tolvanen, Ville and Sivula, Tuomas and Winther, Ole},
	year = {2016},
	pages = {1--38},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/YKTW8ZNS/Vehtari et al. - 2016 - Bayesian Leave-One-Out Cross-Validation Approximat.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/DADQ5NUR/14-540.html:text/html}
}

@article{yang_learning_2017,
	title = {Learning to {Extract} {Semantic} {Structure} from {Documents} {Using} {Multimodal} {Fully} {Convolutional} {Neural} {Network}},
	url = {http://arxiv.org/abs/1706.02337},
	abstract = {We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance.},
	urldate = {2017-10-12},
	journal = {arXiv:1706.02337 [cs]},
	author = {Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Giles, C. Lee},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02337},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: CVPR 2017 Spotlight},
	file = {arXiv\:1706.02337 PDF:/home/jeremiah/Zotero/storage/EB3K2S6H/Yang et al. - 2017 - Learning to Extract Semantic Structure from Docume.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/456D3R8H/1706.html:text/html}
}

@article{lin_test_2016-1,
	title = {Test for rare variants by environment interactions in sequencing association studies},
	volume = {72},
	issn = {1541-0420},
	doi = {10.1111/biom.12368},
	abstract = {We consider in this article testing rare variants by environment interactions in sequencing association studies. Current methods for studying the association of rare variants with traits cannot be readily applied for testing for rare variants by environment interactions, as these methods do not effectively control for the main effects of rare variants, leading to unstable results and/or inflated Type 1 error rates. We will first analytically study the bias of the use of conventional burden-based tests for rare variants by environment interactions, and show the tests can often be invalid and result in inflated Type 1 error rates. To overcome these difficulties, we develop the interaction sequence kernel association test (iSKAT) for assessing rare variants by environment interactions. The proposed test iSKAT is optimal in a class of variance component tests and is powerful and robust to the proportion of variants in a gene that interact with environment and the signs of the effects. This test properly controls for the main effects of the rare variants using weighted ridge regression while adjusting for covariates. We demonstrate the performance of iSKAT using simulation studies and illustrate its application by analysis of a candidate gene sequencing study of plasma adiponectin levels.},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Lin, Xinyi and Lee, Seunggeun and Wu, Michael C. and Wang, Chaolong and Chen, Han and Li, Zilin and Lin, Xihong},
	month = mar,
	year = {2016},
	pmid = {26229047},
	pmcid = {PMC4733434},
	keywords = {High-Throughput Nucleotide Sequencing, Humans, Polymorphism, Single Nucleotide, Genetic Association Studies, Genetic Predisposition to Disease, Genetic Variation, Sensitivity and Specificity, Reproducibility of Results, Data Interpretation, Statistical, Adiponectin, Alcoholism, Bias analysis, Gene-environment interactions, Rare Diseases, Sequencing association studies},
	pages = {156--164}
}

@article{wang_boosting_2016,
	title = {Boosting the {Power} of the {Sequence} {Kernel} {Association} {Test} by {Properly} {Estimating} {Its} {Null} {Distribution}},
	volume = {99},
	issn = {0002-9297},
	url = {http://www.sciencedirect.com/science/article/pii/S000292971630146X},
	doi = {10.1016/j.ajhg.2016.05.011},
	abstract = {The sequence kernel association test (SKAT) is probably the most popular statistical test used in rare-variant association studies. Its null distribution involves unknown parameters that need to be estimated. The current estimation method has a valid type I error rate, but the power is compromised given that all subjects are used for estimation. I have developed an estimation method that uses only control subjects. Named SKAT+, this method uses the same test statistic as SKAT but differs in the way the null distribution is estimated. Extensive simulation studies and applications to data from the Genetic Analysis Workshop 17 and the Ocular Hypertension Treatment Study demonstrated that SKAT+ has superior power over SKAT while maintaining control over the type I error rate. This method is applicable to extensions of SKAT in the literature.},
	number = {1},
	urldate = {2017-10-12},
	journal = {The American Journal of Human Genetics},
	author = {Wang, Kai},
	month = jul,
	year = {2016},
	keywords = {power, association test, rare variant, sequence data, SKAT},
	pages = {104--114},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/JAGTTA8E/Wang - 2016 - Boosting the Power of the Sequence Kernel Associat.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/WXB6Q4T4/S000292971630146X.html:text/html}
}

@article{zhang_which_2017,
	title = {Which {Encoding} is the {Best} for {Text} {Classification} in {Chinese}, {English}, {Japanese} and {Korean}?},
	url = {http://arxiv.org/abs/1708.02657},
	abstract = {This article offers an empirical study on the different ways of encoding Chinese, Japanese, Korean (CJK) and English languages for text classification. Different encoding levels are studied, including UTF-8 bytes, characters, words, romanized characters and romanized words. For all encoding levels, whenever applicable, we provide comparisons with linear models, fastText and convolutional networks. For convolutional networks, we compare between encoding mechanisms using character glyph images, one-hot (or one-of-n) encoding, and embedding. In total there are 473 models, using 14 large-scale text classification datasets in 4 languages including Chinese, English, Japanese and Korean. Some conclusions from these results include that byte-level one-hot encoding based on UTF-8 consistently produces competitive results for convolutional networks, that word-level n-grams linear models are competitive even without perfect word segmentation, and that fastText provides the best result using character-level n-gram encoding but can overfit when the features are overly rich.},
	urldate = {2017-10-04},
	journal = {arXiv:1708.02657 [cs]},
	author = {Zhang, Xiang and LeCun, Yann},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.02657},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv\:1708.02657 PDF:/home/jeremiah/Zotero/storage/T32DVVLS/Zhang and LeCun - 2017 - Which Encoding is the Best for Text Classification.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/9R9K7LAU/1708.html:text/html}
}

@phdthesis{bruce_mathematical_2014,
	type = {Thesis},
	title = {Mathematical {Expression} {Detection} and {Segmentation} in {Document} {Images}},
	copyright = {This Item is protected by copyright and/or related rights. Some uses of this Item may be deemed fair and permitted by law even without permission from the rights holder(s), or the rights holder(s) may have licensed the work for use under certain conditions. For other uses you need to obtain permission from the rights holder(s).},
	url = {https://vtechworks.lib.vt.edu/handle/10919/46724},
	abstract = {Various document layout analysis techniques are employed in order to enhance the accuracy of optical character recognition (OCR) in document images. Type-specific document layout analysis involves localizing and segmenting specific zones in an image so that they may be recognized by specialized OCR modules. Zones of interest include titles, headers/footers, paragraphs, images, mathematical expressions, chemical equations, musical notations, tables, circuit diagrams, among others. False positive/negative detections, oversegmentations, and undersegmentations made during the detection and segmentation stage will confuse a specialized OCR system and thus may result in garbled, incoherent output. In this work a mathematical expression detection and segmentation (MEDS) module is implemented and then thoroughly evaluated. The module is fully integrated with the open source OCR software, Tesseract, and is designed to function as a component of it. Evaluation is carried out on freely available public domain images so that future and existing techniques may be objectively compared.},
	school = {Virginia Tech},
	author = {Bruce, Jacob Robert},
	month = mar,
	year = {2014},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/P3IPKDA6/Bruce - 2014 - Mathematical Expression Detection and Segmentation.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/5TGEKLLQ/46724.html:text/html}
}

@misc{noauthor_creating_nodate,
	title = {Creating a {Modern} {OCR} {Pipeline} {Using} {Computer} {Vision} and {Deep} {Learning} {\textbar} {Dropbox} {Tech} {Blog}},
	url = {https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/},
	urldate = {2017-10-04},
	file = {Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning | Dropbox Tech Blog:/home/jeremiah/Zotero/storage/8CQ6VGV2/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning.html:text/html}
}

@inproceedings{liu_simple_2013,
	title = {A {Simple} {Equation} {Region} {Detector} for {Printed} {Document} {Images} in {Tesseract}},
	doi = {10.1109/ICDAR.2013.56},
	abstract = {Detecting equation regions from scanned books has received attention in the document image research community in the past few years. Compared with regular text blocks, equation regions have more complicated layouts so we can not simply use text lines to model them. On the other hand, these regions consist of text symbols that can be reflowed, so that the OCR engines should parse them instead of rasterizing them like image regions. In this paper, we present an equation detector with two major contributions: (i) it is built on a simple algorithm that uses the density of special symbols, such that no additional classifier is required, (ii) it has been built into the open source Tesseract that can be accessed and used by the OCR community. The algorithm is tested on the Google Books database with 1534 entries sampled from books/magazines/newspapers of over thirty languages. And we show that Tesseract performance is improved after enabling the detector.},
	booktitle = {2013 12th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Liu, Z. and Smith, R.},
	month = aug,
	year = {2013},
	keywords = {document image processing, detecting equation regions, Detectors, document image research community, equation detector, equation region detection, equation regions, Equations, Google, Google books database, image regions, Layout, layout analysis, Mathematical model, OCR community, OCR engines, optical character recognition, Optical character recognition software, printed document images, scanned books, simple equation region detector, Tesseract, Text analysis, text blocks, text symbols},
	pages = {245--249},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/W2N85GA8/6628621.html:text/html}
}

@inproceedings{breuel_high-performance_2013,
	title = {High-{Performance} {OCR} for {Printed} {English} and {Fraktur} {Using} {LSTM} {Networks}},
	doi = {10.1109/ICDAR.2013.140},
	abstract = {Long Short-Term Memory (LSTM) networks have yielded excellent results on handwriting recognition. This paper describes an application of bidirectional LSTM networks to the problem of machine-printed Latin and Fraktur recognition. Latin and Fraktur recognition differs significantly from handwriting recognition in both the statistical properties of the data, as well as in the required, much higher levels of accuracy. Applications of LSTM networks to handwriting recognition use two-dimensional recurrent networks, since the exact position and baseline of handwritten characters is variable. In contrast, for printed OCR, we used a one-dimensional recurrent network combined with a novel algorithm for baseline and x-height normalization. A number of databases were used for training and testing, including the UW3 database, artificially generated and degraded Fraktur text and scanned pages from a book digitization project. The LSTM architecture achieved 0.6\% character-level test-set error on English text. When the artificially degraded Fraktur data set is divided into training and test sets, the system achieves an error rate of 1.64\%. On specific books printed in Fraktur (not part of the training set), the system achieves error rates of 0.15\% (Fontane) and 1.47\% (Ersch-Gruber). These recognition accuracies were found without using any language modelling or any other post-processing techniques.},
	booktitle = {2013 12th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Breuel, T. M. and Ul-Hasan, A. and Al-Azawi, M. A. and Shafait, F.},
	month = aug,
	year = {2013},
	keywords = {Training, Recurrent neural networks, RNN, optical character recognition, Optical character recognition software, book digitization project, English text, Error analysis, Fraktur, Fraktur text, handwriting recognition, Handwriting recognition, handwritten characters, Hidden Markov models, high-performance OCR, long short term memory networks, LSTM networks, LSTM Networks, machine printed Fraktur recognition, machine printed Latin recognition, natural language processing, OCR, printed English, printed OCR, recurrent networks, scanned pages, statistical analysis, statistical properties, text analysis, UW3 database},
	pages = {683--687},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/6RNUCQUY/6628705.html:text/html;IEEE Xplore Full Text PDF:/home/jeremiah/Zotero/storage/MCXV6T95/Breuel et al. - 2013 - High-Performance OCR for Printed English and Frakt.pdf:application/pdf}
}

@article{graves_novel_2009,
	title = {A {Novel} {Connectionist} {System} for {Unconstrained} {Handwriting} {Recognition}},
	volume = {31},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2008.137},
	abstract = {Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Graves, A. and Liwicki, M. and Fernández, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},
	month = may,
	year = {2009},
	keywords = {Robustness, Databases, recurrent neural nets, recurrent neural networks, Recurrent neural networks, handwriting recognition, Handwriting recognition, Hidden Markov models, bidirectional long short-term memory, Character recognition, connectionist system, connectionist temporal classification, Connectionist temporal classification, handwritten character recognition, hidden Markov model., hidden Markov models, image segmentation, Labeling, language modeling, Long Short-Term Memory, offline handwriting, Offline handwriting recognition, online handwriting, Online handwriting recognition, overlapping character segmentation, recurrent neural network, Size measurement, Speech, Text recognition, unconstrained handwriting databases, Unconstrained handwriting recognition, unconstrained handwriting text recognition},
	pages = {855--868},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/IB5ECFX7/4531750.html:text/html}
}

@article{sahu_sequence_2015,
	title = {Sequence to {Sequence} {Learning} for {Optical} {Character} {Recognition}},
	url = {http://arxiv.org/abs/1511.04176},
	abstract = {We propose an end-to-end recurrent encoder-decoder based sequence learning approach for printed text Optical Character Recognition (OCR). In contrast to present day existing state-of-art OCR solution which uses connectionist temporal classification (CTC) output layer, our approach makes minimalistic assumptions on the structure and length of the sequence. We use a two step encoder-decoder approach -- (a) A recurrent encoder reads a variable length printed text word image and encodes it to a fixed dimensional embedding. (b) This fixed dimensional embedding is subsequently comprehended by decoder structure which converts it into a variable length text output. Our architecture gives competitive performance relative to connectionist temporal classification (CTC) output layer while being executed in more natural settings. The learnt deep word image embedding from encoder can be used for printed text based retrieval systems. The expressive fixed dimensional embedding for any variable length input expedites the task of retrieval and makes it more efficient which is not possible with other recurrent neural network architectures. We empirically investigate the expressiveness and the learnability of long short term memory (LSTMs) in the sequence to sequence learning regime by training our network for prediction tasks in segmentation free printed text OCR. The utility of the proposed architecture for printed text is demonstrated by quantitative and qualitative evaluation of two tasks -- word prediction and retrieval.},
	urldate = {2017-10-04},
	journal = {arXiv:1511.04176 [cs]},
	author = {Sahu, Devendra Kumar and Sukhwani, Mohak},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04176},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 9 pages (including reference), 6 figures (including subfigures), 5 tables},
	file = {arXiv\:1511.04176 PDF:/home/jeremiah/Zotero/storage/E6YNQQKL/Sahu and Sukhwani - 2015 - Sequence to Sequence Learning for Optical Characte.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/TYFSTGRI/1511.html:text/html}
}

@inproceedings{graves_connectionist_2006,
	title = {Connectionist temporal classification: {Labelling} unsegmented sequence data with recurrent neural networks},
	shorttitle = {Connectionist temporal classification},
	abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN. 1.},
	booktitle = {In {Proceedings} of the {International} {Conference} on {Machine} {Learning}, {ICML} 2006},
	author = {Graves, Alex and Fernández, Santiago and Gomez, Faustino},
	year = {2006},
	pages = {369--376},
	file = {Citeseer - Full Text PDF:/home/jeremiah/Zotero/storage/W3FEF8XC/Graves et al. - 2006 - Connectionist temporal classification Labelling u.pdf:application/pdf;Citeseer - Snapshot:/home/jeremiah/Zotero/storage/NDMDQ9BG/summary.html:text/html}
}

@article{ul-hasan_generic_2016,
	title = {Generic {Text} {Recognition} using {Long} {Short}-{Term} {Memory} {Networks}},
	copyright = {Standard gemäß KLUEDO-Leitlinien vom 30.07.2015},
	url = {https://kluedo.ub.uni-kl.de/frontdoor/index/index/docId/4353},
	abstract = {The task of printed Optical Character Recognition (OCR), though considered  ``solved'' by many, still poses several challenges. The complex grapheme structure of many scripts, such as Devanagari and Urdu Nastaleeq, greatly lowers the performance of state-of-the-art OCR systems. 
Moreover, the digitization of historical and multilingual documents still require much probing. Lack of benchmark datasets further complicates the development of reliable OCR systems. This thesis aims to find the answers to some of these challenges using contemporary machine learning technologies. Specifically, the Long Short-Term Memory (LSTM) networks, have been employed to OCR modern as well historical monolingual documents. The excellent OCR results obtained on these have led us to extend their application for multilingual documents. 

The first major contribution of this thesis is to demonstrate the usability of LSTM networks for monolingual documents. The LSTM networks yield very good OCR results on various modern and historical scripts, without using sophisticated features and post-processing techniques. The set of modern scripts include modern English, Urdu Nastaleeq and Devanagari. To address the challenge of OCR of historical documents, this thesis focuses on Old German Fraktur script, medieval Latin script of the 15th century, and Polytonic Greek  script. LSTM-based systems outperform the contemporary OCR systems on all of these scripts. To cater for the lack of ground-truth data, this thesis proposes a new methodology, combining segmentation-based and segmentation-free OCR approaches, to OCR scripts for which no transcribed  training data is available.

Another major contribution of this thesis is the development of a novel multilingual OCR system. A unified framework for dealing with different types of multilingual documents has been proposed. The core motivation behind this generalized framework is the human reading ability to process multilingual documents, where no script identification takes place.
In this design, the LSTM networks recognize multiple scripts simultaneously without the need to identify different scripts. The first step in building this framework is the realization of a language-independent OCR system which recognizes multilingual text in a single step. This language-independent approach is then extended to script-independent OCR that can recognize multiscript documents using a single OCR model. The proposed generalized approach yields low error rate (1.2\%) on a test corpus of English-Greek bilingual documents. 


In summary, this thesis aims to extend the research in document recognition, from modern Latin scripts to Old Latin, to Greek and to other ``under-privilaged'' scripts such as Devanagari and Urdu Nastaleeq. 
It also attempts to add a different perspective in dealing with multilingual documents.},
	author = {Ul-Hasan, Adnan},
	year = {2016},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/J2A7TW37/Ul-Hasan - 2016 - Generic Text Recognition using Long Short-Term Mem.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/5KRYFX3T/4353.html:text/html}
}

@article{kandasamy_high_2015,
	title = {High {Dimensional} {Bayesian} {Optimisation} and {Bandits} via {Additive} {Models}},
	url = {http://arxiv.org/abs/1503.01673},
	abstract = {Bayesian Optimisation (BO) is a technique used in optimising a \$D\$-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on \$D\$ even though the function depends on all \$D\$ dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.},
	urldate = {2017-10-03},
	journal = {arXiv:1503.01673 [cs, stat]},
	author = {Kandasamy, Kirthevasan and Schneider, Jeff and Poczos, Barnabas},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.01673},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: Proceedings of The 32nd International Conference on Machine Learning 2015},
	file = {arXiv\:1503.01673 PDF:/home/jeremiah/Zotero/storage/7EV769LY/Kandasamy et al. - 2015 - High Dimensional Bayesian Optimisation and Bandits.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/H9BATMHX/1503.html:text/html}
}

@article{wang_ensemble_2017,
	title = {Ensemble {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1706.01445},
	abstract = {Bayesian Optimization (BO) has been shown to be a very effective paradigm for tackling hard black-box and non-convex optimization problems encountered in Machine Learning. Despite these successes, the computational complexity of the underlying function approximation has restricted the use of BO to problems that can be handled with less than a few thousand function evaluations. Harder problems like those involving functions operating in very high dimensional spaces may require hundreds of thousands or millions of evaluations or more and become computationally intractable to handle using standard Bayesian Optimization methods. In this paper, we propose Ensemble Bayesian Optimization (EBO) to overcome this problem. Unlike conventional BO methods that operate on a single posterior GP model, EBO works with an ensemble of posterior GP models. Further, we represent each GP model using tile coding random features and an additive function structure. Our approach generates speedups by parallelizing the time consuming hyper-parameter posterior inference and functional evaluations on hundreds of cores and aggregating the models in every iteration of BO. Our extensive experimental evaluation shows that EBO can speed up the posterior inference between 2-3 orders of magnitude (400 times in one experiment) compared to the state-of-the-art by putting data into Mondrian bins without sacrificing the sample quality. We demonstrate the ability of EBO to handle sample-intensive hard optimization problems by applying it to a rover navigation problem with tens of thousands of observations.},
	urldate = {2017-10-03},
	journal = {arXiv:1706.01445 [stat]},
	author = {Wang, Zi and Gehring, Clement and Kohli, Pushmeet and Jegelka, Stefanie},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.01445},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1706.01445 PDF:/home/jeremiah/Zotero/storage/GCQ2NJLN/Wang et al. - 2017 - Ensemble Bayesian Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/I3ME544B/1706.html:text/html}
}

@article{hernandez-lobato_probabilistic_2015-1,
	title = {Probabilistic {Backpropagation} for {Scalable} {Learning} of {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.05336},
	abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
	urldate = {2017-09-26},
	journal = {arXiv:1502.05336 [stat]},
	author = {Hernández-Lobato, José Miguel and Adams, Ryan P.},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05336},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1502.05336 PDF:/home/jeremiah/Zotero/storage/RBYVE7QU/Hernández-Lobato and Adams - 2015 - Probabilistic Backpropagation for Scalable Learnin.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/JS7ZW8AP/1502.html:text/html}
}

@inproceedings{li_learning_2016-1,
	title = {Learning to {Generate} with {Memory}},
	url = {http://proceedings.mlr.press/v48/lie16.html},
	abstract = {Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generat...},
	language = {en},
	booktitle = {{PMLR}},
	author = {Li, Chongxuan and Zhu, Jun and Zhang, Bo},
	month = jun,
	year = {2016},
	pages = {1177--1186},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/6BP23IMD/Li et al. - 2016 - Learning to Generate with Memory.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/NL4BBYBP/lie16.html:text/html}
}

@inproceedings{feng_learning_2015,
	title = {Learning {The} {Structure} of {Deep} {Convolutional} {Networks}},
	url = {https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Feng_Learning_The_Structure_ICCV_2015_paper.html},
	author = {Feng, Jiashi and Darrell, Trevor},
	year = {2015},
	pages = {2749--2757},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/DA8BINRP/Feng and Darrell - 2015 - Learning The Structure of Deep Convolutional Netwo.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/A2L3DNT9/Feng_Learning_The_Structure_ICCV_2015_paper.html:text/html}
}

@inproceedings{adams_learning_2010,
	title = {Learning the {Structure} of {Deep} {Sparse} {Graphical} {Models}},
	url = {http://proceedings.mlr.press/v9/adams10a.html},
	abstract = {Deep belief networks are a powerful way to model complex probability   distributions.  However, it is difficult to learn the structure of a   belief network, particularly one with hidden units.  Th...},
	language = {en},
	booktitle = {{PMLR}},
	author = {Adams, Ryan and Wallach, Hanna and Ghahramani, Zoubin},
	month = mar,
	year = {2010},
	pages = {1--8},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/QVHCQFI3/Adams et al. - 2010 - Learning the Structure of Deep Sparse Graphical Mo.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/EUWIVPEA/adams10a.html:text/html}
}

@inproceedings{adams_learning_2010-1,
	title = {Learning the {Structure} of {Deep} {Sparse} {Graphical} {Models}},
	url = {http://proceedings.mlr.press/v9/adams10a.html},
	abstract = {Deep belief networks are a powerful way to model complex probability   distributions.  However, it is difficult to learn the structure of a   belief network, particularly one with hidden units.  Th...},
	language = {en},
	booktitle = {{PMLR}},
	author = {Adams, Ryan and Wallach, Hanna and Ghahramani, Zoubin},
	month = mar,
	year = {2010},
	pages = {1--8},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/EJL5IXDY/Adams et al. - 2010 - Learning the Structure of Deep Sparse Graphical Mo.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/LJF9YJIM/adams10a.html:text/html}
}

@article{gal_deep_2017,
	title = {Deep {Bayesian} {Active} {Learning} with {Image} {Data}},
	url = {http://arxiv.org/abs/1703.02910},
	abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
	urldate = {2017-09-19},
	journal = {arXiv:1703.02910 [cs, stat]},
	author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.02910},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	file = {arXiv\:1703.02910 PDF:/home/jeremiah/Zotero/storage/76FQ5SWW/Gal et al. - 2017 - Deep Bayesian Active Learning with Image Data.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/PTK32VTL/1703.html:text/html}
}

@article{snoek_scalable_2015,
	title = {Scalable {Bayesian} {Optimization} {Using} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.05700},
	abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
	urldate = {2017-09-19},
	journal = {arXiv:1502.05700 [stat]},
	author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md Mostofa Ali and Prabhat and Adams, Ryan P.},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05700},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1502.05700 PDF:/home/jeremiah/Zotero/storage/3RAUQIM7/Snoek et al. - 2015 - Scalable Bayesian Optimization Using Deep Neural N.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/XIG3IZPD/1502.html:text/html}
}

@inproceedings{krogh_neural_1994,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'94},
	title = {Neural {Network} {Ensembles}, {Cross} {Validation} and {Active} {Learning}},
	url = {http://dl.acm.org/citation.cfm?id=2998687.2998716},
	abstract = {Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme.},
	urldate = {2017-09-19},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Krogh, Anders and Vedelsby, Jesper},
	year = {1994},
	pages = {231--238}
}

@article{dumoulin_guide_2016,
	title = {A guide to convolution arithmetic for deep learning},
	url = {http://arxiv.org/abs/1603.07285},
	abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
	journal = {arXiv:1603.07285 [cs, stat]},
	author = {Dumoulin, Vincent and Visin, Francesco},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.07285},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1603.07285 PDF:/home/jeremiah/Zotero/storage/HAH87SDV/Dumoulin and Visin - 2016 - A guide to convolution arithmetic for deep learnin.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/AVF2VV5K/1603.html:text/html}
}

@article{zhang_text_2015,
	title = {Text {Understanding} from {Scratch}},
	url = {http://arxiv.org/abs/1502.01710},
	abstract = {This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.},
	journal = {arXiv:1502.01710 [cs]},
	author = {Zhang, Xiang and LeCun, Yann},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.01710},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	annote = {Comment: This technical report is superseded by a paper entitled "Character-level Convolutional Networks for Text Classification", arXiv:1509.01626. It has considerably more experimental results and a rewritten introduction},
	file = {arXiv\:1502.01710 PDF:/home/jeremiah/Zotero/storage/Z8BTWBPK/Zhang and LeCun - 2015 - Text Understanding from Scratch.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/PBDKKSNV/1502.html:text/html}
}

@article{wang_scene_2017,
	title = {Scene {Flow} to {Action} {Map}: {A} {New} {Representation} for {RGB}-{D} based {Action} {Recognition} with {Convolutional} {Neural} {Networks}},
	shorttitle = {Scene {Flow} to {Action} {Map}},
	url = {http://arxiv.org/abs/1702.08652},
	abstract = {Scene flow describes the motion of 3D objects in real world and potentially could be the basis of a good feature for 3D action recognition. However, its use for action recognition, especially in the context of convolutional neural networks (ConvNets), has not been previously studied. In this paper, we propose the extraction and use of scene flow for action recognition from RGB-D data. Previous works have considered the depth and RGB modalities as separate channels and extract features for later fusion. We take a different approach and consider the modalities as one entity, thus allowing feature extraction for action recognition at the beginning. Two key questions about the use of scene flow for action recognition are addressed: how to organize the scene flow vectors and how to represent the long term dynamics of videos based on scene flow. In order to calculate the scene flow correctly on the available datasets, we propose an effective self-calibration method to align the RGB and depth data spatially without knowledge of the camera parameters. Based on the scene flow vectors, we propose a new representation, namely, Scene Flow to Action Map (SFAM), that describes several long term spatio-temporal dynamics for action recognition. We adopt a channel transform kernel to transform the scene flow vectors to an optimal color space analogous to RGB. This transformation takes better advantage of the trained ConvNets models over ImageNet. Experimental results indicate that this new representation can surpass the performance of state-of-the-art methods on two large public datasets.},
	journal = {arXiv:1702.08652 [cs]},
	author = {Wang, Pichao and Li, Wanqing and Gao, Zhimin and Zhang, Yuyao and Tang, Chang and Ogunbona, Philip},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08652},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1702.08652 PDF:/home/jeremiah/Zotero/storage/SAD6HJ32/Wang et al. - 2017 - Scene Flow to Action Map A New Representation for.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/QMSEGSXC/1702.html:text/html}
}

@article{liu_skeleton_2017,
	title = {Skeleton {Based} {Human} {Action} {Recognition} with {Global} {Context}-{Aware} {Attention} {LSTM} {Networks}},
	url = {http://arxiv.org/abs/1707.05740},
	abstract = {Human action recognition in 3D skeleton sequences has attracted a lot of research attention. Recently, Long Short-Term Memory (LSTM) networks have shown promising performance in this task due to their strengths in modeling the dependencies and dynamics in sequential data. As not all skeletal joints are informative for action recognition, and the irrelevant joints often bring noise which can degrade the performance, we need to pay more attention to the informative ones. However, the original LSTM network does not have explicit attention ability. In this paper, we propose a new class of LSTM network, Global Context-Aware Attention LSTM (GCA-LSTM), for skeleton based action recognition. This network is capable of selectively focusing on the informative joints in each frame of each skeleton sequence by using a global context memory cell. To further improve the attention capability of our network, we also introduce a recurrent attention mechanism, with which the attention performance of the network can be enhanced progressively. Moreover, we propose a stepwise training scheme in order to train our network effectively. Our approach achieves state-of-the-art performance on five challenging benchmark datasets for skeleton based action recognition.},
	journal = {arXiv:1707.05740 [cs]},
	author = {Liu, Jun and Wang, Gang and Duan, Ling-Yu and Hu, Ping and Kot, Alex C.},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.05740},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1707.05740 PDF:/home/jeremiah/Zotero/storage/3XSR6P4M/Liu et al. - 2017 - Skeleton Based Human Action Recognition with Globa.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/PHP6DN53/1707.html:text/html}
}

@article{ritter_cognitive_2017,
	title = {Cognitive {Psychology} for {Deep} {Neural} {Networks}: {A} {Shape} {Bias} {Case} {Study}},
	shorttitle = {Cognitive {Psychology} for {Deep} {Neural} {Networks}},
	url = {https://128.84.21.199/abs/1706.08606},
	urldate = {2017-06-29},
	author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
	month = jun,
	year = {2017},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/E2NTH6SQ/Ritter et al. - 2017 - Cognitive Psychology for Deep Neural Networks A S.pdf:application/pdf}
}

@article{miller_explanation_2017,
	title = {Explanation in {Artificial} {Intelligence}: {Insights} from the {Social} {Sciences}},
	shorttitle = {Explanation in {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/1706.07269},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that, if these techniques are to succeed, the explanations they generate should have a structure that humans accept. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	journal = {arXiv:1706.07269 [cs]},
	author = {Miller, Tim},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.07269},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1706.07269 PDF:/home/jeremiah/Zotero/storage/66JJCG5V/Miller - 2017 - Explanation in Artificial Intelligence Insights f.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/VDDQ348B/1706.html:text/html}
}

@article{akanbi_framework_2017,
	title = {A {Framework} for {Accurate} {Drought} {Forecasting} {System} {Using} {Semantics}-{Based} {Data} {Integration} {Middleware}},
	url = {http://arxiv.org/abs/1706.07294},
	abstract = {Technological advancement in Wireless Sensor Networks (WSN) has made it become an invaluable component of a reliable environmental monitoring system; they form the digital skin' through which to 'sense' and collect the context of the surroundings and provides information on the process leading to complex events such as drought. However, these environmental properties are measured by various heterogeneous sensors of different modalities in distributed locations making up the WSN, using different abstruse terms and vocabulary in most cases to denote the same observed property, causing data heterogeneity. Adding semantics and understanding the relationships that exist between the observed properties, and augmenting it with local indigenous knowledge is necessary for an accurate drought forecasting system. In this paper, we propose the framework for the semantic representation of sensor data and integration with indigenous knowledge on drought using a middleware for an efficient drought forecasting system.},
	journal = {arXiv:1706.07294 [cs]},
	author = {Akanbi, A. K. and Masinde, M.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.07294},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases},
	annote = {Comment: 5 pages, 1 figure. arXiv admin note: substantial text overlap with arXiv:1601.01920},
	file = {arXiv\:1706.07294 PDF:/home/jeremiah/Zotero/storage/B2QISXAN/Akanbi and Masinde - 2017 - A Framework for Accurate Drought Forecasting Syste.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/X3EFWVCC/1706.html:text/html}
}

@article{ghosh_model_2017-1,
	title = {Model {Selection} in {Bayesian} {Neural} {Networks} via {Horseshoe} {Priors}},
	url = {http://arxiv.org/abs/1705.10388},
	abstract = {Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties. However, model selection---even choosing the number of nodes---remains an open question. In this work, we apply a horseshoe prior over node pre-activations of a Bayesian neural network, which effectively turns off nodes that do not help explain the data. We demonstrate that our prior prevents the BNN from under-fitting even when the number of nodes required is grossly over-estimated. Moreover, this model selection over the number of nodes doesn't come at the expense of predictive or computational performance; in fact, we learn smaller networks with comparable predictive performance to current approaches.},
	journal = {arXiv:1705.10388 [stat]},
	author = {Ghosh, Soumya and Doshi-Velez, Finale},
	month = may,
	year = {2017},
	note = {arXiv: 1705.10388},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1705.10388 PDF:/home/jeremiah/Zotero/storage/999H9FF2/Ghosh and Doshi-Velez - 2017 - Model Selection in Bayesian Neural Networks via Ho.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/GDW25X4Q/1705.html:text/html}
}

@article{duvenaud_structure_2013-1,
	title = {Structure {Discovery} in {Nonparametric} {Regression} through {Compositional} {Kernel} {Search}},
	url = {http://arxiv.org/abs/1302.4922},
	abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.},
	journal = {arXiv:1302.4922 [cs, stat]},
	author = {Duvenaud, David and Lloyd, James Robert and Grosse, Roger and Tenenbaum, Joshua B. and Ghahramani, Zoubin},
	month = feb,
	year = {2013},
	note = {arXiv: 1302.4922},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology, G.3, I.2.6},
	annote = {Comment: 9 pages, 7 figures, To appear in proceedings of the 2013 International Conference on Machine Learning},
	file = {arXiv\:1302.4922 PDF:/home/jeremiah/Zotero/storage/GCETVS7X/Duvenaud et al. - 2013 - Structure Discovery in Nonparametric Regression th.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/G2CXSDX2/1302.html:text/html}
}

@article{ge_optimization_2017,
	title = {On the {Optimization} {Landscape} of {Tensor} {Decompositions}},
	url = {http://arxiv.org/abs/1706.05598},
	abstract = {Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that "all local optima are (approximately) global optima", and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult. In this paper, we analyze the optimization landscape of the random over-complete tensor decomposition problem, which has many applications in unsupervised learning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant \${\textbackslash}epsilon {\textgreater} 0\$, among the set of points with function values \$(1+{\textbackslash}epsilon)\$-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem. Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients.},
	journal = {arXiv:1706.05598 [cs, math, stat]},
	author = {Ge, Rong and Ma, Tengyu},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.05598},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Mathematics - Probability, Mathematics - Optimization and Control, Computer Science - Data Structures and Algorithms},
	annote = {Comment: Best paper in the NIPS 2016 Workshop on Nonconvex Optimization for Machine Learning: Theory and Practice. In submission},
	file = {arXiv\:1706.05598 PDF:/home/jeremiah/Zotero/storage/RZC264GC/Ge and Ma - 2017 - On the Optimization Landscape of Tensor Decomposit.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/GQ349J6K/1706.html:text/html}
}

@article{nguyen_loss_2017,
	title = {The loss surface of deep and wide neural networks},
	url = {http://arxiv.org/abs/1704.08045},
	abstract = {While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.},
	journal = {arXiv:1704.08045 [cs, stat]},
	author = {Nguyen, Quynh and Hein, Matthias},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.08045},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: ICML 2017. Main results now hold for larger classes of loss functions},
	file = {arXiv\:1704.08045 PDF:/home/jeremiah/Zotero/storage/A6ETAGHC/Nguyen and Hein - 2017 - The loss surface of deep and wide neural networks.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/7PF54RWH/1704.html:text/html}
}

@inproceedings{mcintire_sparse_2016,
	address = {Arlington, Virginia, United States},
	series = {{UAI}'16},
	title = {Sparse {Gaussian} {Processes} for {Bayesian} {Optimization}},
	isbn = {978-0-9966431-1-5},
	url = {http://dl.acm.org/citation.cfm?id=3020948.3021002},
	abstract = {Bayesian optimization schemes often rely on Gaussian processes (GP). GP models are very flexible, but are known to scale poorly with the number of training points. While several efficient sparse GP models are known, they have limitations when applied in optimization settings. We propose a novel Bayesian optimization framework that uses sparse online Gaussian processes. We introduce a new updating scheme for the online GP that accounts for our preference during optimization for regions with better performance. We apply this method to optimize the performance of a free-electron laser, and demonstrate empirically that the weighted updating scheme leads to substantial improvements to performance in optimization.},
	booktitle = {Proceedings of the {Thirty}-{Second} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {McIntire, Mitchell and Ratner, Daniel and Ermon, Stefano},
	year = {2016},
	pages = {517--526}
}

@article{calandra_manifold_2014,
	title = {Manifold {Gaussian} {Processes} for {Regression}},
	url = {http://arxiv.org/abs/1402.5876},
	abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
	journal = {arXiv:1402.5876 [cs, stat]},
	author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.5876},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: 8 pages, accepted to IJCNN 2016},
	file = {arXiv\:1402.5876 PDF:/home/jeremiah/Zotero/storage/RQX5NZPB/Calandra et al. - 2014 - Manifold Gaussian Processes for Regression.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/M4TCCGI2/1402.html:text/html}
}

@article{duchesne_computing_2010,
	title = {Computing the distribution of quadratic forms: {Further} comparisons between the {Liu}–{Tang}–{Zhang} approximation and exact methods},
	volume = {54},
	issn = {0167-9473},
	shorttitle = {Computing the distribution of quadratic forms},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947309004381},
	doi = {10.1016/j.csda.2009.11.025},
	abstract = {Liu, Tang and Zhang [Liu, H. Tang, Y., Zhang H.H. 2009. A new chi-square approximation to the distribution of non-negative definite quadratic forms in non-central normal variables. Computational Statistics \&amp; Data Analysis 53, 853–856] proposed a chi-square approximation to the distribution of non-negative definite quadratic forms in non-central normal variables. To approximate the distribution of interest, they used a non-central chi-square distribution, where the degrees of freedom and the non-centrality parameter were calculated using the first four cumulants of the quadratic form. Numerical examples were encouraging, suggesting that the approximation was particularly accurate in the upper tail of the distribution. We present here additional empirical evidence, comparing Liu–Tang–Zhang’s four-moment non-central chi-square approximation with exact methods. While the moment-based method is interesting because of its simplicity, we demonstrate that it should be used with care in practical work, since numerical examples suggest that significant differences may occur between that method and exact methods, even in the upper tail of the distribution.},
	number = {4},
	journal = {Computational Statistics \& Data Analysis},
	author = {Duchesne, Pierre and Lafaye De Micheaux, Pierre},
	month = apr,
	year = {2010},
	pages = {858--862},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/9FGWHSF6/S0167947309004381.html:text/html}
}

@book{noauthor_distribution-free_nodate,
	title = {A {Distribution}-{Free} {Theory} of {Nonparametric} {Regression} {\textbar} {László} {Györfi} {\textbar} {Springer}},
	url = {http://www.springer.com/us/book/9780387954417},
	abstract = {The regression estimation problem has a long history. Already in 1632 Galileo Galilei used a procedure which can be interpreted as ?tting a linear...},
	urldate = {2017-06-08}
}

@incollection{paciorek_nonstationary_2004,
	title = {Nonstationary {Covariance} {Functions} for {Gaussian} {Process} {Regression}},
	url = {http://papers.nips.cc/paper/2350-nonstationary-covariance-functions-for-gaussian-process-regression.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 16},
	publisher = {MIT Press},
	author = {Paciorek, Christopher J. and Schervish, Mark J.},
	editor = {Thrun, S. and Saul, L. K. and Schölkopf, P. B.},
	year = {2004},
	pages = {273--280},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/RGUBRR6K/Paciorek and Schervish - 2004 - Nonstationary Covariance Functions for Gaussian Pr.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/NC6I5PQC/2350-nonstationary-covariance-functions-for-gaussian-process-regression.html:text/html}
}

@article{van_der_laan_asymptotic_2003,
	title = {Asymptotic {Optimality} of {Likelihood} {Based} {Cross}-{Validation}},
	url = {http://biostats.bepress.com/ucbbiostat/paper125},
	journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {van der Laan, Mark and Dudoit, Sandrine and Keles, Sunduz},
	month = feb,
	year = {2003},
	file = {"Asymptotic Optimality of Likelihood Based Cross-Validation" by Mark J. van der Laan, Sandrine Dudoit et al.:/home/jeremiah/Zotero/storage/39KXISWF/paper125.html:text/html}
}

@article{friston_free-energy_2010-1,
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	copyright = {© 2010 Nature Publishing Group},
	issn = {1471-003X},
	shorttitle = {The free-energy principle},
	url = {https://www.nature.com/nrn/journal/v11/n2/full/nrn2787.html},
	doi = {10.1038/nrn2787},
	abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories — optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.},
	language = {en},
	number = {2},
	urldate = {2017-06-04},
	journal = {Nature Reviews Neuroscience},
	author = {Friston, Karl},
	month = feb,
	year = {2010},
	pages = {127--138},
	file = {Snapshot:/home/jeremiah/Zotero/storage/Q6BPZNRS/nrn2787.html:text/html}
}

@article{friston_free-energy_2010-2,
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	copyright = {© 2010 Nature Publishing Group},
	issn = {1471-003X},
	shorttitle = {The free-energy principle},
	url = {https://www.nature.com/nrn/journal/v11/n2/full/nrn2787.html},
	doi = {10.1038/nrn2787},
	abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories — optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.},
	language = {en},
	number = {2},
	urldate = {2017-06-04},
	journal = {Nature Reviews Neuroscience},
	author = {Friston, Karl},
	month = feb,
	year = {2010},
	pages = {127--138},
	file = {Snapshot:/home/jeremiah/Zotero/storage/37RFV8G3/nrn2787.html:text/html}
}

@article{friston_predictive_2009,
	title = {Predictive coding under the free-energy principle},
	volume = {364},
	copyright = {© 2009 The Royal Society},
	issn = {0962-8436, 1471-2970},
	url = {http://rstb.royalsocietypublishing.org/content/364/1521/1211},
	doi = {10.1098/rstb.2008.0300},
	abstract = {This paper considers prediction and perceptual categorization as an inference problem that is solved by the brain. We assume that the brain models the world as a hierarchy or cascade of dynamical systems that encode causal structure in the sensorium. Perception is equated with the optimization or inversion of these internal models, to explain sensory data. Given a model of how sensory data are generated, we can invoke a generic approach to model inversion, based on a free energy bound on the model's evidence. The ensuing free-energy formulation furnishes equations that prescribe the process of recognition, i.e. the dynamics of neuronal activity that represent the causes of sensory input. Here, we focus on a very general model, whose hierarchical and dynamical structure enables simulated brains to recognize and predict trajectories or sequences of sensory states. We first review hierarchical dynamical models and their inversion. We then show that the brain has the necessary infrastructure to implement this inversion and illustrate this point using synthetic birds that can recognize and categorize birdsongs.},
	language = {en},
	number = {1521},
	urldate = {2017-06-04},
	journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
	author = {Friston, Karl and Kiebel, Stefan},
	month = may,
	year = {2009},
	pmid = {19528002},
	pages = {1211--1221},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/QSW6ICRR/Friston and Kiebel - 2009 - Predictive coding under the free-energy principle.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/JTERETH7/1211.html:text/html}
}

@article{hernandez-lobato_general_2015,
	title = {A {General} {Framework} for {Constrained} {Bayesian} {Optimization} using {Information}-based {Search}},
	url = {http://arxiv.org/abs/1511.09422},
	abstract = {We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real-world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta-computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.},
	journal = {arXiv:1511.09422 [stat]},
	author = {Hernández-Lobato, José Miguel and Gelbart, Michael A. and Adams, Ryan P. and Hoffman, Matthew W. and Ghahramani, Zoubin},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.09422},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1511.09422 PDF:/home/jeremiah/Zotero/storage/83MJJG33/Hernández-Lobato et al. - 2015 - A General Framework for Constrained Bayesian Optim.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/5XZV39ID/1511.html:text/html}
}

@inproceedings{gardner_bayesian_2014,
	title = {Bayesian {Optimization} with {Inequality} {Constraints}},
	url = {http://proceedings.mlr.press/v32/gardner14.html},
	abstract = {Bayesian optimization is a powerful framework for minimizing expensive objective functions while using very few function evaluations.  It has been successfully applied to a variety of problems, inc...},
	language = {en},
	urldate = {2017-06-04},
	booktitle = {{PMLR}},
	author = {Gardner, Jacob and Kusner, Matt and Zhixiang and Weinberger, Kilian and Cunningham, John},
	month = jan,
	year = {2014},
	pages = {937--945},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/WIPSSARP/Gardner et al. - 2014 - Bayesian Optimization with Inequality Constraints.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/PTNSCPS5/gardner14.html:text/html}
}

@inproceedings{hernandez-lobato_predictive_2015,
	title = {Predictive {Entropy} {Search} for {Bayesian} {Optimization} with {Unknown} {Constraints}},
	url = {http://proceedings.mlr.press/v37/hernandez-lobatob15.html},
	abstract = {Unknown constraints arise in many types of expensive black-box optimization problems. Several methods have been proposed recently for performing Bayesian optimization with constraints, based on the...},
	language = {en},
	urldate = {2017-06-04},
	booktitle = {{PMLR}},
	author = {Hernandez-Lobato, Jose Miguel and Gelbart, Michael and Hoffman, Matthew and Adams, Ryan and Ghahramani, Zoubin},
	month = jun,
	year = {2015},
	pages = {1699--1707},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/7U3P4XTE/Hernandez-Lobato et al. - 2015 - Predictive Entropy Search for Bayesian Optimizatio.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/BQUJDDMV/hernandez-lobatob15.html:text/html}
}

@article{gelbart_bayesian_2014,
	title = {Bayesian {Optimization} with {Unknown} {Constraints}},
	url = {http://arxiv.org/abs/1403.5607},
	abstract = {Recent work on Bayesian optimization has shown its effectiveness in global optimization of difficult black-box objective functions. Many real-world optimization problems of interest also have constraints which are unknown a priori. In this paper, we study Bayesian optimization for constrained problems in the general case that noise may be present in the constraint functions, and the objective and constraints may be evaluated independently. We provide motivating practical examples, and present a general framework to solve such problems. We demonstrate the effectiveness of our approach on optimizing the performance of online latent Dirichlet allocation subject to topic sparsity constraints, tuning a neural network given test-time memory constraints, and optimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed time, subject to passing standard convergence diagnostics.},
	journal = {arXiv:1403.5607 [cs, stat]},
	author = {Gelbart, Michael A. and Snoek, Jasper and Adams, Ryan P.},
	month = mar,
	year = {2014},
	note = {arXiv: 1403.5607},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: 14 pages, 3 figures},
	file = {arXiv\:1403.5607 PDF:/home/jeremiah/Zotero/storage/FFD5WS8N/Gelbart et al. - 2014 - Bayesian Optimization with Unknown Constraints.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/DQ4CV594/1403.html:text/html}
}

@article{didier_gaussian_2008,
	title = {Gaussian {Stationary} {Processes}: {Adaptive} {Wavelet} {Decompositions}, {Discrete} {Approximations}, and {Their} {Convergence}},
	volume = {14},
	issn = {1069-5869, 1531-5851},
	shorttitle = {Gaussian {Stationary} {Processes}},
	url = {https://link.springer.com/article/10.1007/s00041-008-9012-6},
	doi = {10.1007/s00041-008-9012-6},
	abstract = {We establish particular wavelet-based decompositions of Gaussian stationary processes in continuous time. These decompositions have a multiscale structure, independent Gaussian random variables in high-frequency terms, and the random coefficients of low-frequency terms approximating the Gaussian stationary process itself. They can also be viewed as extensions of the earlier wavelet-based decompositions of Zhang and Walter (IEEE Trans. Signal Process. 42(7):1737–1745, [1994]) for stationary processes, and Meyer et al. (J. Fourier Anal. Appl. 5(5):465–494, [1999]) for fractional Brownian motion. Several examples of Gaussian random processes are considered such as the processes with rational spectral densities. An application to simulation is presented where an associated Fast Wavelet Transform-like algorithm plays a key role.},
	language = {en},
	number = {2},
	urldate = {2017-06-04},
	journal = {Journal of Fourier Analysis and Applications},
	author = {Didier, Gustavo and Pipiras, Vladas},
	month = apr,
	year = {2008},
	pages = {203--234},
	file = {Snapshot:/home/jeremiah/Zotero/storage/7D93XWWQ/s00041-008-9012-6.html:text/html}
}

@article{noauthor_hierarchical_2006,
	title = {Hierarchical {Dirichlet} {Processes}},
	volume = {101},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302},
	doi = {10.1198/016214506000000302},
	abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
	number = {476},
	journal = {Journal of the American Statistical Association},
	month = dec,
	year = {2006},
	pages = {1566--1581},
	file = {Snapshot:/home/jeremiah/Zotero/storage/M3TWTCTR/016214506000000302.html:text/html}
}

@book{muller_chapter_2013,
	title = {Chapter 5: {Dependent} {Dirichlet} {Processes} and {Other} {Extensions}},
	isbn = {978-0-940600-82-9},
	shorttitle = {Chapter 5},
	url = {http://projecteuclid.org/euclid.cbms/1362163750},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-06-04},
	publisher = {IMS and ASA},
	author = {Müller, Peter and Rodriguez, Abel},
	year = {2013},
	file = {Snapshot:/home/jeremiah/Zotero/storage/7ZF3WBUB/1362163750.html:text/html}
}

@article{antoniadis_wavelet_2001,
	title = {Wavelet {Estimators} in {Nonparametric} {Regression}: {A} {Comparative} {Simulation} {Study}},
	volume = {6},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/v006/i06},
	doi = {10.18637/jss.v006.i06},
	abstract = {Wavelet analysis has been found to be a powerful tool for the nonparametric estimation of spatially-variable objects. We discuss in detail wavelet methods in nonparametric regression, where the data are modelled as observations of a signal contaminated with additive Gaussian noise, and provide an extensive review of the vast literature of wavelet shrinkage and wavelet thresholding estimators developed to denoise such data. These estimators arise from a wide range of classical and empirical Bayes methods treating either individual or blocks of wavelet coefficients. We compare various estimators in an extensive simulation study on a variety of sample sizes, test functions, signal-to-noise ratios and wavelet filters. Because there is no single criterion that can adequately summarise the behaviour of an estimator, we use various criteria to measure performance in finite sample situations. Insight into the performance of these estimators is obtained from graphical outputs and numerical tables. In order to provide some hints of how these estimators should be used to analyse real data sets, a detailed practical step-by-step illustration of a wavelet denoising analysis on electrical consumption is provided. Matlab codes are provided so that all figures and tables in this paper can be reproduced.},
	number = {6},
	journal = {Journal of Statistical Software, Articles},
	author = {Antoniadis, Anestis and Bigot, Jeremie and Sapatinas, Theofanis},
	year = {2001},
	pages = {1--83}
}

@misc{noauthor_wavelet_nodate,
	title = {Wavelet {Estimators} in {Nonparametric} {Regression}: {A} {Comparative} {Simulation} {Study} {\textbar} {Antoniadis} {\textbar} {Journal} of {Statistical} {Software}},
	shorttitle = {Wavelet {Estimators} in {Nonparametric} {Regression}},
	url = {https://www.jstatsoft.org/article/view/v006i06/},
	urldate = {2017-06-04},
	file = {Snapshot:/home/jeremiah/Zotero/storage/RE8FHXZW/v006i06.html:text/html}
}

@article{wang_bayesian_2013,
	title = {Bayesian {Optimization} in a {Billion} {Dimensions} via {Random} {Embeddings}},
	url = {http://arxiv.org/abs/1301.1942},
	abstract = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.},
	journal = {arXiv:1301.1942 [cs, stat]},
	author = {Wang, Ziyu and Hutter, Frank and Zoghi, Masrour and Matheson, David and de Freitas, Nando},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.1942},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: 33 pages},
	file = {arXiv\:1301.1942 PDF:/home/jeremiah/Zotero/storage/EUFTZXS5/Wang et al. - 2013 - Bayesian Optimization in a Billion Dimensions via .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/M52A76HC/1301.html:text/html}
}

@article{zhu_generative_2017,
	title = {Generative {Adversarial} {Active} {Learning}},
	url = {http://arxiv.org/abs/1702.07956},
	abstract = {We propose a new active learning approach using Generative Adversarial Networks (GAN). Different from regular active learning, the resulting algorithm adaptively synthesizes training instances for querying to increase learning speed. We generate queries according to the uncertainty principle, but our idea can work with other active learning principles. We report results from various numerical experiments to demonstrate the effectiveness the proposed approach. In some settings, the proposed algorithm outperforms traditional pool-based approaches. To the best our knowledge, this is the first active learning work using GAN.},
	journal = {arXiv:1702.07956 [cs, stat]},
	author = {Zhu, Jia-Jie and Bento, José},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.07956},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1702.07956 PDF:/home/jeremiah/Zotero/storage/7H8JA3PB/Zhu and Bento - 2017 - Generative Adversarial Active Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/AFR3CETB/1702.html:text/html}
}

@inproceedings{titsias_bayesian_2010,
	title = {Bayesian {Gaussian} {Process} {Latent} {Variable} {Model}},
	url = {http://proceedings.mlr.press/v9/titsias10a.html},
	abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to vari...},
	language = {en},
	urldate = {2017-06-02},
	booktitle = {{PMLR}},
	author = {Titsias, Michalis and Lawrence, Neil},
	month = mar,
	year = {2010},
	pages = {844--851},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/TMWEF95W/Titsias and Lawrence - 2010 - Bayesian Gaussian Process Latent Variable Model.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/26W6RKWA/titsias10a.html:text/html}
}

@article{arlot_survey_2010,
	title = {A survey of cross-validation procedures for model selection},
	volume = {4},
	issn = {1935-7516},
	url = {http://arxiv.org/abs/0907.4728},
	doi = {10.1214/09-SS054},
	abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its apparent universality. Many results exist on the model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
	number = {0},
	journal = {Statistics Surveys},
	author = {Arlot, Sylvain and Celisse, Alain},
	year = {2010},
	note = {arXiv: 0907.4728},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Mathematics - Statistics Theory, Statistics - Applications, 62G08, 62G05, 62G09},
	pages = {40--79},
	file = {arXiv\:0907.4728 PDF:/home/jeremiah/Zotero/storage/E8EP2U9F/Arlot and Celisse - 2010 - A survey of cross-validation procedures for model .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WFI68PXC/0907.html:text/html}
}

@article{snoek_scalable_2015-1,
	title = {Scalable {Bayesian} {Optimization} {Using} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.05700},
	abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
	journal = {arXiv:1502.05700 [stat]},
	author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md Mostofa Ali and Prabhat and Adams, Ryan P.},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05700},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1502.05700 PDF:/home/jeremiah/Zotero/storage/3F479ZG5/Snoek et al. - 2015 - Scalable Bayesian Optimization Using Deep Neural N.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/THQ86MJ5/1502.html:text/html}
}

@article{kandasamy_high_2015-1,
	title = {High {Dimensional} {Bayesian} {Optimisation} and {Bandits} via {Additive} {Models}},
	url = {http://arxiv.org/abs/1503.01673},
	abstract = {Bayesian Optimisation (BO) is a technique used in optimising a \$D\$-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on \$D\$ even though the function depends on all \$D\$ dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.},
	journal = {arXiv:1503.01673 [cs, stat]},
	author = {Kandasamy, Kirthevasan and Schneider, Jeff and Poczos, Barnabas},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.01673},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: Proceedings of The 32nd International Conference on Machine Learning 2015},
	file = {arXiv\:1503.01673 PDF:/home/jeremiah/Zotero/storage/3VMXKR34/Kandasamy et al. - 2015 - High Dimensional Bayesian Optimisation and Bandits.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/VQNZ6R4U/1503.html:text/html}
}

@inproceedings{lawrence_hierarchical_2007,
	address = {New York, NY, USA},
	series = {{ICML} '07},
	title = {Hierarchical {Gaussian} {Process} {Latent} {Variable} {Models}},
	isbn = {978-1-59593-793-3},
	url = {http://doi.acm.org/10.1145/1273496.1273557},
	doi = {10.1145/1273496.1273557},
	abstract = {The Gaussian process latent variable model (GP-LVM) is a powerful approach for probabilistic modelling of high dimensional data through dimensional reduction. In this paper we extend the GP-LVM through hierarchies. A hierarchical model (such as a tree) allows us to express conditional independencies in the data as well as the manifold structure. We first introduce Gaussian process hierarchies through a simple dynamical model, we then extend the approach to a more complex hierarchy which is applied to the visualisation of human motion data sets.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Lawrence, Neil D. and Moore, Andrew J.},
	year = {2007},
	pages = {481--488}
}

@article{zhang_learning_2005,
	title = {Learning bounds for kernel regression using effective data dimensionality},
	volume = {17},
	issn = {0899-7667},
	doi = {10.1162/0899766054323008},
	abstract = {Kernel methods can embed finite-dimensional data into infinite-dimensional feature spaces. In spite of the large underlying feature dimensionality, kernel methods can achieve good generalization ability. This observation is often wrongly interpreted, and it has been used to argue that kernel learning can magically avoid the "curse-of-dimensionality" phenomenon encountered in statistical estimation problems. This letter shows that although using kernel representation, one can embed data into an infinite-dimensional feature space; the effective dimensionality of this embedding, which determines the learning complexity of the underlying kernel machine, is usually small. In particular, we introduce an algebraic definition of a scale-sensitive effective dimension associated with a kernel representation. Based on this quantity, we derive upper bounds on the generalization performance of some kernel regression methods. Moreover, we show that the resulting convergent rates are optimal under various circumstances.},
	language = {eng},
	number = {9},
	journal = {Neural Computation},
	author = {Zhang, Tong},
	month = sep,
	year = {2005},
	pmid = {15992491},
	keywords = {Artificial Intelligence, Models, Theoretical, Signal Processing, Computer-Assisted},
	pages = {2077--2098}
}

@article{zhu_bayesian_2011,
	title = {Bayesian influence analysis: a geometric approach},
	volume = {98},
	issn = {0006-3444},
	shorttitle = {Bayesian influence analysis},
	url = {http://www.jstor.org/stable/23076152},
	abstract = {In this paper we develop a general framework of Bayesian influence analysis for assessing various perturbation schemes to the data, the prior and the sampling distribution for a class of statistical models. We introduce a perturbation model to characterize these various perturbation schemes. We develop a geometric framework, called the Bayesian perturbation manifold, and use its associated geometric quantities including the metric tensor and geodesic to characterize the intrinsic structure of the perturbation model. We develop intrinsic influence measures and local influence measures based on the Bayesian perturbation manifold to quantify the effect of various perturbations to statistical models. Theoretical and numerical examples are examined to highlight the broad spectrum of applications of this local influence method in a formal Bayesian analysis.},
	number = {2},
	journal = {Biometrika},
	author = {ZHU, HONGTU and IBRAHIM, JOSEPH G. and TANG, NIANSHENG},
	year = {2011},
	pages = {307--323}
}

@article{liu_robust_2016,
	title = {Robust {High}-{Dimensional} {Linear} {Regression}},
	url = {http://arxiv.org/abs/1608.02257},
	abstract = {The effectiveness of supervised learning techniques has made them ubiquitous in research and practice. In high-dimensional settings, supervised learning commonly relies on dimensionality reduction to improve performance and identify the most important factors in predicting outcomes. However, the economic importance of learning has made it a natural target for adversarial manipulation of training data, which we term poisoning attacks. Prior approaches to dealing with robust supervised learning rely on strong assumptions about the nature of the feature matrix, such as feature independence and sub-Gaussian noise with low variance. We propose an integrated method for robust regression that relaxes these assumptions, assuming only that the feature matrix can be well approximated by a low-rank matrix. Our techniques integrate improved robust low-rank matrix approximation and robust principle component regression, and yield strong performance guarantees. Moreover, we experimentally show that our methods significantly outperform state of the art both in running time and prediction error.},
	urldate = {2017-06-01},
	journal = {arXiv:1608.02257 [cs, stat]},
	author = {Liu, Chang and Li, Bo and Vorobeychik, Yevgeniy and Oprea, Alina},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.02257},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1608.02257 PDF:/home/jeremiah/Zotero/storage/W8FKM7T6/Liu et al. - 2016 - Robust High-Dimensional Linear Regression.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/GG5SXXI9/1608.html:text/html}
}

@inproceedings{grohans_bayesian_2013,
	title = {Bayesian {Games} for {Adversarial} {Regression} {Problems}},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013_grosshans13},
	urldate = {2017-06-01},
	author = {Gro⬚hans, Michael and Sawade, Christoph and Br⬚ckner, Michael and Scheffer, Tobias},
	year = {2013},
	pages = {55--63},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/X32XKRPB/Grohans et al. - 2013 - Bayesian Games for Adversarial Regression Problems.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/AJXKP6XJ/icml2013_grosshans13.html:text/html}
}

@article{ribeiro_model-agnostic_2016,
	title = {Model-{Agnostic} {Interpretability} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1606.05386},
	abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
	urldate = {2017-05-31},
	journal = {arXiv:1606.05386 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.05386},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY},
	file = {arXiv\:1606.05386 PDF:/home/jeremiah/Zotero/storage/5EVTIHHV/Ribeiro et al. - 2016 - Model-Agnostic Interpretability of Machine Learnin.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/55562QXP/1606.html:text/html}
}

@article{ahn_stable_2010,
	title = {A stable hyperparameter selection for the {Gaussian} {RBF} kernel for discrimination},
	volume = {3},
	issn = {1932-1872},
	url = {http://onlinelibrary.wiley.com.ezp-prod1.hul.harvard.edu/doi/10.1002/sam.10073/abstract},
	doi = {10.1002/sam.10073},
	abstract = {Kernel-based classification methods, for example, support vector machines, map the data into a higher-dimensional space via a kernel function. In practice, choosing the value of hyperparameter in the kernel function is crucial in order to ensure good performance. We propose a method of selecting the hyperparameter in the Gaussian radial basis function (RBF) kernel by considering the geometry of the embedded feature space. This method is independent of the choice of the discrimination algorithm and also computationally efficient. Its classification performance is competitive with existing methods including cross-validation. Using simulated and real-data examples, we show that the proposed method is stable with respect to sampling variability. Copyright © 2010 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 3: 142-148, 2010},
	language = {en},
	number = {3},
	urldate = {2017-05-30},
	journal = {Statistical Analysis and Data Mining},
	author = {Ahn, Jeongyoun},
	month = jun,
	year = {2010},
	keywords = {Classification, Support vector machines, data embedding, hyperparameter selection, kernel method, stability},
	pages = {142--148},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/ZITV5RA3/Ahn - 2010 - A stable hyperparameter selection for the Gaussian.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/IN2STCIQ/abstract.html:text/html}
}

@article{ahn_stable_2010-1,
	title = {A stable hyperparameter selection for the {Gaussian} {RBF} kernel for discrimination},
	volume = {3},
	issn = {1932-1872},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/sam.10073/abstract},
	doi = {10.1002/sam.10073},
	abstract = {Kernel-based classification methods, for example, support vector machines, map the data into a higher-dimensional space via a kernel function. In practice, choosing the value of hyperparameter in the kernel function is crucial in order to ensure good performance. We propose a method of selecting the hyperparameter in the Gaussian radial basis function (RBF) kernel by considering the geometry of the embedded feature space. This method is independent of the choice of the discrimination algorithm and also computationally efficient. Its classification performance is competitive with existing methods including cross-validation. Using simulated and real-data examples, we show that the proposed method is stable with respect to sampling variability. Copyright © 2010 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 3: 142-148, 2010},
	language = {en},
	number = {3},
	urldate = {2017-05-30},
	journal = {Statistical Analysis and Data Mining},
	author = {Ahn, Jeongyoun},
	month = jun,
	year = {2010},
	keywords = {Classification, Support vector machines, data embedding, hyperparameter selection, kernel method, stability},
	pages = {142--148},
	file = {Snapshot:/home/jeremiah/Zotero/storage/X35GE6TB/abstract.html:text/html}
}

@article{zhang_learning_2005-1,
	title = {Learning {Bounds} for {Kernel} {Regression} {Using} {Effective} {Data} {Dimensionality}},
	volume = {17},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/0899766054323008},
	doi = {10.1162/0899766054323008},
	number = {9},
	urldate = {2017-05-23},
	journal = {Neural Computation},
	author = {Zhang, Tong},
	month = sep,
	year = {2005},
	pages = {2077--2098},
	file = {Neural Computation Snapshot:/home/jeremiah/Zotero/storage/M3NJ9V4F/0899766054323008.html:text/html}
}

@article{bousquet_stability_2002,
	title = {Stability and {Generalization}},
	volume = {2},
	issn = {1532-4435},
	url = {http://dx.doi.org/10.1162/153244302760200704},
	doi = {10.1162/153244302760200704},
	urldate = {2017-05-23},
	journal = {J. Mach. Learn. Res.},
	author = {Bousquet, Olivier and Elisseeff, André},
	month = mar,
	year = {2002},
	pages = {499--526}
}

@article{mukherjee_learning_2006,
	title = {Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization},
	volume = {25},
	issn = {1019-7168, 1572-9044},
	shorttitle = {Learning theory},
	url = {https://link.springer.com/article/10.1007/s10444-004-7634-z},
	doi = {10.1007/s10444-004-7634-z},
	abstract = {Solutions of learning problems by Empirical Risk Minimization (ERM) – and almost-ERM when the minimizer does not exist – need to be consistent, so that they may be predictive. They also need to be well-posed in the sense of being stable, so that they might be used robustly. We propose a statistical form of stability, defined as leave-one-out (LOO) stability. We prove that for bounded loss classes LOO stability is (a) sufficient for generalization, that is convergence in probability of the empirical error to the expected error, for any algorithm satisfying it and, (b) necessary and sufficient for consistency of ERM. Thus LOO stability is a weak form of stability that represents a sufficient condition for generalization for symmetric learning algorithms while subsuming the classical conditions for consistency of ERM. In particular, we conclude that a certain form of well-posedness and consistency are equivalent for ERM.},
	language = {en},
	number = {1-3},
	urldate = {2017-05-23},
	journal = {Advances in Computational Mathematics},
	author = {Mukherjee, Sayan and Niyogi, Partha and Poggio, Tomaso and Rifkin, Ryan},
	month = jul,
	year = {2006},
	pages = {161--193},
	file = {Snapshot:/home/jeremiah/Zotero/storage/3DRRUT36/s10444-004-7634-z.html:text/html}
}

@article{sugiyama_trading_2004,
	title = {Trading variance reduction with unbiasedness: the regularized subspace information criterion for robust model selection in kernel regression},
	volume = {16},
	issn = {0899-7667},
	shorttitle = {Trading variance reduction with unbiasedness},
	doi = {10.1162/089976604773135113},
	abstract = {A well-known result by Stein (1956) shows that in particular situations, biased estimators can yield better parameter estimates than their generally preferred unbiased counterparts. This letter follows the same spirit, as we will stabilize the unbiased generalization error estimates by regularization and finally obtain more robust model selection criteria for learning. We trade a small bias against a larger variance reduction, which has the beneficial effect of being more precise on a single training set. We focus on the subspace information criterion (SIC), which is an unbiased estimator of the expected generalization error measured by the reproducing kernel Hilbert space norm. SIC can be applied to the kernel regression, and it was shown in earlier experiments that a small regularization of SIC has a stabilization effect. However, it remained open how to appropriately determine the degree of regularization in SIC. In this article, we derive an unbiased estimator of the expected squared error, between SIC and the expected generalization error and propose determining the degree of regularization of SIC such that the estimator of the expected squared error is minimized. Computer simulations with artificial and real data sets illustrate that the proposed method works effectively for improving the precision of SIC, especially in the high-noise-level cases. We furthermore compare the proposed method to the original SIC, the cross-validation, and an empirical Bayesian method in ridge parameter selection, with good results.},
	language = {eng},
	number = {5},
	journal = {Neural Computation},
	author = {Sugiyama, Masashi and Kawanabe, Motoaki and Müller, Klaus-Robert},
	month = may,
	year = {2004},
	pmid = {15070511},
	keywords = {Models, Statistical, Bayes Theorem, Regression analysis, Models, Neurological, Normal Distribution},
	pages = {1077--1104}
}

@article{zhang_understanding_2016,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	urldate = {2017-05-22},
	journal = {arXiv:1611.03530 [cs]},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.03530},
	keywords = {Computer Science - Learning},
	annote = {Comment: Published in ICLR 2017},
	file = {arXiv\:1611.03530 PDF:/home/jeremiah/Zotero/storage/W4TU8RAR/Zhang et al. - 2016 - Understanding deep learning requires rethinking ge.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/2AAQG55J/1611.html:text/html}
}

@article{sriperumbudur_optimal_2016,
	title = {On the optimal estimation of probability measures in weak and strong topologies},
	volume = {22},
	issn = {1350-7265},
	url = {https://projecteuclid.org/euclid.bj/1458133001},
	doi = {10.3150/15-BEJ713},
	abstract = {Given random samples drawn i.i.d. from a probability measure ℙP{\textbackslash}mathbb\{P\} (defined on say, ℝdRd{\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}), it is well-known that the empirical estimator is an optimal estimator of ℙP{\textbackslash}mathbb\{P\} in weak topology but not even a consistent estimator of its density (if it exists) in the strong topology (induced by the total variation distance). On the other hand, various popular density estimators such as kernel and wavelet density estimators are optimal in the strong topology in the sense of achieving the minimax rate over all estimators for a Sobolev ball of densities. Recently, it has been shown in a series of papers by Giné and Nickl that these density estimators on ℝR{\textbackslash}mathbb\{R\} that are optimal in strong topology are also optimal in ‖⋅‖‖⋅‖F{\textbackslash}Vert{\textbackslash}cdot{\textbackslash}Vert\_\{{\textbackslash}mathcal\{F\} \} for certain choices of F{\textbackslash}mathcal\{F\} such that ‖⋅‖‖⋅‖F{\textbackslash}Vert{\textbackslash}cdot{\textbackslash}Vert\_\{{\textbackslash}mathcal\{F\} \} metrizes the weak topology, where ‖ℙ‖:=sup\{∫fdℙ: f∈\}‖P‖F:=sup\{∫fdP: f∈F\}{\textbackslash}Vert{\textbackslash}mathbb\{P\} {\textbackslash}Vert\_\{{\textbackslash}mathcal\{F\} \}:={\textbackslash}sup{\textbackslash}\{{\textbackslash}int f{\textbackslash},{\textbackslash}mathrm\{d\}{\textbackslash}mathbb\{P\} {\textbackslash}colon{\textbackslash} f{\textbackslash}in{\textbackslash}mathcal\{F\} {\textbackslash}\}. In this paper, we investigate this problem of optimal estimation in weak and strong topologies by choosing F{\textbackslash}mathcal\{F\} to be a unit ball in a reproducing kernel Hilbert space (say HFH{\textbackslash}mathcal\{F\}\_\{H\} defined over ℝdRd{\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}), where this choice is both of theoretical and computational interest. Under some mild conditions on the reproducing kernel, we show that ‖⋅‖H‖⋅‖FH{\textbackslash}Vert{\textbackslash}cdot{\textbackslash}Vert\_\{{\textbackslash}mathcal\{F\}\_\{H\}\} metrizes the weak topology and the kernel density estimator (with L1L1L{\textasciicircum}\{1\} optimal bandwidth) estimates ℙP{\textbackslash}mathbb\{P\} at dimension independent optimal rate of n−1/2n−1/2n{\textasciicircum}\{-1/2\} in ‖⋅‖H‖⋅‖FH{\textbackslash}Vert{\textbackslash}cdot{\textbackslash}Vert\_\{{\textbackslash}mathcal\{F\}\_\{H\}\} along with providing a uniform central limit theorem for the kernel density estimator.},
	language = {EN},
	number = {3},
	journal = {Bernoulli},
	author = {Sriperumbudur, Bharath},
	month = aug,
	year = {2016},
	mrnumber = {MR3474835},
	zmnumber = {06579716},
	keywords = {reproducing kernel Hilbert space, adaptive estimation, bounded Lipschitz metric, exponential inequality, kernel density estimator, Rademacher chaos, smoothed empirical processes, total variation distance, two-sample test, U-processes, uniform central limit theorem},
	pages = {1839--1893},
	file = {Snapshot:/home/jeremiah/Zotero/storage/R3ID8R5V/1458133001.html:text/html}
}

@article{reiss_learning_2002,
	title = {Learning to prove: {The} idea of heuristic examples},
	volume = {34},
	issn = {1615-679X, 1863-9704},
	shorttitle = {Learning to prove},
	url = {https://link.springer.com/article/10.1007/BF02655690},
	doi = {10.1007/BF02655690},
	abstract = {Proof is an important topic in the area of mathematics curriculum and an essential aspect of mathematical competence. However, recent studies have revealed wide gaps in student's understanding of proof. Furthermore, effective teaching to prove, for example, by Schoenfeld's approach, is a real challenge for teachers. A very powerful and empirically well founded method of learning mathematics, which is also relatively easy to implement in the classroom, is learning through worked-out examples. It is, however, primarily suited for algorithmic content areas. We propose the concept of using heuristic worked-out examples, which do not provide an algorithmic problem solution but offer instead heuristic steps that lead towards finding a proof. We rely on Boero's model of proving in designing the single sub-steps of a heuristic example. We illustrate our instructional idea by presenting an heuristic example for proving that the interior angles in any triangle add up to 180°.},
	language = {en},
	number = {1},
	journal = {Zentralblatt für Didaktik der Mathematik},
	author = {Reiss, Kristina and Renkl, Alexander},
	month = feb,
	year = {2002},
	pages = {29--35},
	file = {Snapshot:/home/jeremiah/Zotero/storage/X29MFB8T/BF02655690.html:text/html}
}

@article{koh_understanding_2017,
	title = {Understanding {Black}-box {Predictions} via {Influence} {Functions}},
	url = {http://arxiv.org/abs/1703.04730},
	abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
	urldate = {2017-12-08},
	journal = {arXiv:1703.04730 [cs, stat]},
	author = {Koh, Pang Wei and Liang, Percy},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.04730},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: International Conference on Machine Learning, 2017},
	file = {arXiv\:1703.04730 PDF:/home/jeremiah/Zotero/storage/D67KG93B/Koh and Liang - 2017 - Understanding Black-box Predictions via Influence .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/ZY2S4N2Z/1703.html:text/html}
}

@article{koh_understanding_2017-1,
	title = {Understanding {Black}-box {Predictions} via {Influence} {Functions}},
	url = {http://arxiv.org/abs/1703.04730},
	abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
	urldate = {2017-12-08},
	journal = {arXiv:1703.04730 [cs, stat]},
	author = {Koh, Pang Wei and Liang, Percy},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.04730},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: International Conference on Machine Learning, 2017},
	file = {arXiv\:1703.04730 PDF:/home/jeremiah/Zotero/storage/MSYB8HE6/Koh and Liang - 2017 - Understanding Black-box Predictions via Influence .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/JTJFKJUD/1703.html:text/html}
}

@article{bach_breaking_2014,
	title = {Breaking the {Curse} of {Dimensionality} with {Convex} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1412.8690},
	abstract = {We consider neural networks with a single hidden layer and non-decreasing homogeneous activa-tion functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of ob-servations. In addition, we provide a simple geometric interpretation to the non-convex problem of addition of a new unit, which is the core potentially hard computational element in the framework of learning from continuously many basis functions. We provide simple conditions for convex relaxations to achieve the same generalization error bounds, even when constant-factor approxi-mations cannot be found (e.g., because it is NP-hard such as for the zero-homogeneous activation function). We were not able to find strong enough convex relaxations and leave open the existence or non-existence of polynomial-time algorithms.},
	urldate = {2017-12-06},
	journal = {arXiv:1412.8690 [cs, math, stat]},
	author = {Bach, Francis},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.8690},
	keywords = {Computer Science - Learning, Mathematics - Statistics Theory, Mathematics - Optimization and Control},
	file = {arXiv\:1412.8690 PDF:/home/jeremiah/Zotero/storage/I3TRPZY5/Bach - 2014 - Breaking the Curse of Dimensionality with Convex N.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/VKRDZY35/1412.html:text/html}
}

@article{levy_generalizing_2017,
	title = {Generalizing {Hamiltonian} {Monte} {Carlo} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.09268},
	abstract = {We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 49x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. We release an open source TensorFlow implementation of the algorithm.},
	urldate = {2017-12-02},
	journal = {arXiv:1711.09268 [cs, stat]},
	author = {Levy, Daniel and Hoffman, Matthew D. and Sohl-Dickstein, Jascha},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09268},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1711.09268 PDF:/home/jeremiah/Zotero/storage/5D4CMY77/Levy et al. - 2017 - Generalizing Hamiltonian Monte Carlo with Neural N.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/236DIX22/1711.html:text/html}
}

@incollection{chwialkowski_wild_2014,
	title = {A {Wild} {Bootstrap} for {Degenerate} {Kernel} {Tests}},
	url = {http://papers.nips.cc/paper/5452-a-wild-bootstrap-for-degenerate-kernel-tests.pdf},
	urldate = {2017-12-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Chwialkowski, Kacper P and Sejdinovic, Dino and Gretton, Arthur},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3608--3616},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/33IYSVEE/Chwialkowski et al. - 2014 - A Wild Bootstrap for Degenerate Kernel Tests.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/TYXK2H8C/5452-a-wild-bootstrap-for-degenerate-kernel-tests.html:text/html}
}

@book{serfling_approximation_2001,
	address = {New York, NY},
	edition = {1 edition},
	title = {Approximation {Theorems} of {Mathematical} {Statistics}},
	isbn = {978-0-471-21927-9},
	abstract = {Approximation Theorems of Mathematical Statistics  This convenient paperback edition makes a seminal text in statistics accessible to a new generation of students and practitioners. Approximation Theorems of Mathematical Statistics covers a broad range of limit theorems useful in mathematical statistics, along with methods of proof and techniques of application. The manipulation of "probability" theorems to obtain "statistical" theorems is emphasized. Besides a knowledge of these basic statistical theorems, this lucid introduction to the subject imparts an appreciation of the instrumental role of probability theory.  The book makes accessible to students and practicing professionals in statistics, general mathematics, operations research, and engineering the essentials of: * The tools and foundations that are basic to asymptotic theory in statistics * The asymptotics of statistics computed from a sample, including transformations of vectors of more basic statistics, with emphasis on asymptotic distribution theory and strong convergence * Important special classes of statistics, such as maximum likelihood estimates and other asymptotic efficient procedures; W. Hoeffding's U-statistics and R. von Mises's "differentiable statistical functions" * Statistics obtained as solutions of equations ("M-estimates"), linear functions of order statistics ("L-statistics"), and rank statistics ("R-statistics") * Use of influence curves * Approaches toward asymptotic relative efficiency of statistical test procedures},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Serfling, Robert J.},
	month = dec,
	year = {2001}
}

@article{flanigan_discriminative_2014-1,
	title = {A {Discriminative} {Graph}-{Based} {Parser} for the {Abstract} {Meaning} {Representation}},
	url = {http://repository.cmu.edu/lti/153},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	author = {Flanigan, Jeffrey and Thomson, Sam and Carbonell, Jaime and Dyer, Chris and Smith, Noah},
	month = jun,
	year = {2014},
	pages = {1426--1436},
	file = {"A Discriminative Graph-Based Parser for the Abstract Meaning Represent" by Jeffrey Flanigan, Sam Thomson et al.:/home/jeremiah/Zotero/storage/GITYDDAT/153.html:text/html}
}

@misc{noauthor_learning_nodate,
	title = {Learning to {Automatically} {Solve} {Algebra} {Word} {Problems} - {Semantic} {Scholar}},
	url = {/paper/Learning-to-Automatically-Solve-Algebra-Word-Probl-Kushman-Zettlemoyer/10c52142b71267e22d57dcb610510a02a8f91d7d},
	abstract = {We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations, while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demonstrating that the system can correctly answer almost 70\% of the questions in the dataset. This is, to our knowledge, the first learning result for this task.},
	file = {Snapshot:/home/jeremiah/Zotero/storage/JUCRY2MT/10c52142b71267e22d57dcb610510a02a8f91d7d.html:text/html}
}

@article{krishnan_generating_2016,
	title = {Generating {Synthetic} {Data} for {Text} {Recognition}},
	url = {http://arxiv.org/abs/1608.04224},
	abstract = {Generating synthetic images is an art which emulates the natural process of image generation in a closest possible manner. In this work, we exploit such a framework for data generation in handwritten domain. We render synthetic data using open source fonts and incorporate data augmentation schemes. As part of this work, we release 9M synthetic handwritten word image corpus which could be useful for training deep network architectures and advancing the performance in handwritten word spotting and recognition tasks.},
	urldate = {2017-11-27},
	journal = {arXiv:1608.04224 [cs]},
	author = {Krishnan, Praveen and Jawahar, C. V.},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.04224},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 5 pages, 2 figures},
	file = {arXiv\:1608.04224 PDF:/home/jeremiah/Zotero/storage/PFF6LSVI/Krishnan and Jawahar - 2016 - Generating Synthetic Data for Text Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/GRGUYF4Z/1608.html:text/html}
}

@article{alvin_automatic_2015,
	title = {Automatic {Synthesis} of {Geometry} {Problems} for an {Intelligent} {Tutoring} {System}},
	url = {http://arxiv.org/abs/1510.08525},
	abstract = {This paper presents an intelligent tutoring system, GeoTutor, for Euclidean Geometry that is automatically able to synthesize proof problems and their respective solutions given a geometric figure together with a set of properties true of it. GeoTutor can provide personalized practice problems that address student deficiencies in the subject matter.},
	urldate = {2017-11-21},
	journal = {arXiv:1510.08525 [cs]},
	author = {Alvin, Chris and Gulwani, Sumit and Majumdar, Rupak and Mukhopadhyay, Supratik},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.08525},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: A formal version of the accepted AAAI '14 paper},
	file = {arXiv\:1510.08525 PDF:/home/jeremiah/Zotero/storage/XMTLWIR8/Alvin et al. - 2015 - Automatic Synthesis of Geometry Problems for an In.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/265MVQ9D/1510.html:text/html}
}

@inproceedings{wu_automatic_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Automatic {Geometry} {Theorem}-{Proving} and {Automatic} {Geometry} {Problem}-{Solving}},
	isbn = {978-3-540-66672-1 978-3-540-47997-0},
	url = {https://link.springer.com/chapter/10.1007/3-540-47997-X_1},
	doi = {10.1007/3-540-47997-X_1},
	abstract = {Studies in geometry involve both theorem-proving and problem-solving. So far the latter is concerned, we may cite for example geometrical locus-determination and geometrical constructions by ruler-compass which were much studied in ancient Greece. In particular we may mention the regular polygon construction and the three famous difficult problems of angle-trisection, cube-duplication and circle-squaring.},
	language = {en},
	booktitle = {Automated {Deduction} in {Geometry}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Wu, Wen-tsü},
	month = aug,
	year = {1998},
	pages = {1--13},
	file = {Snapshot:/home/jeremiah/Zotero/storage/L5GNWYY5/3-540-47997-X_1.html:text/html}
}

@misc{noauthor_geos_nodate,
	title = {{GeoS} - {Geometry} {Problem} {Solver}},
	url = {http://geometry.allenai.org/},
	urldate = {2017-11-21},
	file = {GeoS - Geometry Problem Solver:/home/jeremiah/Zotero/storage/N2EVBPFQ/geometry.allenai.org.html:text/html}
}

@inproceedings{wang_automated_2015,
	title = {Automated {Geometry} {Theorem} {Proving} for {Human}-{Readable} {Proofs}},
	copyright = {Authors who submit to this conference agree to the following terms:    a) Authors transfer their copyrights in their paper to the International Joint Conferences on Artificial Intelligence, Inc. (IJCAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights currently exist or hereafter come into effect, and also the exclusive right to create electronic versions of the paper, to the extent that such right is not subsumed under copyright.    b) Every named author warrants that he/she is the sole author and owner of the copyright in the paper, except for those portions shown to be in quotations; that the paper is original throughout; and that their right to make the grants set forth above is complete and unencumbered. If anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, each author, individually and collectively, will hold harmless and indemnify IJCAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense IJCAI may make to such claim or action. Moreover, each author agrees to cooperate in any claim or other action seeking to protect or enforce any right the author has granted to IJCAI in the paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, each author agrees to reimburse whomever brings such claim or action for expenses and attorney\&rsquo;s fees incurred therein.   c) \&nbsp;In return for these rights, IJCAI hereby grants to each author, and the employers for whom the work was performed, royalty-free permission to: 1. retain all proprietary rights (such as patent rights) other than copyright and the publication rights transferred to IJCAI; 2. personally reuse all or portions of the paper in other works of their own authorship; 3. make oral presentation of the material in any forum; 4. reproduce, or have reproduced, the  paper for the author\&rsquo;s personal use, or for company use provided that IJCAI copyright and the source are indicated, and that the copies are not used in a way that implies IJCAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the paper in electronic or digital form on any computer network, except by the author or the author\&rsquo;s employer, and then only on the author\&rsquo;s or the employer\&rsquo;s own World Wide Web page or ftp site. Such Web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the IJCAI electronic server (http://www.ijcai.org), and shall not post other IJCAI copyrighted materials not of the author\&rsquo;s or the employer\&rsquo;s creation (including tables of contents with links to other papers) without IJCAI\&rsquo;s written permission; \&gt;5. make limited distribution of all or portions of the above paper prior to publication. 6. In the case of work performed under U.S. Government contract, IJCAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above paper, and to authorize others to do so, for U.S. Government purposes. In the event the above paper is not accepted and published by IJCAI, or is withdrawn by the author(s) before acceptance by IJCAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/view/11277},
	abstract = {Geometry reasoning and proof form a major and challenging component in the K-121 mathematics curriculum. Although several computerized systems exist that help students learn and practice general geometry concepts, they do not target geometry proof problems, which are more advanced and difficult. Powerful geometry theorem provers also exist, however they typically employ advanced algebraic methods and generate complex, difficult to understand proofs, and thus do not meet general K-12 students’ educational needs. This paper tackles these weaknesses of prior systems by introducing a geometry proof system, iGeoTutor, capable of generating human-readable elementary proofs, i.e. proofs using standard Euclidean axioms. We have gathered 77 problems in total from various sources, including ones unsolvable by other systems and from Math competitions. iGeoTutor solves all but two problems in under two minutes each, and more importantly, demonstrates a much more effective and intelligent proof search than prior systems. We have also conducted a pilot study with 12 high school students, and the results show that iGeoTutor provides a clear benefit in helping students learn geometry proofs. We are in active discussions with Khan Academy and local high schools for possible adoption of iGeo-Tutor in real learning environments.},
	language = {en},
	booktitle = {Twenty-{Fourth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Wang, Ke and Su, Zhendong},
	month = jun,
	year = {2015},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/L4TLH3QQ/Wang and Su - 2015 - Automated Geometry Theorem Proving for Human-Reada.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/G2D3E32T/11277.html:text/html}
}

@article{noauthor_automated_nodate,
	title = {Automated {Geometry} {Theorem} {Proving} for {Human}-{Readable} {Proofs}},
	url = {https://www.ijcai.org/Proceedings/15/Papers/172.pdf},
	urldate = {2017-11-21},
	file = {172.pdf:/home/jeremiah/Zotero/storage/PDNNR2JE/172.pdf:application/pdf}
}

@misc{noauthor_learner_nodate,
	title = {Learner answer assessment in {Intelligent} {Tutoring} {Systems} - {ProQuest}},
	url = {https://search-proquest-com.ezp-prod1.hul.harvard.edu/docview/304860619},
	file = {Snapshot:/home/jeremiah/Zotero/storage/3C79JYAJ/304860619.html:text/html}
}

@inproceedings{mendis_automatic_2017,
	title = {Automatic assessment of student answers for geometric theorem proving questions},
	doi = {10.1109/MERCon.2017.7980520},
	abstract = {In this paper, we present a system to automatically assess multi-step answers for geometric theorem proving questions in high school Mathematics. The system is capable of allocating partial marks for steps considering a marking rubric. Moreover, the system evaluates the natural language reasoning part in each step. Currently, 30 theorems related to straight lines have been implemented as inference rules. The system has been tested with 100 student answers for two geometric theorem proving questions.},
	booktitle = {2017 {Moratuwa} {Engineering} {Research} {Conference} ({MERCon})},
	author = {Mendis, C. and Lahiru, D. and Pamudika, N. and Madushanka, S. and Ranathunga, S. and Dias, G.},
	month = may,
	year = {2017},
	keywords = {Geometry, natural language processing, automatic assessment, automatic multistep answer assessment, automatic student answers assessment, Clustering methods, Cognition, educational computing, Engines, geometric theorem proving, geometric theorem proving questions, geometry, high school Mathematics, inference engine, inference mechanisms, inference rules, knowledge based approach, Knowledge based systems, marking rubric, mathematics computing, multi-step answers, natural language reasoning part, Natural languages, partial mark allocation, partial marks, theorem proving},
	pages = {413--418},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/XFC7TS9H/7980520.html:text/html}
}

@article{bridge_machine_2014,
	title = {Machine {Learning} for {First}-{Order} {Theorem} {Proving}},
	volume = {53},
	issn = {0168-7433, 1573-0670},
	url = {https://link-springer-com.ezp-prod1.hul.harvard.edu/article/10.1007/s10817-014-9301-5},
	doi = {10.1007/s10817-014-9301-5},
	abstract = {We applied two state-of-the-art machine learning techniques to the problem of selecting a good heuristic in a first-order theorem prover. Our aim was to demonstrate that sufficient information is available from simple feature measurements of a conjecture and axioms to determine a good choice of heuristic, and that the choice process can be automatically learned. Selecting from a set of 5 heuristics, the learned results are better than any single heuristic. The same results are also comparable to the prover’s own heuristic selection method, which has access to 82 heuristics including the 5 used by our method, and which required additional human expertise to guide its design. One version of our system is able to decline proof attempts. This achieves a significant reduction in total time required, while at the same time causing only a moderate reduction in the number of theorems proved. To our knowledge no earlier system has had this capability.},
	language = {en},
	number = {2},
	journal = {Journal of Automated Reasoning},
	author = {Bridge, James P. and Holden, Sean B. and Paulson, Lawrence C.},
	month = aug,
	year = {2014},
	pages = {141--172},
	file = {Snapshot:/home/jeremiah/Zotero/storage/PPRRGZHL/s10817-014-9301-5.html:text/html}
}

@article{chou_automated_1996,
	title = {Automated generation of readable proofs with geometric invariants},
	volume = {17},
	issn = {0168-7433, 1573-0670},
	url = {https://link.springer.com/article/10.1007/BF00283133},
	doi = {10.1007/BF00283133},
	abstract = {In this series of papers, we discuss how to use a fixed set of high level geometry lemmas or rules related to geometric invariants, such as area, full-angle, etc., to produce short and human-readable proofs in geometry, especially to produce multiple and shortest proofs of a given geometry theorem. These rules are proved to be much more effective and concise than the rules based on triangle congruence used in related work before. The success of our approach is partially due to a skillful selection of geometric invariants and the related rules. Control and search strategies are proposed and experimented with to enhance the efficiency of the pover. In part I of this series, the high level geometry lemmas are about area and the Ceva-Menelaus configurations.},
	language = {en},
	number = {3},
	journal = {Journal of Automated Reasoning},
	author = {Chou, Shang-Ching and Gao, Xiao-Shan and Zhang, Jing-Zhong},
	month = dec,
	year = {1996},
	pages = {325--347},
	file = {Snapshot:/home/jeremiah/Zotero/storage/LEDA4R8A/BF00283133.html:text/html}
}

@book{singhal_automated_2014,
	title = {Automated generation of geometry questions for high school mathematics},
	volume = {2},
	abstract = {We describe a framework that combines a combinatorial approach, pattern matching and automated deduction to generate and solve geometry problems for high school mathematics. Such a system would help teachers to quickly generate large numbers of questions on a geometry topic. Students can explore and revise specific topics covered in classes and textbooks based on generated questions. The system can act as a personalized instructor - it can generate problems that meet users specific weaknesses. This system may also help standardize tests such as GMAT and SAT. Our novel methodology uses (i) a combinatorial approach for generating geometric figures (ii) a pattern matching approach for generating questions and (iii) automated deduction to generate new questions and solutions. By combining these methods, we are able to generate questions involving finding or proving relationships between geometric objects based on a specification of the geometry objects, concepts and theorems to be covered by the questions. Experimental results show that a large number of questions can be generated in a short time. We have tested our generated questions on an existing geometry question solving software JGEX, verifying the validity of the generated questions.},
	author = {Singhal, R and Henz, Martin and McGee, K},
	month = jan,
	year = {2014}
}

@article{hermann_teaching_2015,
	title = {Teaching {Machines} to {Read} and {Comprehend}},
	url = {http://arxiv.org/abs/1506.03340},
	abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
	urldate = {2017-11-21},
	journal = {arXiv:1506.03340 [cs]},
	author = {Hermann, Karl Moritz and Kočiský, Tomáš and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03340},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 14 pages, 13 figures},
	file = {arXiv\:1506.03340 PDF:/home/jeremiah/Zotero/storage/5SXRRDAJ/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/IHVAZX39/1506.html:text/html}
}

@article{wang_bilateral_2017,
	title = {Bilateral {Multi}-{Perspective} {Matching} for {Natural} {Language} {Sentences}},
	url = {http://arxiv.org/abs/1702.03814},
	abstract = {Natural language sentence matching is a fundamental technology for a variety of tasks. Previous approaches either match sentences from a single direction or only apply single granular (word-by-word or sentence-by-sentence) matching. In this work, we propose a bilateral multi-perspective matching (BiMPM) model under the "matching-aggregation" framework. Given two sentences \$P\$ and \$Q\$, our model first encodes them with a BiLSTM encoder. Next, we match the two encoded sentences in two directions \$P {\textbackslash}rightarrow Q\$ and \$P {\textbackslash}leftarrow Q\$. In each matching direction, each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives. Then, another BiLSTM layer is utilized to aggregate the matching results into a fix-length matching vector. Finally, based on the matching vector, the decision is made through a fully connected layer. We evaluate our model on three tasks: paraphrase identification, natural language inference and answer sentence selection. Experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks.},
	urldate = {2017-11-21},
	journal = {arXiv:1702.03814 [cs]},
	author = {Wang, Zhiguo and Hamza, Wael and Florian, Radu},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.03814},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: To appear in Proceedings of IJCAI 2017},
	file = {arXiv\:1702.03814 PDF:/home/jeremiah/Zotero/storage/BWECG8P3/Wang et al. - 2017 - Bilateral Multi-Perspective Matching for Natural L.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/PCFTISYG/1702.html:text/html}
}

@article{srivastava_highway_2015,
	title = {Highway {Networks}},
	url = {http://arxiv.org/abs/1505.00387},
	abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
	urldate = {2017-11-21},
	journal = {arXiv:1505.00387 [cs]},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
	month = may,
	year = {2015},
	note = {arXiv: 1505.00387},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, G.1.6, I.2.6, 68T01},
	annote = {Comment: 6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop. Full paper is at arXiv:1507.06228},
	file = {arXiv\:1505.00387 PDF:/home/jeremiah/Zotero/storage/MHB2IE32/Srivastava et al. - 2015 - Highway Networks.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/L6ZLECW7/1505.html:text/html}
}

@article{parikh_decomposable_2016,
	title = {A {Decomposable} {Attention} {Model} for {Natural} {Language} {Inference}},
	url = {http://arxiv.org/abs/1606.01933},
	abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
	urldate = {2017-11-21},
	journal = {arXiv:1606.01933 [cs]},
	author = {Parikh, Ankur P. and Täckström, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01933},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 7 pages, 1 figure, Proceeedings of EMNLP 2016},
	file = {arXiv\:1606.01933 PDF:/home/jeremiah/Zotero/storage/IN6K9JZM/Parikh et al. - 2016 - A Decomposable Attention Model for Natural Languag.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/X2QLKM23/1606.html:text/html}
}

@article{gong_natural_2017,
	title = {Natural {Language} {Inference} over {Interaction} {Space}},
	url = {http://arxiv.org/abs/1709.04348},
	abstract = {Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20\% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.},
	urldate = {2017-11-21},
	journal = {arXiv:1709.04348 [cs]},
	author = {Gong, Yichen and Luo, Heng and Zhang, Jian},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.04348},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 2 figures, under review as ICLR proceeding}
}

@article{zhang_advances_2017,
	title = {Advances in {Variational} {Inference}},
	url = {http://arxiv.org/abs/1711.05597},
	abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully used in various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
	urldate = {2017-11-20},
	journal = {arXiv:1711.05597 [cs, stat]},
	author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.05597},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1711.05597 PDF:/home/jeremiah/Zotero/storage/XTA843LM/Zhang et al. - 2017 - Advances in Variational Inference.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/UEABZ7J3/1711.html:text/html}
}

@article{sadeghian_tracking_2017,
	title = {Tracking {The} {Untrackable}: {Learning} {To} {Track} {Multiple} {Cues} with {Long}-{Term} {Dependencies}},
	shorttitle = {Tracking {The} {Untrackable}},
	url = {http://arxiv.org/abs/1701.01909},
	abstract = {The majority of existing solutions to the Multi-Target Tracking (MTT) problem do not combine cues in a coherent end-to-end fashion over a long period of time. However, we present an online method that encodes long-term temporal dependencies across multiple cues. One key challenge of tracking methods is to accurately track occluded targets or those which share similar appearance properties with surrounding objects. To address this challenge, we present a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window. We are able to correct many data association errors and recover observations from an occluded state. We demonstrate the robustness of our data-driven approach by tracking multiple targets using their appearance, motion, and even interactions. Our method outperforms previous works on multiple publicly available datasets including the challenging MOT benchmark.},
	urldate = {2017-11-19},
	journal = {arXiv:1701.01909 [cs]},
	author = {Sadeghian, Amir and Alahi, Alexandre and Savarese, Silvio},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.01909},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1701.01909 PDF:/home/jeremiah/Zotero/storage/FXG674ZY/Sadeghian et al. - 2017 - Tracking The Untrackable Learning To Track Multip.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/EFTCBZJM/1701.html:text/html}
}

@article{henschel_novel_2017,
	title = {A {Novel} {Multi}-{Detector} {Fusion} {Framework} for {Multi}-{Object} {Tracking}},
	url = {http://arxiv.org/abs/1705.08314},
	abstract = {In order to track all persons in a scene, the tracking-by-detection paradigm has proven to be a very effective approach. Yet, relying solely on a single detector is also a major limitation, as useful image information might be ignored. This work demonstrates how to incorporate several detectors into a tracking system, using a novel multi-object tracking formulation. We cast tracking as a weighted graph labeling problem, resulting in a binary quadratic program. As such problems are NP-hard, the solution can only be approximated. Based on the Frank-Wolfe algorithm, we present a new solver that is crucial to handle such difficult problems. As a result, the tracker can take information from many frames and different detectors holistically into account. When applied with head and full-body detections, the fusion helps to recover heavily occluded persons and to reduce false positives. Evaluation on pedestrian tracking is provided for multiple scenarios, showing superior results over single detector tracking and standard QP-solvers. Finally, our tracker performs state-of-the-art on the MOT16 benchmark and is the winner of the MOT17 challenge.},
	urldate = {2017-11-19},
	journal = {arXiv:1705.08314 [cs]},
	author = {Henschel, Roberto and Leal-Taixé, Laura and Cremers, Daniel and Rosenhahn, Bodo},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08314},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages, 5 figures; Winner of the MOT17 challenge},
	file = {arXiv\:1705.08314 PDF:/home/jeremiah/Zotero/storage/RS7NTLPH/Henschel et al. - 2017 - A Novel Multi-Detector Fusion Framework for Multi-.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/LPC8GYTQ/1705.html:text/html}
}

@article{leal-taixe_tracking_2017,
	title = {Tracking the {Trackers}: {An} {Analysis} of the {State} of the {Art} in {Multiple} {Object} {Tracking}},
	shorttitle = {Tracking the {Trackers}},
	url = {http://arxiv.org/abs/1704.02781},
	abstract = {Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for research. We present a benchmark for Multiple Object Tracking launched in the late 2014, with the goal of creating a framework for the standardized evaluation of multiple object tracking methods. This paper collects the two releases of the benchmark made so far, and provides an in-depth analysis of almost 50 state-of-the-art trackers that were tested on over 11000 frames. We show the current trends and weaknesses of multiple people tracking methods, and provide pointers of what researchers should be focusing on to push the field forward.},
	urldate = {2017-11-19},
	journal = {arXiv:1704.02781 [cs]},
	author = {Leal-Taixé, Laura and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian and Roth, Stefan},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.02781},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1704.02781 PDF:/home/jeremiah/Zotero/storage/9BSWD4LI/Leal-Taixé et al. - 2017 - Tracking the Trackers An Analysis of the State of.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/IGTF9M2Z/1704.html:text/html}
}

@incollection{jitkrittum_linear-time_2017-1,
	title = {A {Linear}-{Time} {Kernel} {Goodness}-of-{Fit} {Test}},
	url = {http://papers.nips.cc/paper/6630-a-linear-time-kernel-goodness-of-fit-test.pdf},
	urldate = {2017-11-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Jitkrittum, Wittawat and Xu, Wenkai and Szabo, Zoltan and Fukumizu, Kenji and Gretton, Arthur},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {261--270},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/6WYWQ6KS/Jitkrittum et al. - 2017 - A Linear-Time Kernel Goodness-of-Fit Test.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/6KLJVUVI/6630-a-linear-time-kernel-goodness-of-fit-test.html:text/html}
}

@incollection{schulam_what-if_2017,
	title = {What-{If} {Reasoning} using {Counterfactual} {Gaussian} {Processes}},
	url = {http://papers.nips.cc/paper/6767-what-if-reasoning-using-counterfactual-gaussian-processes.pdf},
	urldate = {2017-11-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Schulam, Peter and Saria, Suchi},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {1696--1706},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/96C9A79D/Schulam and Saria - 2017 - What-If Reasoning using Counterfactual Gaussian Pr.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/BHRBBRB2/6767-what-if-reasoning-using-counterfactual-gaussian-processes.html:text/html}
}

@article{tran_hierarchical_2017,
	title = {Hierarchical {Implicit} {Models} and {Likelihood}-{Free} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1702.08896},
	abstract = {Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.},
	urldate = {2017-11-18},
	journal = {arXiv:1702.08896 [cs, stat]},
	author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08896},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Appears in Neural Information Processing Systems, 2017},
	file = {arXiv\:1702.08896 PDF:/home/jeremiah/Zotero/storage/DTCJHIBE/Tran et al. - 2017 - Hierarchical Implicit Models and Likelihood-Free V.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/EIPG53YW/1702.html:text/html}
}

@article{george_generative_2017,
	title = {A generative vision model that trains with high data efficiency and breaks text-based {CAPTCHAs}},
	copyright = {Copyright © 2017, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/early/2017/10/26/science.aag2612},
	doi = {10.1126/science.aag2612},
	abstract = {Learning from few examples and generalizing to dramatically different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing based inference handles recognition, segmentation and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities, and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects like data efficiency and compositionality that may be important in the path toward general artificial intelligence.},
	language = {en},
	journal = {Science},
	author = {George, D. and Lehrach, W. and Kansky, K. and Lázaro-Gredilla, M. and Laan, C. and Marthi, B. and Lou, X. and Meng, Z. and Liu, Y. and Wang, H. and Lavin, A. and Phoenix, D. S.},
	month = oct,
	year = {2017},
	pmid = {29074582},
	pages = {eaag2612},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/6SJJXWPX/George et al. - 2017 - A generative vision model that trains with high da.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/IFHWRNL7/science.html:text/html}
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/350/6266/1332},
	doi = {10.1126/science.aab3050},
	abstract = {Handwritten characters drawn by a model
Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look “right” as judged by Turing-like tests of the model's output in comparison to what real humans produce.
Science, this issue p. 1332
People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.
Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model.
Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model.},
	language = {en},
	number = {6266},
	journal = {Science},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	month = dec,
	year = {2015},
	pmid = {26659050},
	pages = {1332--1338},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/3R44U9CB/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/N85ESBXU/tab-pdf.html:text/html}
}

@article{kingma_improving_2016,
	title = {Improving {Variational} {Inference} with {Inverse} {Autoregressive} {Flow}},
	url = {http://arxiv.org/abs/1606.04934},
	abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
	urldate = {2017-11-17},
	journal = {arXiv:1606.04934 [cs, stat]},
	author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04934},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1606.04934 PDF:/home/jeremiah/Zotero/storage/RYH7RN2H/Kingma et al. - 2016 - Improving Variational Inference with Inverse Autor.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/FPVEGDWZ/1606.html:text/html}
}

@inproceedings{alvaro_offline_2014,
	title = {Offline {Features} for {Classifying} {Handwritten} {Math} {Symbols} with {Recurrent} {Neural} {Networks}},
	doi = {10.1109/ICPR.2014.507},
	abstract = {In mathematical expression recognition, symbol classification is a crucial step. Numerous approaches for recognizing handwritten math symbols have been published, but most of them are either an online approach or a hybrid approach. There is an absence of a study focused on offline features for handwritten math symbol recognition. Furthermore, many papers provide results difficult to compare. In this paper we assess the performance of several well-known offline features for this task. We also test a novel set of features based on polar histograms and the vertical repositioning method for feature extraction. Finally, we report and analyze the results of several experiments using recurrent neural networks on a large public database of online handwritten math expressions. The combination of online and offline features significantly improved the recognition rate.},
	booktitle = {2014 22nd {International} {Conference} on {Pattern} {Recognition}},
	author = {Álvaro, F. and Sánchez, J. A. and Benedí, J. M.},
	month = aug,
	year = {2014},
	keywords = {Training, Feature extraction, Databases, recurrent neural nets, recurrent neural networks, Handwriting recognition, Hidden Markov models, handwritten character recognition, Text recognition, mathematics computing, feature extraction, handwritten math symbol classification, handwritten math symbol recognition, hybrid approach, image classification, mathematical expression recognition, offline features, online approach, online features, polar histograms, Vectors, vertical repositioning method},
	pages = {2944--2949},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/25FJG32B/6977220.html:text/html;IEEE Xplore Full Text PDF:/home/jeremiah/Zotero/storage/8PCZ7TGH/Álvaro et al. - 2014 - Offline Features for Classifying Handwritten Math .pdf:application/pdf}
}

@article{mouchere_advancing_2016,
	title = {Advancing the state of the art for handwritten math recognition: the {CROHME} competitions, 2011–2014},
	volume = {19},
	issn = {1433-2833, 1433-2825},
	shorttitle = {Advancing the state of the art for handwritten math recognition},
	url = {https://link-springer-com.ezp-prod1.hul.harvard.edu/article/10.1007/s10032-016-0263-5},
	doi = {10.1007/s10032-016-0263-5},
	abstract = {The CROHME competitions have helped organize the field of handwritten mathematical expression recognition. This paper presents the evolution of the competition over its first 4 years, and its contributions to handwritten math recognition, and more generally structural pattern recognition research. The competition protocol, evaluation metrics and datasets are presented in detail. Participating systems are analyzed and compared in terms of the central mathematical expression recognition tasks: (1) symbol segmentation, (2) classification of individual symbols, (3) symbol relationships and (4) structural analysis (parsing). The competition led to the development of label graphs, which allow recognition results with conflicting segmentations to be directly compared and quantified using Hamming distances. We introduce structure confusion histograms that provide frequencies for incorrect subgraphs corresponding to ground-truth label subgraphs of a given size and present structure confusion histograms for symbol bigrams (two symbols with a relationship) for CROHME 2014 systems. We provide a novel analysis combining results from competing systems at the level of individual strokes and stroke pairs; this virtual merging of system outputs allows us to more closely examine limitations for current state-of-the-art systems. Datasets along with evaluation and visualization tools produced for the competition are publicly available.},
	language = {en},
	number = {2},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Mouchère, Harold and Zanibbi, Richard and Garain, Utpal and Viard-Gaudin, Christian},
	month = jun,
	year = {2016},
	pages = {173--189},
	file = {Snapshot:/home/jeremiah/Zotero/storage/NWFUXG9T/s10032-016-0263-5.html:text/html}
}

@inproceedings{alvaro_recognition_2011,
	title = {Recognition of {Printed} {Mathematical} {Expressions} {Using} {Two}-{Dimensional} {Stochastic} {Context}-{Free} {Grammars}},
	doi = {10.1109/ICDAR.2011.247},
	abstract = {In this work, a system for recognition of printed mathematical expressions has been developed. Hence, a statistical framework based on two-dimensional stochastic context-free grammars has been defined. This formal framework allows to jointly tackle the segmentation, symbol recognition and structural analysis of a mathematical expression by computing its most probable parsing. In order to test this approach a reproducible and comparable experiment has been carried out over a large publicly available (InftyCDB-1) database. Results are reported using a well-defined global dissimilitude measure. Experimental results show that this technique is able to properly recognize mathematical expressions, and that the structural information improves the symbol recognition step.},
	booktitle = {2011 {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Alvaro, F. and S´nchez, J. A. and Benedi, J. M.},
	month = sep,
	year = {2011},
	keywords = {stochastic processes, Grammar, Databases, Text analysis, handwriting recognition, image segmentation, mathematics computing, mathematical expression recognition, context-free grammars, formal framework, Image recognition, Image segmentation, Performance evaluation, printed mathematical expression recognition, statistical framework, stochastic parsing, structural analysis, symbol recognition, two dimensional stochastic context free grammars},
	pages = {1225--1229},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/LIWDB5V8/6065505.html:text/html}
}

@article{mouchere_advancing_2016-1,
	title = {Advancing the state of the art for handwritten math recognition: the {CROHME} competitions, 2011–2014},
	volume = {19},
	issn = {1433-2833, 1433-2825},
	shorttitle = {Advancing the state of the art for handwritten math recognition},
	url = {https://link.springer.com/article/10.1007/s10032-016-0263-5},
	doi = {10.1007/s10032-016-0263-5},
	abstract = {The CROHME competitions have helped organize the field of handwritten mathematical expression recognition. This paper presents the evolution of the competition over its first 4 years, and its contributions to handwritten math recognition, and more generally structural pattern recognition research. The competition protocol, evaluation metrics and datasets are presented in detail. Participating systems are analyzed and compared in terms of the central mathematical expression recognition tasks: (1) symbol segmentation, (2) classification of individual symbols, (3) symbol relationships and (4) structural analysis (parsing). The competition led to the development of label graphs, which allow recognition results with conflicting segmentations to be directly compared and quantified using Hamming distances. We introduce structure confusion histograms that provide frequencies for incorrect subgraphs corresponding to ground-truth label subgraphs of a given size and present structure confusion histograms for symbol bigrams (two symbols with a relationship) for CROHME 2014 systems. We provide a novel analysis combining results from competing systems at the level of individual strokes and stroke pairs; this virtual merging of system outputs allows us to more closely examine limitations for current state-of-the-art systems. Datasets along with evaluation and visualization tools produced for the competition are publicly available.},
	language = {en},
	number = {2},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Mouchère, Harold and Zanibbi, Richard and Garain, Utpal and Viard-Gaudin, Christian},
	month = jun,
	year = {2016},
	pages = {173--189},
	file = {10.1007s10032-016-0263-5.pdf:/home/jeremiah/Zotero/storage/TETK2U75/10.1007s10032-016-0263-5.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/NC3RH73H/s10032-016-0263-5.html:text/html}
}

@article{julca-aguilar_general_2017,
	title = {A {General} {Framework} for the {Recognition} of {Online} {Handwritten} {Graphics}},
	url = {http://arxiv.org/abs/1709.06389},
	abstract = {We propose a new framework for the recognition of online handwritten graphics. Three main features of the framework are its ability to treat symbol and structural level information in an integrated way, its flexibility with respect to different families of graphics, and means to control the tradeoff between recognition effectiveness and computational cost. We model a graphic as a labeled graph generated from a graph grammar. Non-terminal vertices represent subcomponents, terminal vertices represent symbols, and edges represent relations between subcomponents or symbols. We then model the recognition problem as a graph parsing problem: given an input stroke set, we search for a parse tree that represents the best interpretation of the input. Our graph parsing algorithm generates multiple interpretations (consistent with the grammar) and then we extract an optimal interpretation according to a cost function that takes into consideration the likelihood scores of symbols and structures. The parsing algorithm consists in recursively partitioning the stroke set according to structures defined in the grammar and it does not impose constraints present in some previous works (e.g. stroke ordering). By avoiding such constraints and thanks to the powerful representativeness of graphs, our approach can be adapted to the recognition of different graphic notations. We show applications to the recognition of mathematical expressions and flowcharts. Experimentation shows that our method obtains state-of-the-art accuracy in both applications.},
	urldate = {2017-11-14},
	journal = {arXiv:1709.06389 [cs]},
	author = {Julca-Aguilar, Frank and Mouchère, Harold and Viard-Gaudin, Christian and Hirata, Nina S. T.},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.06389},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to TPAMI},
	file = {arXiv\:1709.06389 PDF:/home/jeremiah/Zotero/storage/CZ7CU2SV/Julca-Aguilar et al. - 2017 - A General Framework for the Recognition of Online .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/T6MSIL9Z/1709.html:text/html}
}

@article{higgins_beta-vae:_2016,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	month = nov,
	year = {2016},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/P8H5MG4Q/Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/XNIYSWNU/forum.html:text/html}
}

@article{cohen_learning_2014,
	title = {Learning the {Irreducible} {Representations} of {Commutative} {Lie} {Groups}},
	url = {http://arxiv.org/abs/1402.4437},
	abstract = {We present a new probabilistic model of compact commutative Lie groups that produces invariant-equivariant and disentangled representations of data. To define the notion of disentangling, we borrow a fundamental principle from physics that is used to derive the elementary particles of a system from its symmetries. Our model employs a newfound Bayesian conjugacy relation that enables fully tractable probabilistic inference over compact commutative Lie groups -- a class that includes the groups that describe the rotation and cyclic translation of images. We train the model on pairs of transformed image patches, and show that the learned invariant representation is highly effective for classification.},
	urldate = {2017-11-14},
	journal = {arXiv:1402.4437 [cs]},
	author = {Cohen, Taco and Welling, Max},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.4437},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1402.4437 PDF:/home/jeremiah/Zotero/storage/ZKXESLAQ/Cohen and Welling - 2014 - Learning the Irreducible Representations of Commut.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/WBSJRMTS/1402.html:text/html}
}

@inproceedings{chen_learning_2017,
	title = {Learning to {Learn} without {Gradient} {Descent} by {Gradient} {Descent}},
	url = {http://proceedings.mlr.press/v70/chen17e.html},
	abstract = {We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they ca...},
	language = {en},
	booktitle = {{PMLR}},
	author = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gómez and Denil, Misha and Lillicrap, Timothy P. and Botvinick, Matt and Freitas, Nando},
	month = jul,
	year = {2017},
	pages = {748--756},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/D43KQH2I/Chen et al. - 2017 - Learning to Learn without Gradient Descent by Grad.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/CLYG6B9M/chen17e.html:text/html}
}

@incollection{eslami_attend_2016,
	title = {Attend, {Infer}, {Repeat}: {Fast} {Scene} {Understanding} with {Generative} {Models}},
	shorttitle = {Attend, {Infer}, {Repeat}},
	url = {http://papers.nips.cc/paper/6230-attend-infer-repeat-fast-scene-understanding-with-generative-models.pdf},
	urldate = {2017-11-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Eslami, S. M. Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and kavukcuoglu, koray and Hinton, Geoffrey E},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {3225--3233},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/C7S6KJXN/Eslami et al. - 2016 - Attend, Infer, Repeat Fast Scene Understanding wi.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/977HIVUC/6230-attend-infer-repeat-fast-scene-understanding-with-generative-models.html:text/html}
}

@article{rethage_wavenet_2017,
	title = {A {Wavenet} for {Speech} {Denoising}},
	url = {http://arxiv.org/abs/1706.07162},
	abstract = {Currently, most speech processing techniques use magnitude spectrograms as front-end and are therefore by default discarding part of the signal: the phase. In order to overcome this limitation, we propose an end-to-end learning method for speech denoising based on Wavenet. The proposed model adaptation retains Wavenet's powerful acoustic modeling capabilities, while significantly reducing its time-complexity by eliminating its autoregressive nature. Specifically, the model makes use of non-causal, dilated convolutions and predicts target fields instead of a single target sample. These modifications make the model highly parallelizable during both training and inference. Furthermore, we propose a novel energy-conserving loss that directly operates on the raw audio level. This loss also considers the quality of the estimated background-noise signal (computed by applying a parameterless operation to the input) during training. This direct link to the input enforces conserving the energy of the signal throughout the pipeline. Both computational and perceptual evaluations indicate that the proposed method is preferred to Wiener filtering, a common method based on processing the magnitude spectrogram.},
	urldate = {2017-11-13},
	journal = {arXiv:1706.07162 [cs]},
	author = {Rethage, Dario and Pons, Jordi and Serra, Xavier},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.07162},
	keywords = {Computer Science - Sound},
	annote = {Comment: Code: https://github.com/drethage/speech-denoising-wavenet - Audio examples: http://jordipons.me/apps/speech-denoising-wavenet/},
	file = {arXiv\:1706.07162 PDF:/home/jeremiah/Zotero/storage/HI8622HT/Rethage et al. - 2017 - A Wavenet for Speech Denoising.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/THNEUXNI/1706.html:text/html}
}

@incollection{zhao_image_2011,
	title = {Image {Parsing} with {Stochastic} {Scene} {Grammar}},
	url = {http://papers.nips.cc/paper/4236-image-parsing-with-stochastic-scene-grammar.pdf},
	urldate = {2017-11-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24},
	publisher = {Curran Associates, Inc.},
	author = {Zhao, Yibiao and Zhu, Song-chun},
	editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
	year = {2011},
	pages = {73--81},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/MDBDUN49/Zhao and Zhu - 2011 - Image Parsing with Stochastic Scene Grammar.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/C3UQFDM4/4236-image-parsing-with-stochastic-scene-grammar.html:text/html}
}

@misc{noauthor_mage_nodate,
	title = {mage parsing via stochastic scene gramma - {Google} {Search}},
	url = {https://www.google.com/search?client=ubuntu&channel=fs&q=mage+parsing+via+stochastic+scene+gramma&ie=utf-8&oe=utf-8},
	urldate = {2017-11-12},
	file = {mage parsing via stochastic scene gramma - Google Search:/home/jeremiah/Zotero/storage/7G89A7KJ/search.html:text/html}
}

@article{lake_building_2016,
	title = {Building {Machines} {That} {Learn} and {Think} {Like} {People}},
	url = {http://arxiv.org/abs/1604.00289},
	abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	urldate = {2017-11-11},
	journal = {arXiv:1604.00289 [cs, stat]},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00289},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: In press at Behavioral and Brain Sciences. Open call for commentary proposals (until Nov. 22, 2016). https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/information/calls-for-commentary/open-calls-for-commentary},
	file = {arXiv\:1604.00289 PDF:/home/jeremiah/Zotero/storage/2C9RSUTY/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/68H877PV/1604.html:text/html}
}

@article{higgins_beta-vae:_2016-1,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	month = nov,
	year = {2016},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/5MIYGMUD/Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/NCFQ9YDB/forum.html:text/html}
}

@misc{noauthor_dark_nodate,
	title = {Dark {Matter}},
	url = {http://vcla.stat.ucla.edu/Projects/DarkMatter/},
	urldate = {2017-11-11},
	file = {Dark Matter:/home/jeremiah/Zotero/storage/PS4NAUVP/DarkMatter.html:text/html}
}

@phdthesis{yu_single_2017,
	title = {Single {View} 3D {Reconstruction} and {Parsing} {Using} {Geometric} {Commonsense} for {Scene} {Understanding} - {eScholarship}},
	url = {https://escholarship.org/uc/item/4sh1z61s},
	abstract = {My thesis studies this topic in three perspective: (1) 3D scene reconstruction to understand the 3D structure of a scene. (2) Geometry and physics reasoning to understand the relationships of objects in a scene. (3) The interaction between human action and objects in a scene.Specifically, the 3D reconstruction builds a unified grammatical framework capable of reconstructing a variety of scene types (e.g., urban, campus, county etc.) from a single input image. The key idea of our approach is to study a novel commonsense reasoning framework that mainly exploits two types of prior knowledges: (i) prior distributions over a single dimension of objects, e.g., that the length of a sedan is about 4.5 meters; (ii) pair-wise relationships between the dimensions of scene entities, e.g., that the length of a sedan is shorter than a bus. These unary or relative geometric knowledge, once extracted, are fairly stable across different types of natural scenes, and are informative for enhancing the understanding of various scenes in both 2D images and 3D world. Methodologically, we propose to construct a hierarchical graph representation as a unified representation of the input image and related geometric knowledge. We formulate these objectives with a unified probabilistic formula and develop a data-driven Monte Carlo method to infer the optimal solution with both bottom-to-up and top-down computations. Results with comparisons on public datasets showed that our method clearly outperforms the alternative methods.For geometry and physics reasoning, we present an approach for scene understanding by reasoning physical stability of objects from point cloud. We utilize a simple observation that, by human design, objects in static scenes should be stable with respect to gravity. This assumption is applicable to all scene categories and poses useful constraints for the plausible interpretations (parses) in scene understanding. Our method consists of two major steps: 1) geometric reasoning: recovering solid 3D volumetric primitives from defective point cloud; and 2) physical reasoning: grouping the unstable primitives to physically stable objects by optimizing the stability and the scene prior. We propose to use a novel disconnectivity graph (DG) to represent the energy landscape and use a Swendsen-Wang Cut (MCMC) method for optimization. In experiments, we demonstrate that the algorithm achieves substantially better performance for i) object segmentation, ii) 3D volumetric recovery of the scene, and iii) better parsing result for scene understanding in comparison to state-of-the-art methods in both public dataset and our own new dataset.Detecting potential dangers in the environment is a fundamental ability of living beings. In order to endure such ability to a robot, my thesis presents an algorithm for detecting potential falling objects, i.e. physically unsafe objects, given an input of 3D point clouds captured by the range sensors. We formulate the falling risk as a probability or a potential that an object may fall given human action or certain natural disturbances, such as earthquake and wind. Our approach differs from traditional object detection paradigm, it first infers hidden and situated "causes (disturbance) of the scene, and then introduces intuitive physical mechanics to predict possible "effects (falls) as consequences of the causes. In particular, we infer a disturbance field by making use of motion capture data as a rich source of common human pose movement. We show that, by applying various disturbance fields, our model achieves a human level recognition rate of potential falling objects on a dataset of challenging and realistic indoor scenes.},
	urldate = {2017-11-11},
	school = {UCLA},
	author = {Yu, Chengcheng},
	month = jan,
	year = {2017},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/BMA98BE8/Yu - 2017 - Single View 3D Reconstruction and Parsing Using Ge.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/9IRQSYDC/4sh1z61s.pdf:application/pdf}
}

@misc{noauthor_qianyi_nodate,
	title = {Qianyi {Zhou}'s {Homepage}},
	url = {http://qianyi.info/scene.html},
	urldate = {2017-11-11},
	file = {Qianyi Zhou's Homepage:/home/jeremiah/Zotero/storage/9DHFKAKP/scene.html:text/html}
}

@article{noauthor_poi:_nodate,
	title = {{POI}: {Multiple} {Object} {Tracking} with {High} {Performance} {Detection} and {Appearance} {Feature}},
	url = {https://arxiv.org/abs/1610.06136}
}

@article{noauthor_group_nodate,
	title = {A {Group} {Contextual} {Model} for {Activity} {Recognition} in {Crowded} {Scenes}}
}

@article{ibrahim_hierarchical_2016,
	title = {Hierarchical {Deep} {Temporal} {Models} for {Group} {Activity} {Recognition}},
	url = {http://arxiv.org/abs/1607.02643},
	abstract = {In this paper we present an approach for classifying the activity performed by a group of people in a video sequence. This problem of group activity recognition can be addressed by examining individual person actions and their relations. Temporal dynamics exist both at the level of individual person actions as well as at the level of group activity. Given a video sequence as input, methods can be developed to capture these dynamics at both person-level and group-level detail. We build a deep model to capture these dynamics based on LSTM (long short-term memory) models. In order to model both person-level and group-level dynamics, we present a 2-stage deep temporal model for the group activity recognition problem. In our approach, one LSTM model is designed to represent action dynamics of individual people in a video sequence and another LSTM model is designed to aggregate person-level information for group activity recognition. We collected a new dataset consisting of volleyball videos labeled with individual and group activities in order to evaluate our method. Experimental results on this new Volleyball Dataset and the standard benchmark Collective Activity Dataset demonstrate the efficacy of the proposed models.},
	urldate = {2017-11-11},
	journal = {arXiv:1607.02643 [cs]},
	author = {Ibrahim, Mostafa S. and Muralidharan, Srikanth and Deng, Zhiwei and Vahdat, Arash and Mori, Greg},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.02643},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1511.06040},
	file = {arXiv\:1607.02643 PDF:/home/jeremiah/Zotero/storage/ZXYH8J2G/Ibrahim et al. - 2016 - Hierarchical Deep Temporal Models for Group Activi.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/ICNXNIU3/1607.html:text/html}
}

@misc{noauthor_datasets_nodate,
	title = {Datasets},
	url = {https://www.cs.utexas.edu/~chaoyeh/web_action_data/dataset_list.html},
	urldate = {2017-11-11},
	file = {:/home/jeremiah/Zotero/storage/MUJXFN5A/dataset_list.html:text/html}
}

@inproceedings{ricci_uncovering_2015,
	title = {Uncovering {Interactions} and {Interactors}: {Joint} {Estimation} of {Head}, {Body} {Orientation} and {F}-{Formations} from {Surveillance} {Videos}},
	shorttitle = {Uncovering {Interactions} and {Interactors}},
	doi = {10.1109/ICCV.2015.529},
	abstract = {We present a novel approach for jointly estimating targets' head, body orientations and conversational groups called F-formations from a distant social scene (e.g., a cocktail party captured by surveillance cameras). Differing from related works that have (i) coupled head and body pose learning by exploiting the limited range of orientations that the two can jointly take, or (ii) determined F-formations based on the mutual head (but not body) orientations of interactors, we present a unified framework to jointly infer both (i) and (ii). Apart from exploiting spatial and orientation relationships, we also integrate cues pertaining to temporal consistency and occlusions, which are beneficial while handling low-resolution data under surveillance settings. Efficacy of the joint inference framework reflects via increased head, body pose and F-formation estimation accuracy over the state-of-the-art, as confirmed by extensive experiments on two social datasets.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Ricci, E. and Varadarajan, J. and Subramanian, R. and Bulò, S. R. and Ahuja, N. and Lanz, O.},
	month = dec,
	year = {2015},
	keywords = {cameras, learning (artificial intelligence), pose estimation, cocktail party, coupled head body pose learning, distant social scene, F-formations, Foot, Head, head-body orientation estimation, image resolution, interaction uncovering, joint inference framework, low-resolution data handling, occlusions, Surveillance, surveillance cameras, surveillance videos, Target tracking, temporal consistency, video surveillance, Videos},
	pages = {4660--4668},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/ILKE67KN/7410886.html:text/html;Ricci_Uncovering_Interactions_and_ICCV_2015_paper.pdf:/home/jeremiah/Zotero/storage/DFRG3U4H/Ricci_Uncovering_Interactions_and_ICCV_2015_paper.pdf:application/pdf}
}

@article{zhao_scene_nodate,
	title = {Scene {Parsing} by {Integrating} {Function}, {Geometry} and {Appearance} {Models}},
	author = {Zhao, Yibiao and Song-chun, Zhu},
	file = {IJCV_Scene_Functionality.pdf:/home/jeremiah/Zotero/storage/QNMLNW5P/IJCV_Scene_Functionality.pdf:application/pdf}
}

@article{jin_real-time_nodate,
	title = {Real-{Time} {Action} {Detection} in {Video} {Surveillance} using {Sub}-{Action} {Descriptor} with {Multi}-{CNN}},
	url = {https://arxiv.org/abs/1710.03383v1},
	author = {Jin, Cheng-bin}
}

@inproceedings{naikal_joint_2014,
	title = {Joint detection and recognition of human actions in wireless surveillance camera networks},
	doi = {10.1109/ICRA.2014.6907554},
	abstract = {Automatic recognition of human actions in video has been a highly addressed problem in robotics and computer vision. Majority of the recent work in literature has focused on classifying pre-segmented video clips, and some progress has also been made on joint detection and recognition of actions in complex video sequences. These methods, however, are not designed for wireless camera networks where the sensors have limited internal processing and communication capabilities. In this paper we present an efficient system for the joint detection and recognition of human actions using a network of wireless smart cameras. The foundation of our work is based on Deformable Part Models (DPMs) for detecting objects in static images. We have extended this framework to the single-view and multi-view video setting to jointly detect and recognize actions. We call this the Deformable Keyframe Model (DKM) and tightly integrate it within a centralized video analysis system. In our system, feature extraction is locally performed on-board wireless smart cameras, and the classification is performed at a base station with higher processing power. Our analysis demonstrates that this decoupling of the the recognition pipeline can significantly minimize the power and bandwidth consumed by the wireless cameras. We experimentally validate our DKMs on two data sets. We first demonstrate the competitiveness of our algorithm by comparing its performance against other state-of-the-art methods, on a publicly available dataset. Then, we extensively validate our system on a novel dataset called the Bosch Multiview Complex Action (BMCA) dataset. Our dataset consists of 11 actions continuously performed by 20 different subjects while being captured by cameras located at 4 different vantage points. In our experiments, we demonstrate that the presence of multiple-views improves the performance of action detection and recognition by about 15\% over the single-view setting.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Naikal, N. and Lajevardi, P. and Sastry, S. S.},
	month = may,
	year = {2014},
	keywords = {Cameras, image sequences, object detection, Feature extraction, Labeling, feature extraction, image classification, Surveillance, video surveillance, action detection, automatic human action recognition, base station, BMCA, Bosch multiview complex action, centralized video analysis system, complex video sequences, deformable keyframe model, Deformable models, deformable part model, DKM, DPM, multiview video, object recognition, on-board wireless smart camera network, presegmented video clip classification, recognition pipeline decoupling, single view video, Spatiotemporal phenomena, static images, video cameras, Wireless communication, wireless sensor networks, wireless surveillance camera network},
	pages = {4747--4754},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/ZD38XKBW/6907554.html:text/html}
}

@article{tu_joint_2013,
	title = {Joint {Video} and {Text} {Parsing} for {Understanding} {Events} and {Answering} {Queries}},
	url = {http://arxiv.org/abs/1308.6628},
	abstract = {We propose a framework for parsing video and text jointly for understanding events and answering user queries. Our framework produces a parse graph that represents the compositional structures of spatial information (objects and scenes), temporal information (actions and events) and causal information (causalities between events and fluents) in the video and text. The knowledge representation of our framework is based on a spatial-temporal-causal And-Or graph (S/T/C-AOG), which jointly models possible hierarchical compositions of objects, scenes and events as well as their interactions and mutual contexts, and specifies the prior probabilistic distribution of the parse graphs. We present a probabilistic generative model for joint parsing that captures the relations between the input video/text, their corresponding parse graphs and the joint parse graph. Based on the probabilistic model, we propose a joint parsing system consisting of three modules: video parsing, text parsing and joint inference. Video parsing and text parsing produce two parse graphs from the input video and text respectively. The joint inference module produces a joint parse graph by performing matching, deduction and revision on the video and text parse graphs. The proposed framework has the following objectives: Firstly, we aim at deep semantic parsing of video and text that goes beyond the traditional bag-of-words approaches; Secondly, we perform parsing and reasoning across the spatial, temporal and causal dimensions based on the joint S/T/C-AOG representation; Thirdly, we show that deep joint parsing facilitates subsequent applications such as generating narrative text descriptions and answering queries in the forms of who, what, when, where and why. We empirically evaluated our system based on comparison against ground-truth as well as accuracy of query answering and obtained satisfactory results.},
	urldate = {2017-11-10},
	journal = {arXiv:1308.6628 [cs]},
	author = {Tu, Kewei and Meng, Meng and Lee, Mun Wai and Choe, Tae Eun and Zhu, Song-Chun},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.6628},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Multimedia},
	file = {arXiv\:1308.6628 PDF:/home/jeremiah/Zotero/storage/GZYJWDPH/Tu et al. - 2013 - Joint Video and Text Parsing for Understanding Eve.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/X6T3MKQ6/1308.html:text/html}
}

@misc{noauthor_dukemtmc_nodate,
	title = {{DukeMTMC}},
	url = {http://vision.cs.duke.edu/DukeMTMC/#systems},
	urldate = {2017-11-10},
	file = {DukeMTMC:/home/jeremiah/Zotero/storage/DTIFB9AK/DukeMTMC.html:text/html}
}

@article{zheng_person_2016,
	title = {Person {Re}-identification: {Past}, {Present} and {Future}},
	shorttitle = {Person {Re}-identification},
	url = {http://arxiv.org/abs/1610.02984},
	abstract = {Person re-identification (re-ID) has become increasingly popular in the community due to its application and research significance. It aims at spotting a person of interest in other cameras. In the early days, hand-crafted algorithms and small-scale evaluation were predominantly reported. Recent years have witnessed the emergence of large-scale datasets and deep learning systems which make use of large data volumes. Considering different tasks, we classify most current re-ID methods into two classes, i.e., image-based and video-based; in both tasks, hand-crafted and deep learning systems will be reviewed. Moreover, two new re-ID tasks which are much closer to real-world applications are described and discussed, i.e., end-to-end re-ID and fast re-ID in very large galleries. This paper: 1) introduces the history of person re-ID and its relationship with image classification and instance retrieval; 2) surveys a broad selection of the hand-crafted systems and the large-scale methods in both image- and video-based re-ID; 3) describes critical future directions in end-to-end re-ID and fast retrieval in large galleries; and 4) finally briefs some important yet under-developed issues.},
	urldate = {2017-11-10},
	journal = {arXiv:1610.02984 [cs]},
	author = {Zheng, Liang and Yang, Yi and Hauptmann, Alexander G.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.02984},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 20 pages, 5 tables, 10 images},
	file = {arXiv\:1610.02984 PDF:/home/jeremiah/Zotero/storage/NIKT9Y5X/Zheng et al. - 2016 - Person Re-identification Past, Present and Future.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/3JYP58J7/1610.html:text/html}
}

@inproceedings{chen_learning_2017-1,
	title = {Learning to {Learn} without {Gradient} {Descent} by {Gradient} {Descent}},
	url = {http://proceedings.mlr.press/v70/chen17e.html},
	abstract = {We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they ca...},
	language = {en},
	booktitle = {{PMLR}},
	author = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gómez and Denil, Misha and Lillicrap, Timothy P. and Botvinick, Matt and Freitas, Nando},
	month = jul,
	year = {2017},
	pages = {748--756},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/2UFMWN7B/Chen et al. - 2017 - Learning to Learn without Gradient Descent by Grad.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/V5WYQHMR/chen17e.html:text/html}
}

@article{shahriari_taking_2016,
	title = {Taking the {Human} {Out} of the {Loop}: {A} {Review} of {Bayesian} {Optimization}},
	volume = {104},
	issn = {0018-9219},
	shorttitle = {Taking the {Human} {Out} of the {Loop}},
	doi = {10.1109/JPROC.2015.2494218},
	abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Shahriari, B. and Swersky, K. and Wang, Z. and Adams, R. P. and Freitas, N. de},
	month = jan,
	year = {2016},
	keywords = {Bayes methods, Bayesian optimization, Big data, Big Data, Big data application, decision making, Decision making, design of experiments, Design of experiments, Genomes, genomic medicine, human productivity, large-scale heterogeneous computing, Linear programming, massive complex software system, optimisation, optimization, Optimization, product quality, response surface methodology, Statistical analysis, statistical learning, storage allocation, storage architecture},
	pages = {148--175},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/8B8V2LR7/7352306.html:text/html;IEEE Xplore Full Text PDF:/home/jeremiah/Zotero/storage/DUI3K7MQ/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf:application/pdf}
}

@article{barbour_steins_1992,
	title = {Stein's method and point process approximation},
	volume = {43},
	issn = {0304-4149},
	doi = {10.1016/0304-4149(92)90073-Y},
	abstract = {The Stein-Chen method for Poisson approximation is adapted into a form suitable for obtaining error estimates for the approximation of the whole distribution of a point process on a suitable topological space by that of a Poisson process. The adaptation involves consideration of an immigration-death process on the topological space, whose equilibrium distribution is that of the approximating Poisson process; the Stein equation has a simple interpretation in terms of the generator of the immigration-death process. The error estimates for process approximation in total variation do not have the ‘magic’ Stein-Chein multiplying constants, which for univariate approximation tend to zero as the mean gets larger, but examples, including Bernoulli trials and the hard-core model on the torus, show that this is not possible. By choosing weaker metrics on the space of distributions of point processes, it is possible to reintroduce these constants. The proofs actually yield an improved estimate for one of the constants in the univariate case.},
	language = {eng},
	number = {1},
	journal = {Stochastic Processes and their Applications},
	author = {Barbour, A. D. and Brown, T. C.},
	year = {1992},
	keywords = {60g55, Coupling, Hard-Core Model, Immigration-Death Process, Palm Probability, Point Process, Poisson Approximation, Stein-Chen Method},
	pages = {9--31}
}

@article{ranganath_operator_2016-2,
	title = {Operator {Variational} {Inference}},
	url = {http://arxiv.org/abs/1610.09033},
	abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
	urldate = {2017-11-07},
	journal = {arXiv:1610.09033 [cs, stat]},
	author = {Ranganath, Rajesh and Altosaar, Jaan and Tran, Dustin and Blei, David M.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.09033},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Appears in Neural Information Processing Systems, 2016},
	file = {arXiv\:1610.09033 PDF:/home/jeremiah/Zotero/storage/W555N28L/Ranganath et al. - 2016 - Operator Variational Inference.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/YGGYJTZK/1610.html:text/html}
}

@article{tran_hierarchical_2017-1,
	title = {Hierarchical {Implicit} {Models} and {Likelihood}-{Free} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1702.08896},
	abstract = {Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.},
	urldate = {2017-11-07},
	journal = {arXiv:1702.08896 [cs, stat]},
	author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08896},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Appears in Neural Information Processing Systems, 2017},
	file = {arXiv\:1702.08896 PDF:/home/jeremiah/Zotero/storage/ULJLFHEE/Tran et al. - 2017 - Hierarchical Implicit Models and Likelihood-Free V.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/H28LJMPU/1702.html:text/html}
}

@book{berlinet_reproducing_2003,
	address = {Boston},
	edition = {2004 edition},
	title = {Reproducing {Kernel} {Hilbert} {Spaces} in {Probability} and {Statistics}},
	isbn = {978-1-4020-7679-4},
	abstract = {The book covers theoretical questions including the latest extension of the formalism, and computational issues and focuses on some of the more fruitful and promising applications, including statistical signal processing, nonparametric curve estimation, random measures, limit theorems, learning theory and some applications at the fringe between Statistics and Approximation Theory. It is geared to graduate students in Statistics, Mathematics or Engineering, or to scientists with an equivalent level.},
	language = {English},
	publisher = {Springer},
	author = {Berlinet, Alain and Thomas-Agnan, Christine},
	month = dec,
	year = {2003}
}

@article{muandet_kernel_2017,
	title = {Kernel {Mean} {Embedding} of {Distributions}: {A} {Review} and {Beyond}},
	volume = {10},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Kernel {Mean} {Embedding} of {Distributions}},
	url = {http://www.nowpublishers.com/article/Details/MAL-060},
	doi = {10.1561/2200000060},
	abstract = {Kernel Mean Embedding of Distributions: A Review and Beyond},
	language = {English},
	number = {1-2},
	urldate = {2017-11-06},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	month = jun,
	year = {2017},
	pages = {1--141},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/X5XNFL3Q/Muandet et al. - 2017 - Kernel Mean Embedding of Distributions A Review a.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/I6KGJZYD/MAL-060.html:text/html}
}

@inproceedings{cuturi_fast_2014,
	title = {Fast {Computation} of {Wasserstein} {Barycenters}},
	url = {http://proceedings.mlr.press/v32/cuturi14.html},
	abstract = {We present new algorithms to compute the mean of a set of N empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter {\textbackslash}citepagueh2011barycent...},
	language = {en},
	urldate = {2017-11-03},
	booktitle = {{PMLR}},
	author = {Cuturi, Marco and Doucet, Arnaud},
	month = jan,
	year = {2014},
	pages = {685--693},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/K83AIGL5/Cuturi and Doucet - 2014 - Fast Computation of Wasserstein Barycenters.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/5IN7RIVR/cuturi14.html:text/html}
}

@techreport{bentley_problem_1978,
	title = {A {Problem} in {Multivariate} {Statistics}: {Algorithm}, {Data} {Structure} and {Applications}.},
	shorttitle = {A {Problem} in {Multivariate} {Statistics}},
	url = {http://www.dtic.mil/docs/citations/ADA055818},
	abstract = {Problems and applications are investigated which are associated with computing the empirical cumulative distribution function of N points in k-dimensional space and a multidimensional divide-and-conquer technique is employed that gives rise to a compact data structure for geometric and statistical search problems. A large number of important statistical quantities are computed much faster than was previously possible.},
	number = {CMU-CS-78-110},
	institution = {CARNEGIE-MELLON UNIV PITTSBURGH PA DEPT OF COMPUTER SCIENCE, CARNEGIE-MELLON UNIV PITTSBURGH PA DEPT OF COMPUTER SCIENCE},
	author = {Bentley, Jon Louis and Shamos, Michael Ian},
	month = apr,
	year = {1978},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/7PSFGQZX/Bentley and Shamos - 1978 - A Problem in Multivariate Statistics Algorithm, D.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/52DNYNCR/ADA055818.html:text/html}
}

@article{frogner_learning_2015,
	title = {Learning with a {Wasserstein} {Loss}},
	url = {http://arxiv.org/abs/1506.05439},
	abstract = {Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.},
	urldate = {2017-11-03},
	journal = {arXiv:1506.05439 [cs, stat]},
	author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya-Polo, Mauricio and Poggio, Tomaso},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.05439},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: NIPS 2015; v3 updates Algorithm 1 and Equations 6, 8},
	file = {arXiv\:1506.05439 PDF:/home/jeremiah/Zotero/storage/ESERHBWL/Frogner et al. - 2015 - Learning with a Wasserstein Loss.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/KIAVBZDY/1506.html:text/html}
}

@article{sejdinovic_equivalence_2013-1,
	title = {Equivalence of distance-based and {RKHS}-based statistics in hypothesis testing},
	volume = {41},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1383661264},
	doi = {10.1214/13-AOS1140},
	abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies (MMD), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the MMD corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the MMD as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
	language = {EN},
	number = {5},
	journal = {The Annals of Statistics},
	author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
	month = oct,
	year = {2013},
	mrnumber = {MR3127866},
	zmnumber = {1281.62117},
	keywords = {distance covariance, independence testing, Reproducing kernel Hilbert spaces, two-sample testing},
	pages = {2263--2291},
	file = {Snapshot:/home/jeremiah/Zotero/storage/GB8B7GTH/1383661264.html:text/html}
}

@incollection{gretton_optimal_2012,
	title = {Optimal kernel choice for large-scale two-sample tests},
	url = {http://papers.nips.cc/paper/4727-optimal-kernel-choice-for-large-scale-two-sample-tests.pdf},
	urldate = {2017-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Gretton, Arthur and Sejdinovic, Dino and Strathmann, Heiko and Balakrishnan, Sivaraman and Pontil, Massimiliano and Fukumizu, Kenji and Sriperumbudur, Bharath K.},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1205--1213},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/F4FHH6Q3/Gretton et al. - 2012 - Optimal kernel choice for large-scale two-sample t.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/AQ4PNYS9/4727-optimal-kernel-choice-for-large-scale-two-sample-tests.html:text/html}
}

@incollection{fukumizu_kernel_2008,
	title = {Kernel {Measures} of {Conditional} {Dependence}},
	url = {http://papers.nips.cc/paper/3340-kernel-measures-of-conditional-dependence.pdf},
	urldate = {2017-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Schölkopf, Prof. Bernhard},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {489--496},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/5RFGRFT6/Fukumizu et al. - 2008 - Kernel Measures of Conditional Dependence.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/YVM5Q8VX/3340-kernel-measures-of-conditional-dependence.html:text/html}
}

@incollection{fukumizu_kernel_2008-1,
	title = {Kernel {Measures} of {Conditional} {Dependence}},
	url = {http://papers.nips.cc/paper/3340-kernel-measures-of-conditional-dependence.pdf},
	urldate = {2017-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Schölkopf, Prof. Bernhard},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {489--496},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/PTLTATZW/Fukumizu et al. - 2008 - Kernel Measures of Conditional Dependence.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/RZLIYKBS/3340-kernel-measures-of-conditional-dependence.html:text/html}
}

@incollection{giordano_linear_2015,
	title = {Linear {Response} {Methods} for {Accurate} {Covariance} {Estimates} from {Mean} {Field} {Variational} {Bayes}},
	url = {http://papers.nips.cc/paper/5755-linear-response-methods-for-accurate-covariance-estimates-from-mean-field-variational-bayes.pdf},
	urldate = {2017-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Giordano, Ryan J and Broderick, Tamara and Jordan, Michael I},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {1441--1449},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/H3W2566N/Giordano et al. - 2015 - Linear Response Methods for Accurate Covariance Es.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/AW49BBXZ/5755-linear-response-methods-for-accurate-covariance-estimates-from-mean-field-variational-baye.html:text/html}
}

@article{sriperumbudur_hilbert_2009,
	title = {Hilbert space embeddings and metrics on probability measures},
	url = {http://arxiv.org/abs/0907.5309},
	abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be defined as the distance between distribution embeddings: we denote this as \${\textbackslash}gamma\_k\$, indexed by the kernel function \$k\$ that defines the inner product in the RKHS. We present three theoretical properties of \${\textbackslash}gamma\_k\$. First, we consider the question of determining the conditions on the kernel \$k\$ for which \${\textbackslash}gamma\_k\$ is a metric: such \$k\$ are denoted \{{\textbackslash}em characteristic kernels\}. Unlike pseudometrics, a metric is zero only when two distributions coincide, thus ensuring the RKHS embedding maps all distributions uniquely (i.e., the embedding is injective). While previously published conditions may apply only in restricted circumstances (e.g. on compact domains), and are difficult to check, our conditions are straightforward and intuitive: bounded continuous strictly positive definite kernels are characteristic. Alternatively, if a bounded continuous kernel is translation-invariant on \${\textbackslash}bb\{R\}{\textasciicircum}d\$, then it is characteristic if and only if the support of its Fourier transform is the entire \${\textbackslash}bb\{R\}{\textasciicircum}d\$. Second, we show that there exist distinct distributions that are arbitrarily close in \${\textbackslash}gamma\_k\$. Third, to understand the nature of the topology induced by \${\textbackslash}gamma\_k\$, we relate \${\textbackslash}gamma\_k\$ to other popular metrics on probability measures, and present conditions on the kernel \$k\$ under which \${\textbackslash}gamma\_k\$ metrizes the weak topology.},
	urldate = {2017-11-02},
	journal = {arXiv:0907.5309 [math, stat]},
	author = {Sriperumbudur, Bharath K. and Gretton, Arthur and Fukumizu, Kenji and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
	month = jul,
	year = {2009},
	note = {arXiv: 0907.5309},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory},
	annote = {Comment: 48 pages},
	file = {arXiv\:0907.5309 PDF:/home/jeremiah/Zotero/storage/ENIZFEUH/Sriperumbudur et al. - 2009 - Hilbert space embeddings and metrics on probabilit.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/CA3QKRCJ/0907.html:text/html}
}

@incollection{graves_offline_2009,
	title = {Offline {Handwriting} {Recognition} with {Multidimensional} {Recurrent} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.pdf},
	urldate = {2017-11-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 21},
	publisher = {Curran Associates, Inc.},
	author = {Graves, Alex and Schmidhuber, Juergen},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
	pages = {545--552},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/ZEG6GWMV/Graves and Schmidhuber - 2009 - Offline Handwriting Recognition with Multidimensio.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/K9B6REGI/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.html:text/html}
}

@article{zhang_drawing_2016,
	title = {Drawing and {Recognizing} {Chinese} {Characters} with {Recurrent} {Neural} {Network}},
	url = {http://arxiv.org/abs/1606.06539},
	abstract = {Recent deep learning based approaches have achieved great success on handwriting recognition. Chinese characters are among the most widely adopted writing systems in the world. Previous research has mainly focused on recognizing handwritten Chinese characters. However, recognition is only one aspect for understanding a language, another challenging and interesting task is to teach a machine to automatically write (pictographic) Chinese characters. In this paper, we propose a framework by using the recurrent neural network (RNN) as both a discriminative model for recognizing Chinese characters and a generative model for drawing (generating) Chinese characters. To recognize Chinese characters, previous methods usually adopt the convolutional neural network (CNN) models which require transforming the online handwriting trajectory into image-like representations. Instead, our RNN based approach is an end-to-end system which directly deals with the sequential structure and does not require any domain-specific knowledge. With the RNN system (combining an LSTM and GRU), state-of-the-art performance can be achieved on the ICDAR-2013 competition database. Furthermore, under the RNN framework, a conditional generative model with character embedding is proposed for automatically drawing recognizable Chinese characters. The generated characters (in vector format) are human-readable and also can be recognized by the discriminative RNN model with high accuracy. Experimental results verify the effectiveness of using RNNs as both generative and discriminative models for the tasks of drawing and recognizing Chinese characters.},
	urldate = {2017-11-01},
	journal = {arXiv:1606.06539 [cs]},
	author = {Zhang, Xu-Yao and Yin, Fei and Zhang, Yan-Ming and Liu, Cheng-Lin and Bengio, Yoshua},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.06539},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1606.06539 PDF:/home/jeremiah/Zotero/storage/A824G7Q9/Zhang et al. - 2016 - Drawing and Recognizing Chinese Characters with Re.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/8QMVHFRT/1606.html:text/html}
}

@misc{noauthor_chinese_nodate,
	title = {chinese handwriting generation - {Google} {Search}},
	url = {https://www.google.com/search?client=ubuntu&channel=fs&q=chinese+handwriting+generation&ie=utf-8&oe=utf-8},
	urldate = {2017-11-01},
	file = {chinese handwriting generation - Google Search:/home/jeremiah/Zotero/storage/U99YIGSQ/search.html:text/html}
}

@misc{noauthor_icdar2013_nodate,
	title = {{ICDAR}2013 - {Handwriting} {Stroke} {Recovery} from {Offline} {Data}},
	url = {https://www.kaggle.com/c/icdar2013-stroke-recovery-from-offline-data},
	abstract = {Predict the trajectory of a handwritten signature},
	file = {Snapshot:/home/jeremiah/Zotero/storage/478WRSR5/icdar2013-stroke-recovery-from-offline-data.html:text/html}
}

@article{zhang_online_2016,
	title = {Online and {Offline} {Handwritten} {Chinese} {Character} {Recognition}: {A} {Comprehensive} {Study} and {New} {Benchmark}},
	shorttitle = {Online and {Offline} {Handwritten} {Chinese} {Character} {Recognition}},
	url = {http://arxiv.org/abs/1606.05763},
	abstract = {Recent deep learning based methods have achieved the state-of-the-art performance for handwritten Chinese character recognition (HCCR) by learning discriminative representations directly from raw data. Nevertheless, we believe that the long-and-well investigated domain-specific knowledge should still help to boost the performance of HCCR. By integrating the traditional normalization-cooperated direction-decomposed feature map (directMap) with the deep convolutional neural network (convNet), we are able to obtain new highest accuracies for both online and offline HCCR on the ICDAR-2013 competition database. With this new framework, we can eliminate the needs for data augmentation and model ensemble, which are widely used in other systems to achieve their best results. This makes our framework to be efficient and effective for both training and testing. Furthermore, although directMap+convNet can achieve the best results and surpass human-level performance, we show that writer adaptation in this case is still effective. A new adaptation layer is proposed to reduce the mismatch between training and test data on a particular source layer. The adaptation process can be efficiently and effectively implemented in an unsupervised manner. By adding the adaptation layer into the pre-trained convNet, it can adapt to the new handwriting styles of particular writers, and the recognition accuracy can be further improved consistently and significantly. This paper gives an overview and comparison of recent deep learning based approaches for HCCR, and also sets new benchmarks for both online and offline HCCR.},
	urldate = {2017-11-01},
	journal = {arXiv:1606.05763 [cs]},
	author = {Zhang, Xu-Yao and Bengio, Yoshua and Liu, Cheng-Lin},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.05763},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1606.05763 PDF:/home/jeremiah/Zotero/storage/PV6NVBSP/Zhang et al. - 2016 - Online and Offline Handwritten Chinese Character R.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/6FZCX83K/1606.html:text/html}
}

@article{stuner_lv-rover:_2017,
	title = {{LV}-{ROVER}: {Lexicon} {Verified} {Recognizer} {Output} {Voting} {Error} {Reduction}},
	shorttitle = {{LV}-{ROVER}},
	url = {http://arxiv.org/abs/1707.07432},
	abstract = {Offline handwritten text line recognition is a hard task that requires both an efficient optical character recognizer and language model. Handwriting recognition state of the art methods are based on Long Short Term Memory (LSTM) recurrent neural networks (RNN) coupled with the use of linguistic knowledge. Most of the proposed approaches in the literature focus on improving one of the two components and use constraint, dedicated to a database lexicon. However, state of the art performance is achieved by combining multiple optical models, and possibly multiple language models with the Recognizer Output Voting Error Reduction (ROVER) framework. Though handwritten line recognition with ROVER has been implemented by combining only few recognizers because training multiple complete recognizers is hard. In this paper we propose a Lexicon Verified ROVER: LV-ROVER, that has a reduce complexity compare to the original one and that can combine hundreds of recognizers without language models. We achieve state of the art for handwritten line text on the RIMES dataset.},
	urldate = {2017-11-01},
	journal = {arXiv:1707.07432 [cs]},
	author = {Stuner, Bruno and Chatelain, Clément and Paquet, Thierry},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.07432},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to Pattern Recognition Letters},
	file = {arXiv\:1707.07432 PDF:/home/jeremiah/Zotero/storage/EJBILY3H/Stuner et al. - 2017 - LV-ROVER Lexicon Verified Recognizer Output Votin.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/PLMGEW5D/1707.html:text/html}
}

@article{moysset_learning_2016,
	title = {Learning to detect and localize many objects from few examples},
	url = {http://arxiv.org/abs/1611.05664},
	abstract = {The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing.},
	urldate = {2017-11-01},
	journal = {arXiv:1611.05664 [cs]},
	author = {Moysset, Bastien and Kermorvant, Christoper and Wolf, Christian},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05664},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv\:1611.05664 PDF:/home/jeremiah/Zotero/storage/BCBTCF56/Moysset et al. - 2016 - Learning to detect and localize many objects from .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/R3I42HH6/1611.html:text/html}
}

@article{moysset_full-page_2017,
	title = {Full-{Page} {Text} {Recognition}: {Learning} {Where} to {Start} and {When} to {Stop}},
	shorttitle = {Full-{Page} {Text} {Recognition}},
	url = {http://arxiv.org/abs/1704.08628},
	abstract = {Text line detection and localization is a crucial step for full page document analysis, but still suffers from heterogeneity of real life documents. In this paper, we present a new approach for full page text recognition. Localization of the text lines is based on regressions with Fully Convolutional Neural Networks and Multidimensional Long Short-Term Memory as contextual layers. In order to increase the efficiency of this localization method, only the position of the left side of the text lines are predicted. The text recognizer is then in charge of predicting the end of the text to recognize. This method has shown good results for full page text recognition on the highly heterogeneous Maurdor dataset.},
	urldate = {2017-11-01},
	journal = {arXiv:1704.08628 [cs]},
	author = {Moysset, Bastien and Kermorvant, Christopher and Wolf, Christian},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.08628},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1704.08628 PDF:/home/jeremiah/Zotero/storage/PD25JIYQ/Moysset et al. - 2017 - Full-Page Text Recognition Learning Where to Star.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/SCFCJ8VD/1704.html:text/html}
}

@article{moysset_learning_2016-1,
	title = {Learning to detect and localize many objects from few examples},
	url = {http://arxiv.org/abs/1611.05664},
	abstract = {The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing.},
	urldate = {2017-11-01},
	journal = {arXiv:1611.05664 [cs]},
	author = {Moysset, Bastien and Kermorvant, Christoper and Wolf, Christian},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05664},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv\:1611.05664 PDF:/home/jeremiah/Zotero/storage/R4ZURE58/Moysset et al. - 2016 - Learning to detect and localize many objects from .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/6JJLV9XQ/1611.html:text/html}
}

@misc{noauthor_theodore_nodate,
	title = {Theodore {Bluche} - {Research} {Engineer}},
	url = {http://tbluche.com/phd.html},
	urldate = {2017-11-01},
	file = {Theodore Bluche - Research Engineer:/home/jeremiah/Zotero/storage/NKQPTNZ3/phd.html:text/html}
}

@article{bluche_joint_2016,
	title = {Joint {Line} {Segmentation} and {Transcription} for {End}-to-{End} {Handwritten} {Paragraph} {Recognition}},
	url = {http://arxiv.org/abs/1604.08352},
	abstract = {Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and transcript at line level is costly to obtain. On the other hand, automatic line segmentation algorithms are prone to errors, compromising the subsequent recognition. In this paper, we propose a modification of the popular and efficient multi-dimensional long short-term memory recurrent neural networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More particularly, we replace the collapse layer transforming the two-dimensional representation into a sequence of predictions by a recurrent version which can recognize one line at a time. In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image representation. The experiments on paragraphs of Rimes and IAM database yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents.},
	urldate = {2017-11-01},
	journal = {arXiv:1604.08352 [cs]},
	author = {Bluche, Théodore},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.08352},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1604.08352 PDF:/home/jeremiah/Zotero/storage/TX5UA597/Bluche - 2016 - Joint Line Segmentation and Transcription for End-.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/5U6CFCX5/1604.html:text/html}
}

@article{bluche_scan_2016,
	title = {Scan, {Attend} and {Read}: {End}-to-{End} {Handwritten} {Paragraph} {Recognition} with {MDLSTM} {Attention}},
	shorttitle = {Scan, {Attend} and {Read}},
	url = {http://arxiv.org/abs/1604.03286},
	abstract = {We present an attention-based model for end-to-end handwriting recognition. Our system does not require any segmentation of the input paragraph. The model is inspired by the differentiable attention models presented recently for speech recognition, image captioning or translation. The main difference is the covert and overt attention, implemented as a multi-dimensional LSTM network. Our principal contribution towards handwriting recognition lies in the automatic transcription without a prior segmentation into lines, which was crucial in previous approaches. To the best of our knowledge this is the first successful attempt of end-to-end multi-line handwriting recognition. We carried out experiments on the well-known IAM Database. The results are encouraging and bring hope to perform full paragraph transcription in the near future.},
	urldate = {2017-11-01},
	journal = {arXiv:1604.03286 [cs]},
	author = {Bluche, Théodore and Louradour, Jérôme and Messina, Ronaldo},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.03286},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1604.03286 PDF:/home/jeremiah/Zotero/storage/ISYJHMBG/Bluche et al. - 2016 - Scan, Attend and Read End-to-End Handwritten Para.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/9HBNTKCU/1604.html:text/html}
}

@article{xia_multiple-index_2008,
	title = {A {Multiple}-{Index} {Model} and {Dimension} {Reduction}},
	volume = {103},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1198/016214508000000805},
	doi = {10.1198/016214508000000805},
	abstract = {Dimension reduction can be used as an initial step in statistical modeling. Further specification of model structure is imminent and important when the reduced dimension is still greater than 1. In this article we investigate one method of specification that involves separating the linear component from the nonlinear components, leading to further dimension reduction in the unknown link function and, thus, better estimation and easier interpretation of the model. The specified model includes the popular econometric multiple-index model and the partially linear single-index model as its special cases. A criterion is developed to validate the model specification. An algorithm is proposed to estimate the model directly. Asymptotic distributions for the estimators of the parameters and the nonparametric link function are derived. Air pollution data in Chicago are used to illustrate the modeling procedure and to demonstrate its advantages over the existing dimension reduction approaches.},
	number = {484},
	urldate = {2017-10-30},
	journal = {Journal of the American Statistical Association},
	author = {Xia, Yingcun},
	month = dec,
	year = {2008},
	keywords = {Asymptotic distribution, Convergence of algorithm, Dimension reduction, Local linear smoother, Semiparametric model},
	pages = {1631--1640},
	file = {Snapshot:/home/jeremiah/Zotero/storage/US2Y3QVK/016214508000000805.html:text/html}
}

@article{george_generative_2017-1,
	title = {A generative vision model that trains with high data efficiency and breaks text-based {CAPTCHAs}},
	copyright = {Copyright © 2017, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/early/2017/10/26/science.aag2612},
	doi = {10.1126/science.aag2612},
	abstract = {Learning from few examples and generalizing to dramatically different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing based inference handles recognition, segmentation and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities, and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects like data efficiency and compositionality that may be important in the path toward general artificial intelligence.},
	language = {en},
	journal = {Science},
	author = {George, D. and Lehrach, W. and Kansky, K. and Lázaro-Gredilla, M. and Laan, C. and Marthi, B. and Lou, X. and Meng, Z. and Liu, Y. and Wang, H. and Lavin, A. and Phoenix, D. S.},
	month = oct,
	year = {2017},
	pmid = {29074582},
	pages = {eaag2612},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/A7TC5F2F/George et al. - 2017 - A generative vision model that trains with high da.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/WDFQJUQK/science.html:text/html}
}

@article{flachaire_bootstrapping_2005,
	series = {2nd {CSDA} {Special} {Issue} on {Computational} {Econometrics}},
	title = {Bootstrapping heteroskedastic regression models: wild bootstrap vs. pairs bootstrap},
	volume = {49},
	issn = {0167-9473},
	shorttitle = {Bootstrapping heteroskedastic regression models},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947304001537},
	doi = {10.1016/j.csda.2004.05.018},
	abstract = {In regression models, appropriate bootstrap methods for inference robust to heteroskedasticity of unknown form are the wild bootstrap and the pairs bootstrap. The finite sample performance of a heteroskedastic-robust test is investigated with Monte Carlo experiments. The simulation results suggest that one specific version of the wild bootstrap outperforms the other versions of the wild bootstrap and of the pairs bootstrap. It is the only one for which the bootstrap test always gives better results than the asymptotic test.},
	number = {2},
	urldate = {2018-03-08},
	journal = {Computational Statistics \& Data Analysis},
	author = {Flachaire, Emmanuel},
	month = apr,
	year = {2005},
	keywords = {Heteroskedasticity-robust test, Monte Carlo simulations, Pairs bootstrap, Wild bootstrap},
	pages = {361--376}
}

@article{fazayeli_matrix_2016,
	title = {The {Matrix} {Generalized} {Inverse} {Gaussian} {Distribution}: {Properties} and {Applications}},
	shorttitle = {The {Matrix} {Generalized} {Inverse} {Gaussian} {Distribution}},
	url = {http://arxiv.org/abs/1604.03463},
	abstract = {While the Matrix Generalized Inverse Gaussian (\${\textbackslash}mathcal\{MGIG\}\$) distribution arises naturally in some settings as a distribution over symmetric positive semi-definite matrices, certain key properties of the distribution and effective ways of sampling from the distribution have not been carefully studied. In this paper, we show that the \${\textbackslash}mathcal\{MGIG\}\$ is unimodal, and the mode can be obtained by solving an Algebraic Riccati Equation (ARE) equation [7]. Based on the property, we propose an importance sampling method for the \${\textbackslash}mathcal\{MGIG\}\$ where the mode of the proposal distribution matches that of the target. The proposed sampling method is more efficient than existing approaches [32, 33], which use proposal distributions that may have the mode far from the \${\textbackslash}mathcal\{MGIG\}\$'s mode. Further, we illustrate that the the posterior distribution in latent factor models, such as probabilistic matrix factorization (PMF) [25], when marginalized over one latent factor has the \${\textbackslash}mathcal\{MGIG\}\$ distribution. The characterization leads to a novel Collapsed Monte Carlo (CMC) inference algorithm for such latent factor models. We illustrate that CMC has a lower log loss or perplexity than MCMC, and needs fewer samples.},
	urldate = {2018-03-19},
	journal = {arXiv:1604.03463 [stat]},
	author = {Fazayeli, Farideh and Banerjee, Arindam},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.03463},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: Updated Figure 3},
	file = {arXiv\:1604.03463 PDF:/home/jeremiah/Zotero/storage/7TS584D6/Fazayeli and Banerjee - 2016 - The Matrix Generalized Inverse Gaussian Distributi.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/984RK4JA/1604.html:text/html}
}

@article{pourzanjani_general_2017,
	title = {General {Bayesian} {Inference} over the {Stiefel} {Manifold} via the {Givens} {Transform}},
	url = {http://arxiv.org/abs/1710.09443},
	abstract = {We introduce the Givens Transform, a novel transform between the space of orthonormal matrices and \${\textbackslash}mathbb\{R\}{\textasciicircum}D\$. The Givens Transform allows for the application of any general Bayesian inference algorithm to probabilistic models containing constrained unit-vectors or orthonormal matrix parameters. This includes a variety of matrix factorizations and dimensionality reduction models such as Probabilistic PCA (PPCA), Exponential Family PPCA (BXPCA), and Canonical Correlation Analysis (CCA). While previous Bayesian approaches to these models relied on separate sampling update rules for constrained and unconstrained parameters, the Givens Transform enables the treatment of unit-vectors and orthonormal matrices agnostically as unconstrained parameters. Thus any Bayesian inference algorithm can be used on these models without modification. This opens the door to not just sampling algorithms, but Variational Inference (VI) as well. We illustrate with several examples and supplied code, how the Givens Transform allows end-users to easily build complex models in their favorite Bayesian modeling framework such as Stan, Edward, or PyMC3, a task that was previously intractable due to technical constraints.},
	urldate = {2018-03-28},
	journal = {arXiv:1710.09443 [stat]},
	author = {Pourzanjani, Arya A. and Jiang, Richard M. and Mitchell, Brian and Atzberger, Paul J. and Petzold, Linda R.},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.09443},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1710.09443 PDF:/home/jeremiah/Zotero/storage/97LG5R59/Pourzanjani et al. - 2017 - General Bayesian Inference over the Stiefel Manifo.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/YP33DBH2/1710.html:text/html}
}

@inproceedings{blundell_weight_2015-1,
	title = {Weight {Uncertainty} in {Neural} {Network}},
	url = {http://proceedings.mlr.press/v37/blundell15.html},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularis...},
	language = {en},
	urldate = {2018-03-28},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = jun,
	year = {2015},
	pages = {1613--1622},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/LDJDEPG5/Blundell et al. - 2015 - Weight Uncertainty in Neural Network.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/S75ZENL2/blundell15.html:text/html}
}

@article{louizos_structured_2016-1,
	title = {Structured and {Efficient} {Variational} {Deep} {Learning} with {Matrix} {Gaussian} {Posteriors}},
	url = {http://arxiv.org/abs/1603.04733},
	abstract = {We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian {\textbackslash}cite\{gupta1999matrix\} parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the "local reprarametrization trick" {\textbackslash}cite\{kingma2015variational\} on this posterior distribution we arrive at a Gaussian Process {\textbackslash}cite\{rasmussen2006gaussian\} interpretation of the hidden units in each layer and we, similarly with {\textbackslash}cite\{gal2015dropout\}, provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate "pseudo-data" {\textbackslash}cite\{snelson2005sparse\} in our model, which in turn allows for more efficient sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments.},
	urldate = {2018-03-28},
	journal = {arXiv:1603.04733 [cs, stat]},
	author = {Louizos, Christos and Welling, Max},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04733},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: Updated results with the original folds in the regression experiments. Appearing in the International Conference on Machine Learning (ICML) 2016},
	file = {arXiv\:1603.04733 PDF:/home/jeremiah/Zotero/storage/9BISR9I7/Louizos and Welling - 2016 - Structured and Efficient Variational Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/7P26CW37/1603.html:text/html}
}

@article{pham_efficient_2018,
	title = {Efficient {Neural} {Architecture} {Search} via {Parameter} {Sharing}},
	url = {http://arxiv.org/abs/1802.03268},
	abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.},
	urldate = {2018-03-29},
	journal = {arXiv:1802.03268 [cs, stat]},
	author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.03268},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1802.03268 PDF:/home/jeremiah/Zotero/storage/PT7SLVEQ/Pham et al. - 2018 - Efficient Neural Architecture Search via Parameter.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/59D4R9VJ/1802.html:text/html}
}

@article{liu_progressive_2017,
	title = {Progressive {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1712.00559},
	abstract = {We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.},
	urldate = {2018-03-29},
	journal = {arXiv:1712.00559 [cs, stat]},
	author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.00559},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: The code and checkpoint for the PNAS model trained on ImageNet can now be downloaded from https://github.com/tensorflow/models/tree/master/research/slim\#Pretrained},
	file = {arXiv\:1712.00559 PDF:/home/jeremiah/Zotero/storage/YNN5U5HB/Liu et al. - 2017 - Progressive Neural Architecture Search.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/FYPN5T65/1712.html:text/html}
}

@article{durante_note_2016,
	title = {A note on the multiplicative gamma process},
	url = {http://arxiv.org/abs/1610.03408},
	abstract = {Adaptive dimensionality reduction in high-dimensional problems is a key topic in statistics. The multiplicative gamma process takes a relevant step in this direction, but improved studies on its properties are required to ease implementation. This note addresses such aim.},
	urldate = {2018-03-29},
	journal = {arXiv:1610.03408 [stat]},
	author = {Durante, Daniele},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.03408},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1610.03408 PDF:/home/jeremiah/Zotero/storage/RJBVE3Q7/Durante - 2016 - A note on the multiplicative gamma process.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MZYMU5ZC/1610.html:text/html}
}

@article{durante_note_2017,
	title = {A note on the multiplicative gamma process},
	volume = {122},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/S016771521630253X},
	doi = {10.1016/j.spl.2016.11.014},
	abstract = {Adaptive dimensionality reduction in high-dimensional problems is a key topic in statistics. The multiplicative gamma process takes a relevant step in this direction, but improved studies on its properties are required to ease implementation. This note addresses such aim.},
	urldate = {2018-03-29},
	journal = {Statistics \& Probability Letters},
	author = {Durante, Daniele},
	month = mar,
	year = {2017},
	keywords = {Multiplicative gamma process, Matrix factorization, Shrinkage prior, Stochastic order},
	pages = {198--204},
	file = {ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/GAAEXECP/S016771521630253X.html:text/html}
}

@article{roychowdhury_gamma_2014,
	title = {Gamma {Processes}, {Stick}-{Breaking}, and {Variational} {Inference}},
	url = {http://arxiv.org/abs/1410.1068},
	abstract = {While most Bayesian nonparametric models in machine learning have focused on the Dirichlet process, the beta process, or their variants, the gamma process has recently emerged as a useful nonparametric prior in its own right. Current inference schemes for models involving the gamma process are restricted to MCMC-based methods, which limits their scalability. In this paper, we present a variational inference framework for models involving gamma process priors. Our approach is based on a novel stick-breaking constructive definition of the gamma process. We prove correctness of this stick-breaking process by using the characterization of the gamma process as a completely random measure (CRM), and we explicitly derive the rate measure of our construction using Poisson process machinery. We also derive error bounds on the truncation of the infinite process required for variational inference, similar to the truncation analyses for other nonparametric models based on the Dirichlet and beta processes. Our representation is then used to derive a variational inference algorithm for a particular Bayesian nonparametric latent structure formulation known as the infinite Gamma-Poisson model, where the latent variables are drawn from a gamma process prior with Poisson likelihoods. Finally, we present results for our algorithms on nonnegative matrix factorization tasks on document corpora, and show that we compare favorably to both sampling-based techniques and variational approaches based on beta-Bernoulli priors.},
	urldate = {2018-03-29},
	journal = {arXiv:1410.1068 [cs, stat]},
	author = {Roychowdhury, Anirban and Kulis, Brian},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.1068},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1410.1068 PDF:/home/jeremiah/Zotero/storage/EU6BHIVD/Roychowdhury and Kulis - 2014 - Gamma Processes, Stick-Breaking, and Variational I.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/FKXPM5GI/1410.html:text/html}
}

@article{nadarajah_exact_2011,
	title = {Exact distribution of the product of m gamma and n {Pareto} random variables},
	volume = {235},
	issn = {0377-0427},
	url = {http://www.sciencedirect.com/science/article/pii/S0377042711002123},
	doi = {10.1016/j.cam.2011.04.018},
	abstract = {Exact expressions are derived for the probability density function (pdf), cumulative distribution function (cdf), shape of the pdf, asymptotics of the pdf and the cdf, Laplace transform, moment properties and the order statistics properties of the product of m independent gamma and n independent Pareto random variables. Computer programs are provided for computing the probability density function and the associated percentage points. Estimation issues by the methods of moments and maximum likelihood are discussed.},
	number = {15},
	urldate = {2018-03-29},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Nadarajah, Saralees},
	month = jun,
	year = {2011},
	keywords = {Gamma distribution, Meijer  function, Pareto distribution, Product of random variables},
	pages = {4496--4512},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/9R5T4PIE/Nadarajah - 2011 - Exact distribution of the product of m gamma and n.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/BTMJSQZC/S0377042711002123.html:text/html}
}

@article{withers_product_2013,
	title = {On the product of gamma random variables},
	volume = {47},
	issn = {0033-5177, 1573-7845},
	url = {https://link-springer-com.ezp-prod1.hul.harvard.edu/article/10.1007/s11135-011-9474-5},
	doi = {10.1007/s11135-011-9474-5},
	abstract = {We give expressions for the distribution and density of a product of gamma or equivalently chi-square random variables. In particular, we give the distribution of the product of two independent gamma variables of mean k in terms of the Bessel functions K 1, … , K k .},
	language = {en},
	number = {1},
	urldate = {2018-03-29},
	journal = {Quality \& Quantity},
	author = {Withers, Christopher S. and Nadarajah, Saralees},
	month = jan,
	year = {2013},
	pages = {545--552},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/L5T3TA6X/Withers and Nadarajah - 2013 - On the product of gamma random variables.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/THNPG6XN/s11135-011-9474-5.html:text/html}
}

@article{louizos_multiplicative_2017,
	title = {Multiplicative {Normalizing} {Flows} for {Variational} {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.01961},
	abstract = {We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.},
	urldate = {2018-03-30},
	journal = {arXiv:1703.01961 [cs, stat]},
	author = {Louizos, Christos and Welling, Max},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.01961},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {Comment: Appearing at the International Conference on Machine Learning (ICML) 2017},
	file = {arXiv\:1703.01961 PDF:/home/jeremiah/Zotero/storage/LD55DHQR/Louizos and Welling - 2017 - Multiplicative Normalizing Flows for Variational B.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/S7KVKR6K/1703.html:text/html}
}

@article{rahman_investigating_2017,
	title = {Investigating causal relation between prenatal arsenic exposure and birthweight: {Are} smaller infants more susceptible?},
	volume = {108},
	issn = {0160-4120},
	shorttitle = {Investigating causal relation between prenatal arsenic exposure and birthweight},
	url = {http://www.sciencedirect.com/science/article/pii/S0160412017301940},
	doi = {10.1016/j.envint.2017.07.026},
	abstract = {Background
Shortening of gestation and intrauterine growth restriction (IUGR) are the two main determinants of birthweight. Low birthweight has been linked with prenatal arsenic exposure, but the causal relation between arsenic and birthweight is not well understood.
Objectives
We applied a quantile causal mediation analysis approach to determine the association between prenatal arsenic exposure and birthweight in relation to shortening of gestation and IUGR, and whether the susceptibility of arsenic exposure varies by infant birth sizes.
Methods
In a longitudinal birth cohort in Bangladesh, we measured arsenic in drinking water (n=1182) collected at enrollment and maternal toenails (n=1104) collected ≤1-month postpartum using inductively coupled plasma mass spectrometry. Gestational age was determined using ultrasound at ≤16weeks' gestation. Demographic information was collected using a structured questionnaire.
Results
Of 1184 singleton livebirths, 16.4\% (n=194) were low birthweight ({\textless}2500g), 21.9\% (n=259) preterm ({\textless}37weeks' gestation), and 9.2\% (n=109) both low birthweight and preterm. The median concentrations of arsenic in drinking water and maternal toenails were 2.2μg/L (range: below the level of detection [LOD]−1400) and 1.2μg/g (range: {\textless}LOD−46.6), respectively. Prenatal arsenic exposure was negatively associated with birthweight, where the magnitude of the association varied across birthweight percentiles. The effect of arsenic on birthweight mediated via shortening of gestation affected all infants irrespective of birth sizes (β range: 10th percentile=−19.7g [95\% CI: −26.7, −13.3] to 90th percentile=−10.9g [95\% CI: −18.5, −5.9] per natural log water arsenic increase), whereas the effect via pathways independent of gestational age affected only the smaller infants (β range: 10th percentile=−28.0g [95\% CI: −43.8, −9.9] to 20th percentile=−14.9g [95\% CI: −30.3, −1.7] per natural log water arsenic increase). Similar pattern was observed for maternal toenail arsenic.
Conclusions
The susceptibility of prenatal arsenic exposure varied by infant birth sizes, placing smaller infants at greater risk of lower birthweight by shortening of gestation and possibly growth restriction. It is important to mitigate prenatal arsenic exposure to improve perinatal outcomes in Bangladesh.},
	urldate = {2018-03-30},
	journal = {Environment International},
	author = {Rahman, Mohammad L. and Valeri, Linda and Kile, Molly L. and Mazumdar, Maitreyi and Mostofa, Golam and Qamruzzaman, Qazi and Rahman, Mahmudur and Baccarelli, Andrea and Liang, Liming and Hauser, Russ and Christiani, David C.},
	month = nov,
	year = {2017},
	pages = {32--40},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/FSQCXWMG/Rahman et al. - 2017 - Investigating causal relation between prenatal ars.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/G5PYJ2UX/S0160412017301940.html:text/html}
}

@techreport{unicef_2016_2016,
	title = {2016 {Global} {Nutrition} {Report}},
	url = {//data.unicef.org/resources/2016-global-nutrition-report/},
	language = {en-US},
	urldate = {2018-03-30},
	author = {UNICEF},
	month = jun,
	year = {2016},
	file = {Snapshot:/home/jeremiah/Zotero/storage/9ZHJQ96F/2016-global-nutrition-report.html:text/html}
}

@inproceedings{cortes_generalization_2010,
	title = {Generalization {Bounds} for {Learning} {Kernels}},
	url = {http://www.cs.nyu.edu/ mohri/pub/lk.pdf},
	booktitle = {Proceedings of the 27th {Annual} {International} {Conference} on {Machine} {Learning} ({ICML} 2010)},
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	year = {2010}
}

@incollection{cortes_learning_2013,
	title = {Learning {Kernels} {Using} {Local} {Rademacher} {Complexity}},
	url = {http://papers.nips.cc/paper/4896-learning-kernels-using-local-rademacher-complexity.pdf},
	urldate = {2018-03-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Cortes, Corinna and Kloft, Marius and Mohri, Mehryar},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {2760--2768},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/65H4JLNS/Cortes et al. - 2013 - Learning Kernels Using Local Rademacher Complexity.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/658SFUQK/4896-learning-kernels-using-local-rademacher-complexity.html:text/html}
}

@article{mendelson_performance_2003,
	title = {On the {Performance} of {Kernel} {Classes}},
	volume = {4},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v4/mendelson03a.html},
	number = {Oct},
	urldate = {2018-03-31},
	journal = {Journal of Machine Learning Research},
	author = {Mendelson, Shahar},
	year = {2003},
	pages = {759--771},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/8M8LS8KE/Mendelson - 2003 - On the Performance of Kernel Classes.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/8ACAN6N7/mendelson03a.html:text/html}
}

@article{knutson_honeycombs_2000,
	title = {Honeycombs and sums of {Hermitian} matrices},
	volume = {48},
	abstract = {Horn's conjecture, which given the spectra of two Hermitian matrices describes the possible spectra of the sum, was recently settled in the affirmative. In this survey we discuss one of the many steps in this, which required us to introduce a combinatorial gadget called a \{{\textbackslash}em honeycomb\}; the question is then reformulable as about the existence of honeycombs with certain boundary conditions. Another important tool is the connection to the representation theory of the group U(n), by ``classical vs. quantum'' analogies. Comment: 16 pages, 14 figures, submitted, Notices Amer. Math. Soc},
	journal = {Notices Amer. Math. Soc.},
	author = {Knutson, Allen and Tao, Terence},
	month = sep,
	year = {2000}
}

@article{koltchinskii_local_2006,
	title = {Local {Rademacher} complexities and oracle inequalities in risk minimization},
	volume = {34},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1179935055},
	doi = {10.1214/009053606000001019},
	abstract = {Let ℱ be a class of measurable functions f:S↦[0, 1] defined on a probability space (S, , P). Given a sample (X1, …, Xn) of i.i.d. random variables taking values in S with common distribution P, let Pn denote the empirical measure based on (X1, …, Xn). We study an empirical risk minimization problem Pnf→min , f∈ℱ. Given a solution f̂n of this problem, the goal is to obtain very general upper bounds on its excess risk expressed in terms of relevant geometric parameters of the class ℱ. Using concentration inequalities and other empirical processes tools, we obtain both distribution-dependent and data-dependent upper bounds on the excess risk that are of asymptotically correct order in many examples. The bounds involve localized sup-norms of empirical and Rademacher processes indexed by functions from the class. We use these bounds to develop model selection techniques in abstract risk minimization problems that can be applied to more specialized frameworks of regression and classification.},
	language = {EN},
	number = {6},
	urldate = {2018-03-31},
	journal = {The Annals of Statistics},
	author = {Koltchinskii, Vladimir},
	month = dec,
	year = {2006},
	mrnumber = {MR2329442},
	zmnumber = {1118.62065},
	keywords = {classification, concentration inequalities, empirical risk minimization, model selection, oracle inequalities, Rademacher complexities},
	pages = {2593--2656},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/Y24XW8L5/Koltchinskii - 2006 - Local Rademacher complexities and oracle inequalit.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/VCGL9BQH/1179935055.html:text/html}
}

@article{van_der_laan_unified_2003,
	title = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}: {Finite} {Sample} {Oracle} {Inequalities} and {Examples}},
	shorttitle = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}},
	url = {http://biostats.bepress.com/ucbbiostat/paper130},
	journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {van der Laan, Mark and Dudoit, Sandrine},
	month = nov,
	year = {2003},
	file = {"Unified Cross-Validation Methodology For Selection Among Estimators an" by Mark J. van der Laan and Sandrine Dudoit:/home/jeremiah/Zotero/storage/LQ6VXG5N/paper130.html:text/html}
}

@article{yuan_non-negative_2007,
	title = {On the non-negative garrotte estimator},
	volume = {69},
	issn = {1369-7412},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00581.x},
	doi = {10.1111/j.1467-9868.2007.00581.x},
	number = {2},
	urldate = {2018-03-31},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Yuan, Ming and Lin, Yi},
	month = mar,
	year = {2007},
	keywords = {Elastic net, Lasso, Least angle regression selection, Non‐negative garrotte, Path consistency, Piecewise linear solution path},
	pages = {143--161},
	file = {Snapshot:/home/jeremiah/Zotero/storage/WVPDAK9G/j.1467-9868.2007.00581.html:text/html}
}

@article{hansen_jackknife_2012,
	title = {Jackknife model averaging},
	volume = {167},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407611002405},
	doi = {10.1016/j.jeconom.2011.06.019},
	abstract = {We consider the problem of obtaining appropriate weights for averaging M approximate (misspecified) models for improved estimation of an unknown conditional mean in the face of non-nested model uncertainty in heteroskedastic error settings. We propose a “jackknife model averaging” (JMA) estimator which selects the weights by minimizing a cross-validation criterion. This criterion is quadratic in the weights, so computation is a simple application of quadratic programming. We show that our estimator is asymptotically optimal in the sense of achieving the lowest possible expected squared error. Monte Carlo simulations and an illustrative application show that JMA can achieve significant efficiency gains over existing model selection and averaging methods in the presence of heteroskedasticity.},
	number = {1},
	urldate = {2018-03-31},
	journal = {Journal of Econometrics},
	author = {Hansen, Bruce E. and Racine, Jeffrey S.},
	month = mar,
	year = {2012},
	pages = {38--46},
	file = {ScienceDirect Full Text PDF:/home/jeremiah/Zotero/storage/BD2DR9VW/Hansen and Racine - 2012 - Jackknife model averaging.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremiah/Zotero/storage/9C5XWSRP/S0304407611002405.html:text/html}
}

@techreport{lin_study_2003,
	title = {A {Study} on {Sigmoid} {Kernels} for {SVM} and the {Training} of non-{PSD} {Kernels} by {SMO}-type {Methods}},
	author = {Lin, Hsuan-tien and Lin, Chih-Jen},
	year = {2003}
}

@article{weyl_asymptotische_1912,
	title = {Das asymptotische {Verteilungsgesetz} der {Eigenwerte} linearer partieller {Differentialgleichungen} (mit einer {Anwendung} auf die {Theorie} der {Hohlraumstrahlung})},
	volume = {71},
	issn = {0025-5831, 1432-1807},
	url = {https://link.springer.com/article/10.1007/BF01456804},
	doi = {10.1007/BF01456804},
	language = {de},
	number = {4},
	urldate = {2018-04-01},
	journal = {Mathematische Annalen},
	author = {Weyl, Hermann},
	month = dec,
	year = {1912},
	pages = {441--479},
	file = {Snapshot:/home/jeremiah/Zotero/storage/5I8DRVAW/BF01456804.html:text/html}
}

@misc{tao_254a_2010,
	title = {254A, {Notes} 3a: {Eigenvalues} and sums of {Hermitian} matrices},
	shorttitle = {254A, {Notes} 3a},
	url = {https://terrytao.wordpress.com/2010/01/12/254a-notes-3a-eigenvalues-and-sums-of-hermitian-matrices/},
	abstract = {Let \$latex \{A\}\&fg=000000\$ be a Hermitian \$latex \{n {\textbackslash}times n\}\&fg=000000\$ matrix. By the spectral theorem for Hermitian matrices (which, for sake of completeness, we prove below), one can dia…},
	language = {en},
	urldate = {2018-04-01},
	journal = {What's new},
	author = {Tao, Terence},
	month = jan,
	year = {2010},
	file = {Snapshot:/home/jeremiah/Zotero/storage/FHJYQLLQ/254a-notes-3a-eigenvalues-and-sums-of-hermitian-matrices.html:text/html}
}

@article{boonstra_small-sample_2015,
	title = {A {Small}-{Sample} {Choice} of the {Tuning} {Parameter} in {Ridge} {Regression}},
	volume = {25},
	issn = {1017-0405},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4790465/},
	doi = {10.5705/ss.2013.284},
	abstract = {We propose new approaches for choosing the shrinkage parameter in ridge regression, a penalized likelihood method for regularizing linear regression coefficients, when the number of observations is small relative to the number of parameters. Existing methods may lead to extreme choices of this parameter, which will either not shrink the coefficients enough or shrink them by too much. Within this “small-n, large-p” context, we suggest a correction to the common generalized cross-validation (GCV) method that preserves the asymptotic optimality of the original GCV. We also introduce the notion of a “hyperpenalty”, which shrinks the shrinkage parameter itself, and make a specific recommendation regarding the choice of hyperpenalty that empirically works well in a broad range of scenarios. A simple algorithm jointly estimates the shrinkage parameter and regression coefficients in the hyperpenalized likelihood. In a comprehensive simulation study of small-sample scenarios, our proposed approaches offer superior prediction over nine other existing methods.},
	number = {3},
	urldate = {2018-04-01},
	journal = {Statistica Sinica},
	author = {Boonstra, Philip S. and Mukherjee, Bhramar and Taylor, Jeremy M. G.},
	month = jul,
	year = {2015},
	pmid = {26985140},
	pmcid = {PMC4790465},
	pages = {1185--1206},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/LB5FPJQS/Boonstra et al. - 2015 - A Small-Sample Choice of the Tuning Parameter in R.pdf:application/pdf}
}

@article{wahba_comparison_1985,
	title = {A {Comparison} of {GCV} and {GML} for {Choosing} the {Smoothing} {Parameter} in the {Generalized} {Spline} {Smoothing} {Problem}},
	volume = {13},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176349743},
	doi = {10.1214/aos/1176349743},
	abstract = {The partially improper prior behind the smoothing spline model is used to obtain a generalization of the maximum likelihood (GML) estimate for the smoothing parameter. Then this estimate is compared with the generalized cross validation (GCV) estimate both analytically and by Monte Carlo methods. The comparison is based on a predictive mean square error criteria. It is shown that if the true, unknown function being estimated is smooth in a sense to be defined then the GML estimate undersmooths relative to the GCV estimate and the predictive mean square error using the GML estimate goes to zero at a slower rate than the mean square error using the GCV estimate. If the true function is "rough" then the GCV and GML estimates have asymptotically similar behavior. A Monte Carlo experiment was designed to see if the asymptotic results in the smooth case were evident in small sample sizes. Mixed results were obtained for n=32n=32n = 32, GCV was somewhat better than GML for n=64n=64n = 64, and GCV was decidedly superior for n=128n=128n = 128. In the n=32n=32n = 32 case GCV was better for smaller σ2σ2{\textbackslash}sigma{\textasciicircum}2 and the comparison close for larger σ2σ2{\textbackslash}sigma{\textasciicircum}2. The theoretical results are shown to extend to the generalized spline smoothing model, which includes the estimate of functions given noisy values of various integrals of them.},
	language = {EN},
	number = {4},
	urldate = {2018-04-01},
	journal = {The Annals of Statistics},
	author = {Wahba, Grace},
	month = dec,
	year = {1985},
	mrnumber = {MR811498},
	zmnumber = {0596.65004},
	keywords = {cross validation, integral equations, maximum likelihood, Spline smoothing},
	pages = {1378--1402},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/SHIKG7WK/Wahba - 1985 - A Comparison of GCV and GML for Choosing the Smoot.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/A8YSXVV6/1176349743.html:text/html}
}

@inproceedings{damianou_deep_2013,
	title = {Deep {Gaussian} {Processes}},
	url = {http://proceedings.mlr.press/v31/damianou13a.html},
	abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inpu...},
	language = {en},
	urldate = {2018-04-01},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Damianou, Andreas and Lawrence, Neil},
	month = apr,
	year = {2013},
	pages = {207--215},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/QU7N9LM6/Damianou and Lawrence - 2013 - Deep Gaussian Processes.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/CPK3QPJA/damianou13a.html:text/html}
}

@article{braun_accurate_2006-1,
	title = {Accurate {Error} {Bounds} for the {Eigenvalues} of the {Kernel} {Matrix}},
	volume = {7},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v7/braun06a.html},
	number = {Nov},
	urldate = {2018-04-01},
	journal = {Journal of Machine Learning Research},
	author = {Braun, Mikio L.},
	year = {2006},
	pages = {2303--2328},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/RAB9WUN9/Braun - 2006 - Accurate Error Bounds for the Eigenvalues of the K.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/N8IYQCF2/braun06a.html:text/html}
}

@article{gittens_tail_2011,
	title = {Tail bounds for all eigenvalues of a sum of random matrices},
	url = {http://arxiv.org/abs/1104.4513},
	abstract = {This work introduces the minimax Laplace transform method, a modification of the cumulant-based matrix Laplace transform method developed in "User-friendly tail bounds for sums of random matrices" (arXiv:1004.4389v6) that yields both upper and lower bounds on each eigenvalue of a sum of random self-adjoint matrices. This machinery is used to derive eigenvalue analogues of the classical Chernoff, Bennett, and Bernstein bounds. Two examples demonstrate the efficacy of the minimax Laplace transform. The first concerns the effects of column sparsification on the spectrum of a matrix with orthonormal rows. Here, the behavior of the singular values can be described in terms of coherence-like quantities. The second example addresses the question of relative accuracy in the estimation of eigenvalues of the covariance matrix of a random process. Standard results on the convergence of sample covariance matrices provide bounds on the number of samples needed to obtain relative accuracy in the spectral norm, but these results only guarantee relative accuracy in the estimate of the maximum eigenvalue. The minimax Laplace transform argument establishes that if the lowest eigenvalues decay sufficiently fast, on the order of (K{\textasciicircum}2*r*log(p))/eps{\textasciicircum}2 samples, where K is the condition number of an optimal rank-r approximation to C, are sufficient to ensure that the dominant r eigenvalues of the covariance matrix of a N(0, C) random vector are estimated to within a factor of 1+-eps with high probability.},
	urldate = {2018-04-01},
	journal = {arXiv:1104.4513 [math]},
	author = {Gittens, Alex and Tropp, Joel A.},
	month = apr,
	year = {2011},
	note = {arXiv: 1104.4513},
	keywords = {Mathematics - Probability, 60B20 (Primary), 60G50 (Secondary)},
	annote = {Comment: 20 pages, 1 figure, see also arXiv:1004.4389v6},
	file = {arXiv\:1104.4513 PDF:/home/jeremiah/Zotero/storage/GE84RD3K/Gittens and Tropp - 2011 - Tail bounds for all eigenvalues of a sum of random.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/QQDB8WAG/1104.html:text/html}
}

@article{tropp_user-friendly_2012,
	title = {User-{Friendly} {Tail} {Bounds} for {Sums} of {Random} {Matrices}},
	volume = {12},
	issn = {1615-3375, 1615-3383},
	url = {https://link.springer.com/article/10.1007/s10208-011-9099-z},
	doi = {10.1007/s10208-011-9099-z},
	abstract = {This paper presents new probability inequalities for sums of independent, random, self-adjoint matrices. These results place simple and easily verifiable hypotheses on the summands, and they deliver strong conclusions about the large-deviation behavior of the maximum eigenvalue of the sum. Tail bounds for the norm of a sum of random rectangular matrices follow as an immediate corollary. The proof techniques also yield some information about matrix-valued martingales.In other words, this paper provides noncommutative generalizations of the classical bounds associated with the names Azuma, Bennett, Bernstein, Chernoff, Hoeffding, and McDiarmid. The matrix inequalities promise the same diversity of application, ease of use, and strength of conclusion that have made the scalar inequalities so valuable.},
	language = {en},
	number = {4},
	urldate = {2018-04-01},
	journal = {Foundations of Computational Mathematics},
	author = {Tropp, Joel A.},
	month = aug,
	year = {2012},
	pages = {389--434},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/93QSBLMN/Tropp - 2012 - User-Friendly Tail Bounds for Sums of Random Matri.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/FBCLFQKY/s10208-011-9099-z.html:text/html}
}

@article{santin_approximation_2014,
	title = {Approximation of {Eigenfunctions} in {Kernel}-based {Spaces}},
	url = {http://arxiv.org/abs/1411.7656},
	abstract = {Kernel-based methods in Numerical Analysis have the advantage of yielding optimal recovery processes in the "native" Hilbert space \${\textbackslash}calh\$ in which they are reproducing. Continuous kernels on compact domains have an expansion into eigenfunctions that are both \$L\_2\$-orthonormal and orthogonal in \${\textbackslash}calh\$ (Mercer expansion). This paper examines the corresponding eigenspaces and proves that they have optimality properties among all other subspaces of \${\textbackslash}calh\$. These results have strong connections to \$n\$-widths in Approximation Theory, and they establish that errors of optimal approximations are closely related to the decay of the eigenvalues. Though the eigenspaces and eigenvalues are not readily available, they can be well approximated using the standard \$n\$-dimensional subspaces spanned by translates of the kernel with respect to \$n\$ nodes or centers. We give error bounds for the numerical approximation of the eigensystem via such subspaces. A series of examples shows that our numerical technique via a greedy point selection strategy allows to calculate the eigensystems with good accuracy.},
	urldate = {2018-04-01},
	journal = {arXiv:1411.7656 [math]},
	author = {Santin, Gabriele and Schaback, Robert},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.7656},
	keywords = {Mathematics - Numerical Analysis},
	file = {arXiv\:1411.7656 PDF:/home/jeremiah/Zotero/storage/VZFTK58L/Santin and Schaback - 2014 - Approximation of Eigenfunctions in Kernel-based Sp.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/4DXWA76W/1411.html:text/html}
}

@article{santin_approximation_2016,
	title = {Approximation of eigenfunctions in kernel-based spaces},
	volume = {42},
	issn = {1019-7168, 1572-9044},
	url = {https://link.springer.com/article/10.1007/s10444-015-9449-5},
	doi = {10.1007/s10444-015-9449-5},
	abstract = {Kernel-based methods in Numerical Analysis have the advantage of yielding optimal recovery processes in the “native” Hilbert space H{\textbackslash}mathcal \{H\} in which they are reproducing. Continuous kernels on compact domains have an expansion into eigenfunctions that are both L 2-orthonormal and orthogonal in H{\textbackslash}mathcal \{H\} (Mercer expansion). This paper examines the corresponding eigenspaces and proves that they have optimality properties among all other subspaces of H{\textbackslash}mathcal \{H\}. These results have strong connections to n-widths in Approximation Theory, and they establish that errors of optimal approximations are closely related to the decay of the eigenvalues. Though the eigenspaces and eigenvalues are not readily available, they can be well approximated using the standard n-dimensional subspaces spanned by translates of the kernel with respect to n nodes or centers. We give error bounds for the numerical approximation of the eigensystem via such subspaces. A series of examples shows that our numerical technique via a greedy point selection strategy allows to calculate the eigensystems with good accuracy.},
	language = {en},
	number = {4},
	urldate = {2018-04-01},
	journal = {Advances in Computational Mathematics},
	author = {Santin, G. and Schaback, R.},
	month = aug,
	year = {2016},
	pages = {973--993},
	file = {Snapshot:/home/jeremiah/Zotero/storage/CA2FERC6/s10444-015-9449-5.html:text/html}
}

@article{hoorfar_inequalities_2008,
	title = {Inequalities on the {Lambert} {W} function and hyperpower function},
	volume = {9},
	abstract = {In this note, we obtain inequalities for the Lambert W function W(x), defined by W(x)eW(x) = x for x e 1. Also, we get upper and lower bounds for the hyperpower function h(x) = xx x. ..},
	journal = {JIPAM. Journal of Inequalities in Pure \& Applied Mathematics [electronic only]},
	author = {Hoorfar, Abdolhossein and Hassani, Mehdi},
	month = jan,
	year = {2008}
}

@article{bartlett_local_2005,
	title = {Local {Rademacher} complexities},
	volume = {33},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1123250221},
	doi = {10.1214/009053605000000282},
	abstract = {We propose new bounds on the error of learning algorithms in terms of a data-dependent notion of complexity. The estimates we establish give optimal rates and are based on a local and empirical version of Rademacher averages, in the sense that the Rademacher averages are computed from the data, on a subset of functions with small empirical error. We present some applications to classification and prediction with convex function classes, and with kernel classes in particular.},
	language = {en},
	number = {4},
	urldate = {2018-04-02},
	journal = {The Annals of Statistics},
	author = {Bartlett, Peter L. and Bousquet, Olivier and Mendelson, Shahar},
	month = aug,
	year = {2005},
	mrnumber = {MR2166554},
	zmnumber = {1083.62034},
	keywords = {concentration inequalities, data-dependent complexity, Error bounds, Rademacher averages},
	pages = {1497--1537},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/5YY26CBF/Bartlett et al. - 2005 - Local Rademacher complexities.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/M4AHDFKW/1123250221.html:text/html}
}

@inproceedings{huang_finite-sample_2014,
	title = {A {Finite}-{Sample} {Generalization} {Bound} for {Semiparametric} {Regression}: {Partially} {Linear} {Models}},
	shorttitle = {A {Finite}-{Sample} {Generalization} {Bound} for {Semiparametric} {Regression}},
	url = {http://proceedings.mlr.press/v33/huang14.html},
	abstract = {In this paper we provide generalization bounds for semiparametric regression with the so-called partially linear models where the regression function is written as the sum of a linear parametric an...},
	language = {en},
	urldate = {2018-04-02},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Huang, Ruitong and Szepesvari, Csaba},
	month = apr,
	year = {2014},
	pages = {402--410},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/JCEU23VB/Huang and Szepesvari - 2014 - A Finite-Sample Generalization Bound for Semiparam.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/94VC9TRA/huang14.html:text/html}
}

@article{kearns_algorithmic_1999,
	title = {Algorithmic {Stability} and {Sanity}-{Check} {Bounds} for {Leave}-{One}-{Out} {Cross}-{Validation}},
	volume = {11},
	issn = {0899-7667},
	doi = {10.1162/089976699300016304},
	abstract = {In this article we prove sanity-check bounds for the error of the leave-oneout cross-validation estimate of the generalization error: that is, bounds showing that the worst-case error of this estimate is not much worse than that of the training error estimate. The name sanity check refers to the fact that although we often expect the leave-one-out estimate to perform considerably better than the training error estimate, we are here only seeking assurance that its performance will not be considerably worse. Perhaps surprisingly, such assurance has been given only for limited cases in the prior literature on cross-validation. Any nontrivial bound on the error of leave-one-out must rely on some notion of algorithmic stability. Previous bounds relied on the rather strong notion of hypothesis stability, whose application was primarily limited to nearest-neighbor and other local algorithms. Here we introduce the new and weaker notion of error stability and apply it to obtain sanity-check bounds for leave-one-out for other classes of learning algorithms, including training error minimization procedures and Bayesian algorithms. We also provide lower bounds demonstrating the necessity of some form of error stability for proving bounds on the error of the leave-one-out estimate, and the fact that for training error minimization algorithms, in the worst case such bounds must still depend on the Vapnik-Chervonenkis dimension of the hypothesis class.},
	number = {6},
	journal = {Neural Computation},
	author = {Kearns, M. and Ron, D.},
	month = aug,
	year = {1999},
	pages = {1427--1453},
	file = {IEEE Xplore Abstract Record:/home/jeremiah/Zotero/storage/M3IGPDJ8/6790877.html:text/html}
}

@article{bacharoglou_approximation_2010,
	title = {Approximation of probability distributions by convex mixtures of {Gaussian} measures},
	volume = {138},
	doi = {10.1090/S0002-9939-10-10340-2},
	abstract = {Let A(+) = \{a = (a(n)) is an element of boolean AND(p{\textgreater}I) l(p) : a(n) {\textgreater} 0, for all(n) is an element of N\} and let \{\vphantom{\}}171 be an enumeration of all normal distributions with mean a rational number and variance 1/n(2), a = 1,2 .... We prove that there exists an a is an element of A(+) such that that every probability density function, continuous, with compact support in R. can be approximated in L1 and L norm simultaneously by the averages 1/Sigma(n)(j=1)a(j) Sigma(n)(j=1) a(j)phi(j). The set of such sequences is a dense G(delta) set in A(+) and contains a dense positive cone.},
	journal = {Proceedings of The American Mathematical Society - PROC AMER MATH SOC},
	author = {Bacharoglou, Athanassia},
	month = jul,
	year = {2010},
	pages = {2619--2619}
}

@article{bartlett_rademacher_2002,
	title = {Rademacher and {Gaussian} {Complexities}: {Risk} {Bounds} and {Structural} {Results}},
	volume = {3},
	issn = {ISSN 1533-7928},
	shorttitle = {Rademacher and {Gaussian} {Complexities}},
	url = {http://www.jmlr.org/papers/v3/bartlett02a.html},
	number = {Nov},
	urldate = {2018-04-03},
	journal = {Journal of Machine Learning Research},
	author = {Bartlett, Peter L. and Mendelson, Shahar},
	year = {2002},
	pages = {463--482},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/GNJ45ARH/Bartlett and Mendelson - 2002 - Rademacher and Gaussian Complexities Risk Bounds .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/ZQJ2NMTF/bartlett02a.html:text/html}
}

@incollection{wei_early_2017,
	title = {Early stopping for kernel boosting algorithms: {A} general analysis with localized complexities},
	shorttitle = {Early stopping for kernel boosting algorithms},
	url = {http://papers.nips.cc/paper/7187-early-stopping-for-kernel-boosting-algorithms-a-general-analysis-with-localized-complexities.pdf},
	urldate = {2018-04-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Wei, Yuting and Yang, Fanny and Wainwright, Martin J},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {6065--6075},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/SS8JHDWX/Wei et al. - 2017 - Early stopping for kernel boosting algorithms A g.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/U6ZWI2PZ/7187-early-stopping-for-kernel-boosting-algorithms-a-general-analysis-with-localized-complexiti.html:text/html}
}

@article{aitchison_logistic-normal_1980,
	title = {Logistic-{Normal} {Distributions}: {Some} {Properties} and {Uses}},
	volume = {67},
	issn = {0006-3444},
	shorttitle = {Logistic-{Normal} {Distributions}},
	url = {http://www.jstor.org/stable/2335470},
	doi = {10.2307/2335470},
	abstract = {The logistic transformation applied to a d-dimensional normal distribution produces a distribution over the d-dimensional simplex which can sensibly be termed a logistic-normal distribution. Such distributions, implicitly used in a number of recent applications, are here given a formal identity and some useful properties are recorded. A main aim is to extend the area of application from the restricted role as a substitute for the Dirichlet conjugate prior class in the analysis of multinomial and contingency table data to the direct statistical description and analysis of compositional and probabilistic data.},
	number = {2},
	urldate = {2018-04-05},
	journal = {Biometrika},
	author = {Aitchison, J. and Shen, S. M.},
	year = {1980},
	pages = {261--272}
}

@incollection{tsybakov_optimal_2003,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optimal {Rates} of {Aggregation}},
	isbn = {978-3-540-40720-1 978-3-540-45167-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-45167-9_23},
	abstract = {We study the problem of aggregation of M arbitrary estimators of a regression function with respect to the mean squared risk. Three main types of aggregation are considered: model selection, convex and linear aggregation. We define the notion of optimal rate of aggregation in an abstract context and prove lower bounds valid for any method of aggregation. We then construct procedures that attain these bounds, thus establishing optimal rates of linear, convex and model selection type aggregation.},
	language = {en},
	urldate = {2018-04-05},
	booktitle = {Learning {Theory} and {Kernel} {Machines}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Tsybakov, Alexandre B.},
	year = {2003},
	doi = {10.1007/978-3-540-45167-9_23},
	pages = {303--313},
	file = {Snapshot:/home/jeremiah/Zotero/storage/9PTAXDKY/978-3-540-45167-9_23.html:text/html}
}

@article{bartlett_l1-regularized_2012,
	title = {L1-regularized linear regression: persistence and oracle inequalities},
	volume = {154},
	issn = {0178-8051, 1432-2064},
	shorttitle = {{\textless}{Emphasis} {Type}="{Italic}"{\textgreater}ℓ{\textless}/{Emphasis}{\textgreater}{\textless}{Subscript}{\textgreater}1{\textless}/{Subscript}{\textgreater}-regularized linear regression},
	url = {https://link.springer.com/article/10.1007/s00440-011-0367-2},
	doi = {10.1007/s00440-011-0367-2},
	abstract = {We study the predictive performance of ℓ 1-regularized linear regression in a model-free setting, including the case where the number of covariates is substantially larger than the sample size. We introduce a new analysis method that avoids the boundedness problems that typically arise in model-free empirical minimization. Our technique provides an answer to a conjecture of Greenshtein and Ritov (Bernoulli 10(6):971–988, 2004) regarding the “persistence” rate for linear regression and allows us to prove an oracle inequality for the error of the regularized minimizer. It also demonstrates that empirical risk minimization gives optimal rates (up to log factors) of convex aggregation of a set of estimators of a regression function.},
	language = {en},
	number = {1-2},
	urldate = {2018-04-05},
	journal = {Probability Theory and Related Fields},
	author = {Bartlett, Peter L. and Mendelson, Shahar and Neeman, Joseph},
	month = oct,
	year = {2012},
	pages = {193--224},
	file = {Snapshot:/home/jeremiah/Zotero/storage/3MG8TCPH/s00440-011-0367-2.html:text/html}
}

@article{lecue_regularization_2017,
	title = {Regularization and the small-ball method {II}: complexity dependent error rates},
	volume = {18},
	shorttitle = {Regularization and the small-ball method {II}},
	url = {http://jmlr.org/papers/v18/16-422.html},
	number = {146},
	urldate = {2018-04-05},
	journal = {Journal of Machine Learning Research},
	author = {Lecué, Guillaume and Mendelson, Shahar},
	year = {2017},
	pages = {1--48},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/BLFUM8LJ/Lecué and Mendelson - 2017 - Regularization and the small-ball method II compl.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/DJ5624LT/16-422.html:text/html}
}

@article{lecue_regularization_2018,
	title = {Regularization and the small-ball method {I}: {Sparse} recovery},
	volume = {46},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Regularization and the small-ball method {I}},
	url = {https://projecteuclid.org/euclid.aos/1522742431},
	doi = {10.1214/17-AOS1562},
	abstract = {We obtain bounds on estimation error rates for regularization procedures of the form f̂ ∈argminf∈F(1N∑i=1N(Yi−f(Xi))2+λΨ(f))f{\textasciicircum}∈argminf∈F⁡(1N∑i=1N(Yi−f(Xi))2+λΨ(f)){\textbackslash}begin\{equation*\}{\textbackslash}hat\{f\}{\textbackslash}in{\textbackslash}mathop\{{\textbackslash}operatorname\{argmin\}\}\_\{f{\textbackslash}in F\}({\textbackslash}frac\{1\}\{N\}{\textbackslash}sum\_\{i=1\}{\textasciicircum}\{N\}(Y\_\{i\}-f(X\_\{i\})){\textasciicircum}\{2\}+{\textbackslash}lambda {\textbackslash}Psi(f)){\textbackslash}end\{equation*\} when ΨΨ{\textbackslash}Psi is a norm and FFF is convex. Our approach gives a common framework that may be used in the analysis of learning problems and regularization problems alike. In particular, it sheds some light on the role various notions of sparsity have in regularization and on their connection with the size of subdifferentials of ΨΨ{\textbackslash}Psi in a neighborhood of the true minimizer. As “proof of concept” we extend the known estimates for the LASSO, SLOPE and trace norm regularization.},
	language = {EN},
	number = {2},
	urldate = {2018-04-05},
	journal = {The Annals of Statistics},
	author = {Lecué, Guillaume and Mendelson, Shahar},
	month = apr,
	year = {2018},
	keywords = {Empirical processes, high-dimensional statistics},
	pages = {611--641},
	file = {Snapshot:/home/jeremiah/Zotero/storage/67CQIH35/1522742431.html:text/html}
}

@inproceedings{wilson_gaussian_2013,
	title = {Gaussian {Process} {Kernels} for {Pattern} {Discovery} and {Extrapolation}},
	url = {http://proceedings.mlr.press/v28/wilson13.html},
	abstract = {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used w...},
	language = {en},
	urldate = {2018-04-06},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wilson, Andrew and Adams, Ryan},
	month = feb,
	year = {2013},
	pages = {1067--1075},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/QXFLGAT4/Wilson and Adams - 2013 - Gaussian Process Kernels for Pattern Discovery and.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/37NQRDYF/wilson13.html:text/html}
}

@article{jara_class_2011,
	title = {A class of mixtures of dependent tail-free processes},
	volume = {98},
	issn = {0006-3444},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3398659/},
	doi = {10.1093/biomet/asq082},
	abstract = {We propose a class of dependent processes in which density shape is regressed on one or more predictors through conditional tail-free probabilities by using transformed Gaussian processes. A particular linear version of the process is developed in detail. The resulting process is flexible and easy to fit using standard algorithms for generalized linear models. The method is applied to growth curve analysis, evolving univariate random effects distributions in generalized linear mixed models, and median survival modelling with censored data and covariate-dependent errors.},
	number = {3},
	urldate = {2018-04-07},
	journal = {Biometrika},
	author = {Jara, A. and Hanson, T. E.},
	month = sep,
	year = {2011},
	pmid = {22822260},
	pmcid = {PMC3398659},
	pages = {553--566},
	file = {PubMed Central Full Text PDF:/home/jeremiah/Zotero/storage/GHLHSIT2/Jara and Hanson - 2011 - A class of mixtures of dependent tail-free process.pdf:application/pdf}
}

@incollection{rahimi_weighted_2009,
	title = {Weighted {Sums} of {Random} {Kitchen} {Sinks}: {Replacing} minimization with randomization in learning},
	shorttitle = {Weighted {Sums} of {Random} {Kitchen} {Sinks}},
	url = {http://papers.nips.cc/paper/3495-weighted-sums-of-random-kitchen-sinks-replacing-minimization-with-randomization-in-learning.pdf},
	urldate = {2018-04-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 21},
	publisher = {Curran Associates, Inc.},
	author = {Rahimi, Ali and Recht, Benjamin},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
	pages = {1313--1320},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/VHKVURPJ/Rahimi and Recht - 2009 - Weighted Sums of Random Kitchen Sinks Replacing m.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/K9PH5SF4/3495-weighted-sums-of-random-kitchen-sinks-replacing-minimization-with-randomization-in-learnin.html:text/html}
}

@article{bartlett_spectrally-normalized_2017,
	title = {Spectrally-normalized margin bounds for neural networks},
	url = {http://arxiv.org/abs/1706.08498},
	abstract = {This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized "spectral complexity": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.},
	urldate = {2018-04-15},
	journal = {arXiv:1706.08498 [cs, stat]},
	author = {Bartlett, Peter and Foster, Dylan J. and Telgarsky, Matus},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.08498},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Comparison to arXiv v1: 1-norm in main bound refined to (2,1)-group-norm. Comparison to NIPS camera ready: typo fixes},
	file = {arXiv\:1706.08498 PDF:/home/jeremiah/Zotero/storage/QP4DR2EQ/Bartlett et al. - 2017 - Spectrally-normalized margin bounds for neural net.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/Q8CAI9B9/1706.html:text/html}
}

@article{neyshabur_pac-bayesian_2017,
	title = {A {PAC}-{Bayesian} {Approach} to {Spectrally}-{Normalized} {Margin} {Bounds} for {Neural} {Networks}},
	url = {http://arxiv.org/abs/1707.09564},
	abstract = {We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis.},
	urldate = {2018-04-15},
	journal = {arXiv:1707.09564 [cs]},
	author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.09564},
	keywords = {Computer Science - Learning},
	annote = {Comment: Accepted to ICLR 2018},
	file = {arXiv\:1707.09564 PDF:/home/jeremiah/Zotero/storage/98LX937T/Neyshabur et al. - 2017 - A PAC-Bayesian Approach to Spectrally-Normalized M.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/MCDI82CP/1707.html:text/html}
}

@incollection{neyshabur_exploring_2017,
	title = {Exploring {Generalization} in {Deep} {Learning}},
	url = {http://papers.nips.cc/paper/7176-exploring-generalization-in-deep-learning.pdf},
	urldate = {2018-04-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {5947--5956},
	file = {NIPS Full Text PDF:/home/jeremiah/Zotero/storage/PQRHEWZ9/Neyshabur et al. - 2017 - Exploring Generalization in Deep Learning.pdf:application/pdf;NIPS Snapshort:/home/jeremiah/Zotero/storage/YRU5SR9S/7176-exploring-generalization-in-deep-learning.html:text/html}
}

@article{dziugaite_computing_2017,
	title = {Computing {Nonvacuous} {Generalization} {Bounds} for {Deep} ({Stochastic}) {Neural} {Networks} with {Many} {More} {Parameters} than {Training} {Data}},
	url = {http://arxiv.org/abs/1703.11008},
	abstract = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this "deep learning" regime. Logically, in order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
	urldate = {2018-04-15},
	journal = {arXiv:1703.11008 [cs]},
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.11008},
	keywords = {Computer Science - Learning},
	annote = {Comment: 14 pages, 1 table, 2 figures. Corresponds with UAI camera ready and supplement. Includes additional references and related experiments},
	file = {arXiv\:1703.11008 PDF:/home/jeremiah/Zotero/storage/UZVH834A/Dziugaite and Roy - 2017 - Computing Nonvacuous Generalization Bounds for Dee.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/PWZI8BZJ/1703.html:text/html}
}

@book{koltchinskii_oracle_2011,
	address = {Berlin Heidelberg},
	series = {École d'Été de {Probabilités} de {Saint}-{Flour}},
	title = {Oracle {Inequalities} in {Empirical} {Risk} {Minimization} and {Sparse} {Recovery} {Problems}: École d’Été de {Probabilités} de {Saint}-{Flour} {XXXVIII}-2008},
	isbn = {978-3-642-22146-0},
	shorttitle = {Oracle {Inequalities} in {Empirical} {Risk} {Minimization} and {Sparse} {Recovery} {Problems}},
	url = {//www.springer.com/gp/book/9783642221460},
	abstract = {The purpose of these lecture notes is to provide an introduction to the general theory of empirical risk minimization with an emphasis on excess risk bounds and oracle inequalities in penalized problems. In recent years, there have been new developments in this area motivated by the study of new classes of methods in machine learning such as large margin classification methods (boosting, kernel machines). The main probabilistic tools involved in the analysis of these problems are concentration and deviation inequalities by Talagrand along with other methods of empirical processes theory (symmetrization inequalities, contraction inequality for Rademacher sums, entropy and generic chaining bounds). Sparse recovery based on l\_1-type penalization and low rank matrix recovery based on the nuclear norm penalization are other active areas of research, where the main problems can be stated in the framework of penalized empirical risk minimization, and concentration inequalities and empirical processes tools have proved to be very useful.},
	language = {en},
	urldate = {2018-04-15},
	publisher = {Springer-Verlag},
	author = {Koltchinskii, Vladimir},
	year = {2011},
	file = {Snapshot:/home/jeremiah/Zotero/storage/KHYTGIPI/9783642221460.html:text/html}
}

@inproceedings{b._tsybakov_optimal_2003,
	title = {Optimal {Rates} of {Aggregation}},
	volume = {2777},
	doi = {10.1007/978-3-540-45167-9_23},
	abstract = {We study the problem of aggregation of M arbitrary estimators of a regression function with respect to the mean squared risk. Three main types of aggregation are
considered: model selection, convex and linear aggregation. We define the notion of optimal rate of aggregation in an abstract
context and prove lower bounds valid for any method of aggregation. We then construct procedures that attain these bounds,
thus establishing optimal rates of linear, convex and model selection type aggregation.},
	booktitle = {Lect. {Notes} {Artif}. {Intell}.},
	author = {B. Tsybakov, Alexandre},
	month = jan,
	year = {2003},
	pages = {303--313}
}

@article{rigollet_exponential_2010,
	title = {Exponential {Screening} and optimal rates of sparse estimation},
	url = {http://arxiv.org/abs/1003.2654},
	abstract = {In high-dimensional linear regression, the goal pursued here is to estimate an unknown regression function using linear combinations of a suitable set of covariates. One of the key assumptions for the success of any statistical procedure in this setup is to assume that the linear combination is sparse in some sense, for example, that it involves only few covariates. We consider a general, non necessarily linear, regression with Gaussian noise and study a related question that is to find a linear combination of approximating functions, which is at the same time sparse and has small mean squared error (MSE). We introduce a new estimation procedure, called Exponential Screening that shows remarkable adaptation properties. It adapts to the linear combination that optimally balances MSE and sparsity, whether the latter is measured in terms of the number of non-zero entries in the combination (\${\textbackslash}ell\_0\$ norm) or in terms of the global weight of the combination (\${\textbackslash}ell\_1\$ norm). The power of this adaptation result is illustrated by showing that Exponential Screening solves optimally and simultaneously all the problems of aggregation in Gaussian regression that have been discussed in the literature. Moreover, we show that the performance of the Exponential Screening estimator cannot be improved in a minimax sense, even if the optimal sparsity is known in advance. The theoretical and numerical superiority of Exponential Screening compared to state-of-the-art sparse procedures is also discussed.},
	urldate = {2018-04-15},
	journal = {arXiv:1003.2654 [math, stat]},
	author = {Rigollet, Philippe and Tsybakov, Alexandre},
	month = mar,
	year = {2010},
	note = {arXiv: 1003.2654},
	keywords = {Mathematics - Statistics Theory, Primary 62G08, Secondary 62G05, 62J05, 62C20, 62G20},
	file = {arXiv\:1003.2654 PDF:/home/jeremiah/Zotero/storage/3ZII6FBU/Rigollet and Tsybakov - 2010 - Exponential Screening and optimal rates of sparse .pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/UMFKCGNG/1003.html:text/html}
}

@article{rigollet_exponential_2011,
	title = {Exponential {Screening} and optimal rates of sparse estimation},
	volume = {39},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1299680953},
	doi = {10.1214/10-AOS854},
	abstract = {In high-dimensional linear regression, the goal pursued here is to estimate an unknown regression function using linear combinations of a suitable set of covariates. One of the key assumptions for the success of any statistical procedure in this setup is to assume that the linear combination is sparse in some sense, for example, that it involves only few covariates. We consider a general, nonnecessarily linear, regression with Gaussian noise and study a related question, that is, to find a linear combination of approximating functions, which is at the same time sparse and has small mean squared error (MSE). We introduce a new estimation procedure, called Exponential Screening, that shows remarkable adaptation properties. It adapts to the linear combination that optimally balances MSE and sparsity, whether the latter is measured in terms of the number of nonzero entries in the combination (ℓ0 norm) or in terms of the global weight of the combination (ℓ1 norm). The power of this adaptation result is illustrated by showing that Exponential Screening solves optimally and simultaneously all the problems of aggregation in Gaussian regression that have been discussed in the literature. Moreover, we show that the performance of the Exponential Screening estimator cannot be improved in a minimax sense, even if the optimal sparsity is known in advance. The theoretical and numerical superiority of Exponential Screening compared to state-of-the-art sparse procedures is also discussed.},
	language = {EN},
	number = {2},
	urldate = {2018-04-15},
	journal = {The Annals of Statistics},
	author = {Rigollet, Philippe and Tsybakov, Alexandre},
	month = apr,
	year = {2011},
	mrnumber = {MR2816337},
	zmnumber = {1215.62043},
	keywords = {Lasso, adaptation, aggregation, BIC, High-dimensional regression, minimax rates, sparsity, sparsity oracle inequalities},
	pages = {731--771},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/XDBN9S2M/Rigollet and Tsybakov - 2011 - Exponential Screening and optimal rates of sparse .pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/Y36RHKH4/1299680953.html:text/html}
}

@article{lecue_oracle_2012,
	title = {Oracle inequalities for cross-validation type procedures},
	volume = {6},
	issn = {1935-7524},
	url = {https://projecteuclid.org/euclid.ejs/1349355603},
	doi = {10.1214/12-EJS730},
	abstract = {We prove oracle inequalities for three different types of adaptation procedures inspired by cross-validation and aggregation. These procedures are then applied to the construction of Lasso estimators and aggregation with exponential weights with data-driven regularization and temperature parameters, respectively. We also prove oracle inequalities for the cross-validation procedure itself under some convexity assumptions.},
	language = {EN},
	urldate = {2018-04-15},
	journal = {Electronic Journal of Statistics},
	author = {Lecué, Guillaume and Mitchell, Charles},
	year = {2012},
	mrnumber = {MR2988465},
	zmnumber = {1295.62051},
	keywords = {aggregation, sparsity, Adaptation, cross-validation},
	pages = {1803--1837},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/PBFQ42EY/Lecué and Mitchell - 2012 - Oracle inequalities for cross-validation type proc.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/YA6YZ5CC/1349355603.html:text/html}
}

@article{bunea_aggregation_2007,
	title = {Aggregation for {Gaussian} regression},
	volume = {35},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1188405626},
	doi = {10.1214/009053606000001587},
	abstract = {This paper studies statistical aggregation procedures in the regression setting. A motivating factor is the existence of many different methods of estimation, leading to possibly competing estimators. We consider here three different types of aggregation: model selection (MS) aggregation, convex (C) aggregation and linear (L) aggregation. The objective of (MS) is to select the optimal single estimator from the list; that of (C) is to select the optimal convex combination of the given estimators; and that of (L) is to select the optimal linear combination of the given estimators. We are interested in evaluating the rates of convergence of the excess risks of the estimators obtained by these procedures. Our approach is motivated by recently published minimax results [Nemirovski, A. (2000). Topics in non-parametric statistics. Lectures on Probability Theory and Statistics (Saint-Flour, 1998). Lecture Notes in Math. 1738 85–277. Springer, Berlin; Tsybakov, A. B. (2003). Optimal rates of aggregation. Learning Theory and Kernel Machines. Lecture Notes in Artificial Intelligence 2777 303–313. Springer, Heidelberg]. There exist competing aggregation procedures achieving optimal convergence rates for each of the (MS), (C) and (L) cases separately. Since these procedures are not directly comparable with each other, we suggest an alternative solution. We prove that all three optimal rates, as well as those for the newly introduced (S) aggregation (subset selection), are nearly achieved via a single “universal” aggregation procedure. The procedure consists of mixing the initial estimators with weights obtained by penalized least squares. Two different penalties are considered: one of them is of the BIC type, the second one is a data-dependent ℓ1-type penalty.},
	language = {EN},
	number = {4},
	urldate = {2018-04-15},
	journal = {The Annals of Statistics},
	author = {Bunea, Florentina and Tsybakov, Alexandre B. and Wegkamp, Marten H.},
	month = aug,
	year = {2007},
	mrnumber = {MR2351101},
	zmnumber = {1209.62065},
	keywords = {model averaging, model selection, oracle inequalities, Aggregation, Lasso estimator, minimax risk, nonparametric regression, penalized least squares},
	pages = {1674--1697},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/88CK32NJ/Bunea et al. - 2007 - Aggregation for Gaussian regression.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/VU49BMBC/1188405626.html:text/html}
}

@article{kawaguchi_generalization_2017,
	title = {Generalization in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1710.05468},
	abstract = {With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, answering to an open question in the literature. We also discuss limitations of our results and propose additional open problems.},
	urldate = {2018-04-15},
	journal = {arXiv:1710.05468 [cs, stat]},
	author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05468},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Extended version: all previous results remain unchanged and new theoretical results were added with improved presentation},
	file = {arXiv\:1710.05468 PDF:/home/jeremiah/Zotero/storage/X5KNENWM/Kawaguchi et al. - 2017 - Generalization in Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/3MVSCWUP/1710.html:text/html}
}

@article{hanks_constructive_2015,
	title = {A {Constructive} {Spatio}-{Temporal} {Approach} to {Modeling} {Spatial} {Covariance}},
	url = {http://arxiv.org/abs/1506.03824},
	abstract = {I present an approach for modeling areal spatial covariance by considering the stationary distribution of a spatio-temporal Markov random walk. In the areal data case, this stationary distribution corresponds to an intrinsic simultaneous autoregressive (SAR) model for spatial correlation, and provides a principled approach to specifying areal spatial models when a spatio-temporal generating process can be assumed. I apply the approach to a study of spatial genetic variation of trout in a stream network in Connecticut, USA, and a study of crime rates in neighborhoods of Columbus, OH, USA.},
	urldate = {2018-04-16},
	journal = {arXiv:1506.03824 [stat]},
	author = {Hanks, Ephraim M.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03824},
	keywords = {Statistics - Methodology},
	annote = {Comment: 32 pages, 5 figures},
	file = {arXiv\:1506.03824 PDF:/home/jeremiah/Zotero/storage/CQWCJ3KA/Hanks - 2015 - A Constructive Spatio-Temporal Approach to Modelin.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/HSL9JDKD/1506.html:text/html}
}

@article{hanks_modeling_2017,
	title = {Modeling {Spatial} {Covariance} {Using} the {Limiting} {Distribution} of {Spatio}-{Temporal} {Random} {Walks}},
	volume = {112},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1224714},
	doi = {10.1080/01621459.2016.1224714},
	abstract = {We present an approach for modeling areal spatial covariance in observed genetic allele data by considering the stationary (limiting) distribution of a spatio-temporal Markov random walk model for gene flow. This stationary distribution corresponds to an intrinsic simultaneous autoregressive (SAR) model for spatial correlation, and provides a principled approach to specifying areal spatial models when a spatio-temporal generating process can be assumed. We apply the approach to a study of spatial genetic variation of trout in a stream network in Connecticut, USA.},
	number = {518},
	urldate = {2018-04-16},
	journal = {Journal of the American Statistical Association},
	author = {Hanks, Ephraim M.},
	month = apr,
	year = {2017},
	keywords = {Autoregressive models, Diffusion, SAR models, Spatial autocorrelation},
	pages = {497--507},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/S98YIFCH/Hanks - 2017 - Modeling Spatial Covariance Using the Limiting Dis.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/64GTCYIT/01621459.2016.html:text/html}
}

@article{gaiffas_hyper-sparse_2009,
	title = {Hyper-sparse optimal aggregation},
	url = {http://arxiv.org/abs/0912.1618},
	abstract = {In this paper, we consider the problem of "hyper-sparse aggregation". Namely, given a dictionary \$F = {\textbackslash}\{f\_1, ..., f\_M {\textbackslash}\}\$ of functions, we look for an optimal aggregation algorithm that writes \${\textbackslash}tilde f = {\textbackslash}sum\_\{j=1\}{\textasciicircum}M {\textbackslash}theta\_j f\_j\$ with as many zero coefficients \${\textbackslash}theta\_j\$ as possible. This problem is of particular interest when \$F\$ contains many irrelevant functions that should not appear in \${\textbackslash}tilde\{f\}\$. We provide an exact oracle inequality for \${\textbackslash}tilde f\$, where only two coefficients are non-zero, that entails \${\textbackslash}tilde f\$ to be an optimal aggregation algorithm. Since selectors are suboptimal aggregation procedures, this proves that 2 is the minimal number of elements of \$F\$ required for the construction of an optimal aggregation procedures in every situations. A simulated example of this algorithm is proposed on a dictionary obtained using LARS, for the problem of selection of the regularization parameter of the LASSO. We also give an example of use of aggregation to achieve minimax adaptation over anisotropic Besov spaces, which was not previously known in minimax theory (in regression on a random design).},
	urldate = {2018-04-17},
	journal = {arXiv:0912.1618 [stat]},
	author = {Gaïffas, Stéphane and Lecué, Guillaume},
	month = dec,
	year = {2009},
	note = {arXiv: 0912.1618},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: 33 pages},
	file = {arXiv\:0912.1618 PDF:/home/jeremiah/Zotero/storage/E5J9J38A/Gaïffas and Lecué - 2009 - Hyper-sparse optimal aggregation.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/HZ4U8VLK/0912.html:text/html}
}

@article{dai_aggregation_2014,
	title = {Aggregation of affine estimators},
	volume = {8},
	issn = {1935-7524},
	url = {https://projecteuclid.org/euclid.ejs/1397134174},
	doi = {10.1214/14-EJS886},
	abstract = {We consider the problem of aggregating a general collection of affine estimators for fixed design regression. Relevant examples include some commonly used statistical estimators such as least squares, ridge and robust least squares estimators. Dalalyan and Salmon [DS12] have established that, for this problem, exponentially weighted (EW) model selection aggregation leads to sharp oracle inequalities in expectation, but similar bounds in deviation were not previously known. While results [DRZ12] indicate that the same aggregation scheme may not satisfy sharp oracle inequalities with high probability, we prove that a weaker notion of oracle inequality for EW that holds with high probability. Moreover, using a generalization of the newly introduced QQQ-aggregation scheme we also prove sharp oracle inequalities that hold with high probability. Finally, we apply our results to universal aggregation and show that our proposed estimator leads simultaneously to all the best known bounds for aggregation, including ℓqℓq{\textbackslash}ell\_\{q\}-aggregation, q∈(0,1)q∈(0,1)q{\textbackslash}in(0,1), with high probability.},
	language = {EN},
	number = {1},
	urldate = {2018-04-17},
	journal = {Electronic Journal of Statistics},
	author = {Dai, Dong and Rigollet, Philippe and Xia, Lucy and Zhang, Tong},
	year = {2014},
	mrnumber = {MR3192554},
	zmnumber = {1348.62132},
	keywords = {oracle inequalities, Aggregation, affine estimators, Gaussian mean, Maurey’s argument},
	pages = {302--327},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/SVASCJQU/Dai et al. - 2014 - Aggregation of affine estimators.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/UGG2XN8K/1397134174.html:text/html}
}

@article{dalalyan_sharp_2012,
	title = {Sharp oracle inequalities for aggregation of affine estimators},
	volume = {40},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1358951384},
	doi = {10.1214/12-AOS1038},
	abstract = {We consider the problem of combining a (possibly uncountably infinite) set of affine estimators in nonparametric regression model with heteroscedastic Gaussian noise. Focusing on the exponentially weighted aggregate, we prove a PAC-Bayesian type inequality that leads to sharp oracle inequalities in discrete but also in continuous settings. The framework is general enough to cover the combinations of various procedures such as least square regression, kernel ridge regression, shrinking estimators and many other estimators used in the literature on statistical inverse problems. As a consequence, we show that the proposed aggregate provides an adaptive estimator in the exact minimax sense without discretizing the range of tuning parameters or splitting the set of observations. We also illustrate numerically the good performance achieved by the exponentially weighted aggregate.},
	language = {EN},
	number = {4},
	urldate = {2018-04-17},
	journal = {The Annals of Statistics},
	author = {Dalalyan, Arnak S. and Salmon, Joseph},
	month = aug,
	year = {2012},
	mrnumber = {MR3059085},
	zmnumber = {1257.62038},
	keywords = {regression, model selection, oracle inequalities, Aggregation, minimax risk, exponentially weighted aggregation},
	pages = {2327--2355},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/HDABQGU4/Dalalyan and Salmon - 2012 - Sharp oracle inequalities for aggregation of affin.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/DJJ7MXXV/1358951384.html:text/html}
}

@article{rigollet_sparse_2012,
	title = {Sparse {Estimation} by {Exponential} {Weighting}},
	volume = {27},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid-org.ezp-prod1.hul.harvard.edu/euclid.ss/1356098556},
	doi = {10.1214/12-STS393},
	abstract = {Consider a regression model with fixed design and Gaussian noise where the regression function can potentially be well approximated by a function that admits a sparse representation in a given dictionary. This paper resorts to exponential weights to exploit this underlying sparsity by implementing the principle of sparsity pattern aggregation. This model selection take on sparse estimation allows us to derive sparsity oracle inequalities in several popular frameworks, including ordinary sparsity, fused sparsity and group sparsity. One striking aspect of these theoretical results is that they hold under no condition in the dictionary. Moreover, we describe an efficient implementation of the sparsity pattern aggregation principle that compares favorably to state-of-the-art procedures on some basic numerical examples.},
	language = {EN},
	number = {4},
	urldate = {2018-04-17},
	journal = {Statistical Science},
	author = {Rigollet, Philippe and Tsybakov, Alexandre B.},
	month = nov,
	year = {2012},
	mrnumber = {MR3025134},
	zmnumber = {1331.62351},
	keywords = {High-dimensional regression, sparsity, sparsity oracle inequalities, exponential weights, fused sparsity, group sparsity, sparse regression, sparsity pattern aggregation, sparsity prior},
	pages = {558--575},
	file = {Full Text PDF:/home/jeremiah/Zotero/storage/8L8U6ME4/Rigollet and Tsybakov - 2012 - Sparse Estimation by Exponential Weighting.pdf:application/pdf;Snapshot:/home/jeremiah/Zotero/storage/9YLV5GDA/1356098556.html:text/html}
}

@inproceedings{dalalyan_aggregation_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Aggregation by {Exponential} {Weighting} and {Sharp} {Oracle} {Inequalities}},
	isbn = {978-3-540-72925-9 978-3-540-72927-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-72927-3_9},
	doi = {10.1007/978-3-540-72927-3_9},
	abstract = {In the present paper, we study the problem of aggregation under the squared loss in the model of regression with deterministic design. We obtain sharp oracle inequalities for convex aggregates defined via exponential weights, under general assumptions on the distribution of errors and on the functions to aggregate. We show how these results can be applied to derive a sparsity oracle inequality.},
	language = {en},
	urldate = {2018-04-17},
	booktitle = {Learning {Theory}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Dalalyan, Arnak S. and Tsybakov, Alexandre B.},
	month = jun,
	year = {2007},
	pages = {97--111},
	file = {Snapshot:/home/jeremiah/Zotero/storage/LWM4Z3WK/978-3-540-72927-3_9.html:text/html}
}

@book{neal_bayesian_1996,
	address = {New York},
	series = {Lecture {Notes} in {Statistics}},
	title = {Bayesian {Learning} for {Neural} {Networks}},
	isbn = {978-0-387-94724-2},
	url = {//www.springer.com/gp/book/9780387947242},
	abstract = {Artificial "neural networks" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the "overfitting" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence.},
	language = {en},
	urldate = {2018-04-18},
	publisher = {Springer-Verlag},
	author = {Neal, Radford M.},
	year = {1996},
	file = {Snapshot:/home/jeremiah/Zotero/storage/G8XVD7JD/9780387947242.html:text/html}
}

@phdthesis{gal_uncertainty_2016,
	type = {{PhD} {Thesis}},
	title = {Uncertainty in {Deep} {Learning}},
	school = {University of Cambridge},
	author = {Gal, Yarin},
	year = {2016}
}

@misc{noauthor_gal_nodate,
	title = {Gal {Chechik}:  {NIPS} 1-17 data},
	url = {http://ai.stanford.edu/~gal/data.html},
	urldate = {2018-04-21},
	file = {Gal Chechik\: Download data:/home/jeremiah/Zotero/storage/I83HYD5U/data.html:text/html}
}

@misc{synced_tree_2017,
	title = {Tree {Boosting} {With} {XGBoost} — {Why} {Does} {XGBoost} {Win} “{Every}” {Machine} {Learning} {Competition}?},
	url = {https://medium.com/@Synced/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition-ca8034c0b283},
	abstract = {Introduction},
	urldate = {2018-04-25},
	journal = {Medium},
	author = {Synced},
	month = oct,
	year = {2017},
	file = {Snapshot:/home/jeremiah/Zotero/storage/M7L4L5HV/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition-ca8034c0b283.html:text/html}
}

@article{chen_xgboost:_2016,
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	shorttitle = {{XGBoost}},
	url = {http://arxiv.org/abs/1603.02754},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2018-04-25},
	journal = {arXiv:1603.02754 [cs]},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	note = {arXiv: 1603.02754},
	keywords = {Computer Science - Learning},
	pages = {785--794},
	annote = {Comment: KDD'16 changed all figures to type1},
	file = {arXiv\:1603.02754 PDF:/home/jeremiah/Zotero/storage/VG76UV58/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf;arXiv.org Snapshot:/home/jeremiah/Zotero/storage/8SLL7RYY/1603.html:text/html}
}