\documentclass[11pt]{article}

\usepackage{natbib}
\bibliographystyle{unsrtnat}

% change document font family to Palatino, and code font to Courier
\usepackage{mathpazo} % add possibly `sc` and `osf` options
%\usepackage{eulervm}
%\usepackage{courier}
%allow formula formatting

% flow chart
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{bayesnet}

%identation in nested enumerates
\usepackage[shortlabels]{enumitem}
\setlist[enumerate,1]{leftmargin=1cm} % level 1 list
\setlist[enumerate,2]{leftmargin=2cm} % level 2 list

%flush align equations to left, this also loads amsmath 
%\usepackage[fleqn]{mathtools}
\usepackage{mathtools}
\usepackage{amsthm}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\usepackage{comment}

% allow algorithm 
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

%allow equation numbering in {align*} envir
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%declare math symbolz
%# inner product
\DeclarePairedDelimiter{\inner}{\langle}{\rangle}

%declare argmin
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

%declare checkmark
\usepackage{amssymb}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%title positon
\usepackage{titling} %fix title
\setlength{\droptitle}{-6em}   % Move up the title 

%change section title font size
\usepackage{titlesec} 
\titleformat{\section}
  {\normalfont\fontsize{12}{15}}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\fontsize{12}{13}}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\fontsize{12}{13}}{\thesubsubsection}{1em}{}

%overwrite bfseries to allow formula in section title  
\def\bfseries{\fontseries \bfdefault \selectfont \boldmath}

% change page margin
\usepackage[margin=0.8 in]{geometry} 

%disable indentation
\setlength\parindent{0pt}

%allow inserting multiple graphs
\usepackage{graphicx}
\usepackage[skip=1pt]{subcaption}
\usepackage[justification=centering,font=small]{caption}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}%indep sign

%allow code chunks
\usepackage{listings}
%\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{frame=lrbt,xleftmargin=\fboxsep, xrightmargin=-\fboxsep}
\lstset{language=R, commentstyle=\bfseries, 
keywordstyle=\ttfamily} %R-related formatting
\lstset{escapeinside={<@}{@>}}

%allow merged cell in tables
\usepackage{multirow}

%allow http links
\usepackage{hyperref}

%allow different font colors
\usepackage{xcolor}

%Thm and Def environment
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}

\newenvironment{definition2}[1][Definition]{\begin{trivlist} %def without index
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newenvironment{example}[1][Example]{\begin{trivlist} %def without index
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

%allow strikeout effect
\usepackage[normalem]{ulem}

%macros from Bob Gray
\usepackage{"./macro/GrandMacros"}
\usepackage{"./macro/Macro_BIO235"}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% TItle page with contents %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Progress Report \\ Model Setup \\ \today \vspace{-1ex}}

\pretitle{\begin{flushright}\normalsize}
\posttitle{\par\end{flushright}}
\author{}
\date{}
\vspace{-10em}
\maketitle
\vspace{-5em}

\tableofcontents 
%\vspace{10em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Formal Sections %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\textbf{Simulated Data}}
First generate data from a spatial gaussian process with 100 locations, we sample spatial locations $(x, y)$ iid from standard normal, and assume the pollutant $z$ follow below Gaussian Process:
\begin{align*}
z(x, y) &\stackrel{iid}{\sim} N(f(x, y), \sigma^2 = 0.1) \\
f(x, y) &= 0.2 x + 0.5 y + \sqrt{x^2 + y^2 + 5*cos(x*y)} + 
sin(x) + cos(x) + log\Big(exp(x*y)+exp(x)\Big)
\end{align*}

\begin{figure}[!ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{"./plot/loc_site"}
  \caption{Sampled saptial location for monitoring sites (standardized)}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"./plot/y_surface"}
  \caption{Average pollutant surface over space (standardized)}
  \label{fig:sfig2}
\end{subfigure}
%\caption{plots of....}
\label{fig:fig}
\end{figure}

We then generate prediction for $z(x, y)$ from 10 base GP models \citep{duvenaud_structure_2013}:
\begin{enumerate}
\item Linear, 
\item Polynomial, degree 2, 3, 4
\item Gaussian RBF, with ARD
\item Mat\'{e}rn $\frac{1}{2}$, $\frac{3}{2}$, $\frac{5}{2}$ with ARD
\item MLP, with ARD.\\ 
Equivalent to a 2-layer network with Gaussian CDF activation function and infinite hidden units:
$$k(x,y) = \sigma^{2}\frac{2}{\pi }  \text{asin} \left ( \frac{ \sigma_w^2 x^\top y+\sigma_k^2}{\sqrt{\sigma_w^2x^\top x + \sigma_k^2 + 1}\sqrt{\sigma_w^2 y^\top y + \sigma_k^2 +1}} \right )$$
\item Spectral Mixture \citep{wilson_gaussian_2013}
\end{enumerate} 

\section{\textbf{Model Overview}}
Input: data $\{\bx_i, y_i\}_{i=1}^n$, and prediction from base models $\{\hat{\by}_k\}_{k=1}^K$.\\
\underline{Step 1}: Learn model-specific GP from model prediction (i.e. the pre-ensemble processing step):
\begin{alignat*}{3}
y(\bx) &\sim \Gsc\Psc_k(\hat{y}_k(\bx), &&\; cov(z_k)(\bx)) 
\end{alignat*}
\underline{Step 2}: Learn ensemble weights $\{w_k(\bx)\}_{k=1}^K$
\begin{alignat*}{3}
\bw &\sim Dirichlet(exp(\bw')) \qquad \mbox{where } \bw = \{w_k\}_{k=1}^K, \bw' = \{w'_k\}_{k=1}^K \\
w'_k(\bx) & \sim \Gsc\Psc(\bzero, \; k_{w,k}(\bx, \bx') + \sigma_k^2 )
\end{alignat*}
\underline{Step 3}: Generate ensemble predictive distribution:
\begin{align*}
y &\sim \sum_{k=1}^K w_k(\bx) * \Gsc\Psc_k\Big(\hat{y}_k(\bx), \; cov(z_k)(\bx)\Big)\\
\end{align*}
Below sections describe the model derivation for each step in detail. For the purpose of debugging and future model improvement, for each step, I will also (1) highlight model assumption and (2) propose test cases.

\subsection{\textbf{Step 1: Learn Model-specific GP}}
This step learn a standard GP  for each model-specific residual process $\hat{\bepsilon}_k = \by - \hat{\by}_k(\bx)$. \\
\fbox{
  \parbox{\textwidth}{
\textbf{Assumption 1 (A1)}: 

Covariance structure for all residual processes $\bepsilon_k$ can be well captured by a common kernel family $k_\epsilon(\btheta)$. 
  }
}

Denote $\bK_\epsilon$ as the kernel matrix generated by $k_\epsilon$, then GP model adopts below prior:
\begin{align*}
\bepsilon_k | \btheta_{\epsilon, k} &\sim \Gsc\Psc_k \Big(0, \; \bK_{\epsilon}(\btheta_{\epsilon, k}) + \sigma_k^2\bI \Big)
\end{align*}
where $\btheta_{\epsilon, k}$ are the kernel parameters with ARD prior. 

Correspondingly, if denote $\bK_{\epsilon,k} = \bK_{\epsilon}(\btheta_{\epsilon, k})$, the predictive posterior is \citep{rasmussen_gaussian_2006}:
\begin{align*}
\bepsilon_k | \btheta_{\epsilon, k}, \hat{\bepsilon}_k &\sim \Gsc\Psc_k \Big(\bK_{\epsilon,k}(\bK_{\epsilon,k} + \sigma_k^2\bI)\bepsilon_k, \; \bK_{\epsilon,k} - \bK_{\epsilon,k}(\bK_{\epsilon,k} + \sigma_k^2\bI)^{-1}\bK_{\epsilon,k} \Big)
\end{align*}
therefore, the model-specific predictive posterior for observation $\by = \bepsilon_k + \hat{\by}_k$ as:
\begin{align*}
\by ~ &\sim \Gsc\Psc_k \Big(\hat{\by}_k + \bK_{\epsilon,k}(\bK_{\epsilon,k} + \sigma_k^2\bI)\bepsilon_k, \; \bK_{\epsilon,k} - \bK_{\epsilon,k}(\bK_{\epsilon,k} + \sigma_k^2\bI)^{-1}\bK_{\epsilon,k} \Big)
\end{align*}

\fbox{
  \parbox{\textwidth}{
\textbf{Test Case (T1)}: 

Rationale: By \citep{krogh_neural_1994}, a simple bias-variance trade-off derivation shows that the cross-validation error of overall ensemble can be written as $$\epsilon_{ens} = E(\epsilon_{k}) - Var(\epsilon_{k})$$
Therefore we need to examine $E(\epsilon_{k})$ and $Var(\epsilon_{k})$ of model-specific GPs and make sure they give similar (or better) ensemble performance compared to that of the original model-specific predicts. 

\begin{enumerate}
\item Examine $E(\epsilon_{k})$ of the model-specific GP,  compare to that of the original model predicts.
\item Examine $Cov(\epsilon_{k})$ of model-specific GPs, compare to that of the original model predicts.
\end{enumerate}

(Compared to that of the original prediction, we expect decreased $E(\epsilon_{k})$ and decreased $Var(\epsilon_{k})$ for model-specific GPs, because these model-specific GPs are essentially estimates from a two-model GAMs estimated using backfitting).

  }
}

In current implementation, we use RBF for $k_\epsilon$.\\

\underline{Step 1 Experiment}

First visualize residual process from individual model:

\begin{table}[!htbp] 
\centering 
  \label{} 
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|} 
\hline \hline 
Model & Linear & Poly 2 & Poly 3 & Poly 4 & RBF & Matern 1/2 & Matern 3/2 & Matern 5/2 & MLP 
\\ \hline 
CV, Original & 18.5847 & 4.4316 & 4.5581 & 4.9085 & 4.7633 & 4.7373 & 4.7784 & 4.7644 & 4.7614 
\\ 
CV, GP & 4.5211 & 4.5217 & 4.5217 & 4.5216 & 4.5217 & 4.5192 & 4.5208 & 4.5212 & 4.5217 
\\
\hline\hline
\end{tabular} 
\caption{CV Performance for pre-processing step} 
\label{tb:res_joint}
\end{table} 

Variance among original predictors: 1.4226, Variance among GP predictors: $6.2087 \times 10^{-3}$.


\begin{figure}[ht]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/1_res_Linear"}
  \caption{Linear}
  \label{fig:sfig11}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/2_res_Poly2"}
  \caption{Polynomial, Degree 2}
  \label{fig:sfig12}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/3_res_Poly3"}
  \caption{Polynomial, Degree 3}
  \label{fig:sfig13}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/4_res_Poly4"}
  \caption{Polynomial, Degree 4}
  \label{fig:sfig21}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/5_res_RBF_ARD"}
  \caption{RBF with ARD}
  \label{fig:sfig22}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/6_res_Matern_12_ARD"}
  \caption{Mat\'{e}n $\frac{1}{2}$ with ARD}
  \label{fig:sfig23}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/7_res_Matern_32_ARD"}
  \caption{Mat\'{e}n $\frac{3}{2}$ with ARD}
  \label{fig:sfig21}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/8_res_Matern_52_ARD"}
  \caption{Mat\'{e}n $\frac{5}{2}$ with ARD}
  \label{fig:sfig22}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/9_res_MLP_ARD"}
  \caption{MLP with ARD}
  \label{fig:sfig23}
\end{subfigure}

\caption{Residual Process from individual models}
\label{fig:res}
\end{figure}


\clearpage
\subsection{\textbf{Step 2-3: Learn Ensemble Weight \& Predictive Distribution}}


\textbf{Based on discussion on 03/22, try a simplied ensemble}:

\underline{Step 1}: Take prediction as it is, don't learn model-specific GP:
\begin{alignat*}{3}
\{\hat{y}_k(\bx)\}_{k=1}^K
\end{alignat*}

\underline{Step 2}: Learn ensemble weights $\{w_k(\bx)\}_{k=1}^K$

For representing the ensemble weights, we have two options here:
\begin{enumerate}
\item Dirichlet: $w_k \sim Dirichlet(exp(w'_k))$, or equivalently:
\begin{alignat*}{3}
w_k &= \frac{a_k}{\sum_{k=1}^K a_k} \qquad 
a_k \sim Gamma(exp(w'_k), 1)
\end{alignat*}
\item Logistic Normal: $w_k = softmax(exp(w'_k + \epsilon))$, where $\epsilon \sim N(0, 1)$ or equivalently:
\begin{alignat*}{3}
w_k &= \frac{a_k}{\sum_{k=1}^K a_k} \qquad 
a_k \sim logNormal(w'_k, 1)
\end{alignat*}
A more flexible, better behaved distribution compared to Dirichlet \citep{aitchison_logistic-normal_1980}.
\end{enumerate}
for both options, we have:
\begin{alignat*}{3}
w'_k(\bx) & \sim \Gsc\Psc(\bzero, \; k_{w,k}(\bx, \bx') + \sigma_k^2 )
\end{alignat*}

\underline{Step 3}: Generate ensemble predictive distribution:
\begin{align*}
y &\sim N \bigg(
\sum_{k=1}^K w_k(\bx) \hat{y}_k(\bx) + \bepsilon(\bx), \; \bI \bigg)\\
\bepsilon(\bx) &\stackrel{iid}{\sim} N(0, 1)
\end{align*}
here $\bepsilon(\bx)$ is a residual process that serves as "sculpting clay" that makes up for the ensemble, in case that in some space-time location where no base model predicts well.

Note:
\begin{enumerate}
\item For model-specific GP $w'_k(\bx)$, flexibility of these GPs will likely to  have non-trivial influence on model behavior. For example, a very restrictive kernel (say, constant kernel) forces $w_k(\bx)$ to be a constant, in this case, the Step 3 becomes essentially Bayesian linear regression (which, depends on the sample size, may not be a bad choice). On the other hand, a very flexible kernel may lead to model overfit when sample size is small. Need to investigate this in simulation.
\item For Step 2, both options consistutes valid construction of normalized random measure {\color{red} (need to verify if this statement is correct)} over the candidate models. However, compare to Dirichlet, Logistic Normal is empirically much easier to sample both in terms of computation speed ($\times$30 faster under NUTS) and more efficient in terms of sample quality (higher ESS). {\color{red} Question: how do we decide which option is better?}
\end{enumerate}

\newpage
\subsubsection{\textbf{Posterior Visualization}}

\underline{Ensemble Weights}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\textwidth]{"./plot/weight_dist_dirch"}
\caption{Dirichlet Representation}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\textwidth]{"./plot/weight_dist"}
\caption{Logistic Normal Representation}
\end{figure}


Recall the prediction performance in first stage:
\begin{table}[!htbp] 
\centering 
  \label{} 
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} 
\hline \hline 
 Linear & Poly 2 & Poly 3 & Poly 4 & RBF & Matern 1/2 & Matern 3/2 & Matern 5/2 & MLP & Spectral
\\ \hline 
 19.8786 & 5.7871 & 5.8365 & 5.7879 & 5.5445 & 5.6745 & 6.0123 & 6.0106 & 5.9457 & 5.5445
\\
\hline\hline
\end{tabular} 
\caption{CV Performance of base models} 
\label{tb:res_joint}
\end{table}



\clearpage
\section{\textbf{Timeline}}
\begin{enumerate}
\item (March. Week 3-4) \\
Initial implementation. 
\item (March. Week 4 - April. Week 1) \\
Initial experiment on simulated data.
Investigating:
\begin{enumerate}
\item Sensitivity to model and kernel specification
\item Whether improved performance for overall ensemble, how it is tighed to the property of the base models. More specifically:
\begin{enumerate}
\item Variance among model prediction
\item Number and bias of models (few strong model with good prediction for different aspect of the data, or large collection of weak models)
\end{enumerate}
\item Identifiability of individual weights
\end{enumerate} 
\item (April. Week 2-3) \\
More realistic experiment, gradually increase complexity of data-generation mechanism toward (using the mean-surface from QD or Itai's prediction)
\item (April. Week 4) Start experiment on real data
\end{enumerate}

\newpage
\section{\textbf{Direction of Improvement}}

Two idea for improvement: 
\begin{enumerate}
\item \textbf{Step 1}, it should still be ok to use Gaussian process. Since under a zero-mean GP, the marginal distribution of $\epsilon(\bx) = y(\bx) - \hat{y}(\bx)$ is still zero-mean, i.e. $[\epsilon(\bx_{obs}), \epsilon(\bx^*)] \sim N(\bzero, \bK)$ for  training data $\bx_{obs}$ and testing data $\bx^*$. It is just that the conditional posterior $\epsilon(\bx^*)|\epsilon(\bx_{obs})$ follows a non-zero mean Gaussian. However, since our focus in this step is just recovering a probability distribution from data, the focus should be learn hyperparameter of the kernel functions rather performing prediction. (However, still a good idea to make the kernel restrictive)
\item \textbf{Step 3}, model the residual process of overall ensemble $\bepsilon(\bx)$ using a (infinite) random feature ensemble \citep{rahimi_weighted_2009} to enhance prediction. In this case, since support of model index $k$ is countably infinite, we can put a Bayesian nonparametric prior on $w_k(\bx)$ (say, a GP-based dependent DP-Mixture \cite{jara_class_2011}), which makes $\sum_{k=1}^\infty w_k(\bx)y_k(\bx)$ a weak-limit approximation of the Bayesian nonparametric integration over a space of spatial termporal processes.
\end{enumerate}

\clearpage
\bibliography{../report}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
