\documentclass[11pt]{article}

\usepackage{natbib}
\bibliographystyle{unsrtnat}

% change document font family to Palatino, and code font to Courier
\usepackage{mathpazo} % add possibly `sc` and `osf` options
%\usepackage{eulervm}
%\usepackage{courier}
%allow formula formatting

% flow chart
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{bayesnet}

%identation in nested enumerates
\usepackage[shortlabels]{enumitem}
\setlist[enumerate,1]{leftmargin=1cm} % level 1 list
\setlist[enumerate,2]{leftmargin=2cm} % level 2 list

%flush align equations to left, this also loads amsmath 
%\usepackage[fleqn]{mathtools}
\usepackage{mathtools}
\usepackage{amsthm}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\usepackage{comment}

% allow algorithm 
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

%allow equation numbering in {align*} envir
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%declare math symbolz
%# inner product
\DeclarePairedDelimiter{\inner}{\langle}{\rangle}

%declare argmin
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

%declare checkmark
\usepackage{amssymb}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%title positon
\usepackage{titling} %fix title
\setlength{\droptitle}{-6em}   % Move up the title 

%change section title font size
\usepackage{titlesec} 
\titleformat{\section}
  {\normalfont\fontsize{12}{15}}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\fontsize{12}{13}}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\fontsize{12}{13}}{\thesubsubsection}{1em}{}

%overwrite bfseries to allow formula in section title  
\def\bfseries{\fontseries \bfdefault \selectfont \boldmath}

% change page margin
\usepackage[margin=0.8 in]{geometry} 

%disable indentation
\setlength\parindent{0pt}

%allow inserting multiple graphs
\usepackage{graphicx}
\usepackage[skip=1pt]{subcaption}
\usepackage[justification=centering,font=small]{caption}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}%indep sign

%allow code chunks
\usepackage{listings}
%\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{frame=lrbt,xleftmargin=\fboxsep, xrightmargin=-\fboxsep}
\lstset{language=R, commentstyle=\bfseries, 
keywordstyle=\ttfamily} %R-related formatting
\lstset{escapeinside={<@}{@>}}

%allow merged cell in tables
\usepackage{multirow}

%allow http links
\usepackage{hyperref}

%allow different font colors
\usepackage{xcolor}

%Thm and Def environment
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}

\newenvironment{definition2}[1][Definition]{\begin{trivlist} %def without index
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newenvironment{example}[1][Example]{\begin{trivlist} %def without index
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

%allow strikeout effect
\usepackage[normalem]{ulem}

%macros from Bob Gray
\usepackage{"./macro/GrandMacros"}
\usepackage{"./macro/Macro_BIO235"}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% TItle page with contents %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Progress Report \\ Model Setup \\ \today \vspace{-1ex}}

\pretitle{\begin{flushright}\normalsize}
\posttitle{\par\end{flushright}}
\author{}
\date{}
\vspace{-10em}
\maketitle
\vspace{-5em}

\tableofcontents 
%\vspace{10em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Formal Sections %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{\textbf{Model Overview}}
Input: data $\{\bx_i, y_i\}_{i=1}^n$, and prediction from base models $\{\hat{\by}_k\}_{k=1}^K$.\\

\underline{Step 1}: Take prediction as it is, don't learn model-specific GP:
\begin{alignat*}{3}
\{\hat{y}_k(\bx)\}_{k=1}^K
\end{alignat*}

\underline{Step 2}: Learn ensemble weights $\{w_k(\bx)\}_{k=1}^K$

\begin{alignat*}{3}
w_k &= \frac{exp(w'_k)}{\sum_{k=1}^K exp(w'_k)} \qquad 
\mbox{where}\\
w'_k(\bx) & \sim \Gsc\Psc(\bzero, \; k_{w,k}(\bx, \bx') + \sigma_k^2 )
\end{alignat*}
Under above construction, $w_k$ follows logistic normal distribution \citep{aitchison_logistic-normal_1980}, which is more flexible (slightly heavier tail) and slightly better behaved (in terms of sampling) compared to Dirichlet . From the view of Bayesian nonparametrics, this construction corresponds to a tail-free dependent process prior for $w_k$ \citep{jara_class_2011}.

Another option is to assume $w_k \sim Dirichlet(exp(w'_k))$. Initial experiments show that this formulation has less stability, in that slight perturbation in training set can lead to drastic variation in prediction. However it does have the benefit of being a conjugate prior for multinomial distribution, which we can potentially deploy at the third stage. We will later revisit this formulation for more detailed analysis.\\

\underline{Step 3}: Generate ensemble predictive distribution:
\begin{align*}
y &\sim N \bigg(
\sum_{k=1}^K w_k(\bx) \hat{y}_k(\bx) + \bepsilon(\bx), \; \bI \bigg)\\
\bepsilon(\bx) &= \sum_{l=1}^L\epsilon_l(\bx) \qquad \epsilon_{l}(\bx) \stackrel{iid}{\sim} CART(\bx)
\end{align*}
where the residual process $\bepsilon(\bx)$ is modelled  using an ensemble of CART trees, where each tree is modelled in a fashion similar to that used by gradient boosting \citep{chen_xgboost:_2016}. So that it serves as additionally "pasting clay" to complement the prediction of the base models.

\subsection*{\underline{Comments}}

\begin{enumerate}
\item For model-specific GP $w'_k(\bx)$ in \textbf{Step 2}, flexibility of these GPs will impact on the ensemble model's predictive behavior in \textbf{Step 3}. For example, a very restrictive kernel (say, constant kernel) forces $w_k(\bx)$ to be a constant for all $\bx$, making Step 3  essentially Bayesian linear regression (which, depends on the sample size, may not be a bad choice). On the other hand, a very flexible kernel may lead to model overfit when sample size is small. See Figure \ref{fig:cv_error}
\item For Step 2, both models for $w_k$ consistutes valid construction of normalized random measure over the space of candidate models. However, compare to Dirichlet, Logistic Normal is empirically much easier to sample,  in terms of (1) computation speed ($\times$30 faster under NUTS) and (2) sample quality (higher ESS). We shall evaluate the relative merit of two options by evaluating their cross validation performance.
\end{enumerate}


\section{\textbf{Experiment}}

\subsection{\underline{\textbf{Simulated Data}}}
First generate data from a spatial gaussian process with 250 locations, we sample spatial locations $(x, y)$ iid from standard normal, and assume the pollutant $z$ follow below Gaussian Process:
\begin{align*}
z(x, y) &\stackrel{iid}{\sim} N(f(x, y), \sigma^2 = 0.1) \\
f(x, y) &= 0.2 x + 0.5 y + \sqrt{x^2 + y^2 + 5*cos(x*y)} + 
sin(x) + cos(x) + log\Big(exp(x*y)+exp(x)\Big)
\end{align*}

\begin{figure}[!ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{"./plot/loc_site"}
  \caption{Sampled saptial location for monitoring sites (standardized)}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"./plot/y_surface"}
  \caption{Simulated pollution concentration surface over space}
  \label{fig:sfig2}
\end{subfigure}
%\caption{plots of....}
\label{fig:fig}
\end{figure}

We then generate prediction for $z(x, y)$ from 10 base GP models \citep{duvenaud_structure_2013}:
\begin{itemize}
\item 4 Polynomial Kernels, degree 1, 2, 3, 4
\item Gaussian RBF Kernel, with ARD (i.e. automated relevance determination \citep{neal_mcmc_2012})
\item 3 Mat\'{e}rn Kernels: $\nu = \frac{1}{2}$, $\frac{3}{2}$, $\frac{5}{2}$ with ARD \\
(Corresponding to non-, one- and twice-differentiable functions from Sobolev space)
\item MLP, with ARD. \citep{williams_computation_1998} \\
(a 1-layer BNN with Gaussian CDF activation $\&$ infinite hidden units)
\item Spectral Mixture \citep{wilson_gaussian_2013}\\
(Inverse fourier transformation of a Gaussian mixture. Theoretically can model the spectral density of arbitrary stationary functions)
\end{itemize} 


\newpage

\subsection{\underline{\textbf{Result}}}
\subsubsection{\underline{Individual Model}}

First visualize residual process from individual model:

\begin{table}[!htbp] 
\centering 
  \label{} 
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} 
\hline \hline 
Linear & Poly 2 & Poly 3 & Poly 4 & RBF & Matern 1/2 & Matern 3/2 & Matern 5/2 & MLP & SpecMix
\\ \hline 
19.8766 & 5.7871 & 5.8365 & 5.7879 & 5.5445 & 5.6745 & 6.0123 & 6.0106 & 5.9457 & 5.5445
\\
\hline\hline
\end{tabular} 
\caption{CV Performance for base models} 
\label{tb:resid_joint}
\end{table} 

\begin{figure}[ht]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/1_resid_Linear"}
  \caption{Linear}
  \label{fig:sfig11}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/2_resid_Poly2"}
  \caption{Polynomial, Degree 2}
  \label{fig:sfig12}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/3_resid_Poly3"}
  \caption{Polynomial, Degree 3}
  \label{fig:sfig13}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/4_resid_Poly4"}
  \caption{Polynomial, Degree 4}
  \label{fig:sfig21}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/5_resid_RBF_ARD"}
  \caption{RBF with ARD}
  \label{fig:sfig22}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/6_resid_Matern_12_ARD"}
  \caption{Mat\'{e}n $\frac{1}{2}$ with ARD}
  \label{fig:sfig23}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/7_resid_Matern_32_ARD"}
  \caption{Mat\'{e}n $\frac{3}{2}$ with ARD}
  \label{fig:sfig21}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/8_resid_Matern_52_ARD"}
  \caption{Mat\'{e}n $\frac{5}{2}$ with ARD}
  \label{fig:sfig22}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/resid/9_resid_MLP_ARD"}
  \caption{MLP with ARD}
  \label{fig:sfig23}
\end{subfigure}

\caption{residual process from individual base models}
\label{fig:res}
\end{figure}

\newpage
\subsubsection{\underline{Overall Performance: Cross validation Error}}
Given observed data $\by_{obs}$, and K model predictions $\widehat{\bY}_{obs}(\bx) = \{\hat{\by}(\bx)_{k}\}_{k=1}^K$,  we estimate $\bw = \{ w_k(\bx)\}_{k=1}^K$, and evaluate resulting ensemble $\hat{\by}_{ens, obs}(\bx) = \widehat{\bY}_{obs}(\bx)\bw(\bx) $.

In this sets of experiments, we model ensemble weights using the logistic normal formulation $w_k = softmax(w'_k)$, where $w'_k$ follows a Gaussian process with RBF kernel $exp(-\frac{||\bx - \bx'||^2}{\sigma})$. We vary the hyperparameter $\sigma$ to examine its impact on model's 5-fold cross-validation error $||\by_{cv, obs} - \hat{\by}_{ens, obs}(\bx_{cv})||_2^2$. \\
(In order to save time, I followed John's advice and used MAP estimates).\\

Additionally, since in addition to $\by_{obs}$, we also have base model's cross-validation prediction in spatial regions that is not in training data $\by_{obs}$ (call this $\bx_{pred}$), we also compute the "true" cross-validation error $||\by_{cv, pred} - \hat{\by}_{ens, obs}(\bx_{pred})||_2^2$.

\begin{figure}[ht]
\centering
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"./plot/cv_error_obs"}
  \caption{Cross-validation Error, based on $\by_{obs}$.\\
  x-axis: $\sigma^2$ of the RBF kernel for weight GP}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"./plot/cv_error_pred"}
  \caption{Cross-validation Error, based on $\by_{pred}$.\\
  x-axis: $\sigma^2$ of the RBF kernel for weight GP}
\end{subfigure}\hspace*{-0.1em}
\caption{5-fold Cross-validation Error in training set and hold-out set}
\label{fig:cv_error}
\end{figure}

\textbf{Interpretation of the plot}: 

The x-axis is the value for $\sigma^2$, the hyperparameter for the RBF kernel for weight GP, larger value of $\sigma^2$ indicates a more restrictive $w_k(\bx)$ (in terms of how $w_k(\bx)$ can vary as a function of $\bx$). In the extreme case of $\sigma^2 > 10$, $w_k(\bx)$ is nearly constant across all spatial locations.\\

Left plot is based on model's in-sample prediction $\widehat{\bY}_{obs}(\bx) = \{\hat{\by}_{obs}(\bx)_{k}\}_{k=1}^K$. As we can see, as $w_k(\bx)$ becomes more restrictive, the train/cv error based on in-sample data becomes smaller and are close to each other. We can understand this from the view-point of bias-variance trade-off: since all in-sample predictions $\hat{\by}_{obs}(\bx)_{k}$ are already very close to data $\by_{obs}$, there is little to no  bias in the ensemble model regardless of how restrictive the weights are (that is to say, since the base model prediction are already perfect even across space, using constant weights is sufficient to guarantee good prediction). But at the same time, since more flexible model of $w_k(\bx)$ has higher variance, more flexible models with smaller $\sigma^2$ will (somehow counterintuitively) give higher overall error.

On the other hand, in the right plot, the CV error based on unobserved data $\by_{pred}$ (i.e. data that base models have no access to during training) shows classical "dipping valley" pattern. The model has best performance at $\sigma^2 \in (2, 2.5)$. From the view of bias-variance trade-off, this tells us that there exists bias among $\hat{\by}_{obs}(\bx_{pred})$ from the base models, therefore a somewhat flexible model can help striking a better balance in terms of mitigating bias while controlling estimator variance.

In conclusion, allowing $w_k(\bx)$ to be a function of $\bx$ is useful in improving prediction of model ensemble. Our approach is promising



\newpage
\subsubsection{\underline{Posterior Estimate: Posterior Mean for  Ensemble Weights $w_k(\bx)$}}
Using the logistic normal formulation for weights $w_k = softmax(exp(w'_k + \epsilon))$, where $w'_k$ follows a Gaussian process with RBF kernel with $\sigma^2 = 2$.
\begin{figure}[ht]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/weight/1_weight_Linear"}
  \caption{Linear}
  \label{fig:sfig11}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/weight/2_weight_Poly2"}
  \caption{Polynomial, Degree 2}
  \label{fig:sfig12}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/weight/3_weight_Poly3"}
  \caption{Polynomial, Degree 3}
  \label{fig:sfig13}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/weight/4_weight_Poly4"}
  \caption{Polynomial, Degree 4}
  \label{fig:sfig21}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/weight/5_weight_RBF_ARD"}
  \caption{RBF with ARD}
  \label{fig:sfig22}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/weight/6_weight_Matern_12_ARD"}
  \caption{Mat\'{e}n $\frac{1}{2}$ with ARD}
  \label{fig:sfig23}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/weight/7_weight_Matern_32_ARD"}
  \caption{Mat\'{e}n $\frac{3}{2}$ with ARD}
  \label{fig:sfig21}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/weight/8_weight_Matern_52_ARD"}
  \caption{Mat\'{e}n $\frac{5}{2}$ with ARD}
  \label{fig:sfig22}
\end{subfigure}\hspace*{-0.1em}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{"../../data/plot/weight/9_weight_MLP_ARD"}
  \caption{MLP with ARD}
  \label{fig:sfig23}
\end{subfigure}
\caption{posterior mean for ensemble weight $w_k(\bx)$ from individual models}
\label{fig:ens_weight}
\end{figure}

\textbf{Interpretation of the plot}: 

Recall that the ensemble weights are trained only on 250 spatial locations as shown in Figure \ref{fig:sfig1}. There exists high uncertainty in four corners of the spatial region due to lack of data, which explains the unstable behavior of individual weight surfaces on the corners.




\clearpage
\section{\textbf{Direction of Improvement}}

Two idea for improvement: 
\begin{enumerate}
\item \textbf{Step 1, recover probability distribution using a simpler model}. \\ 
I think it should still be ok to use Gaussian process (but will a simpler kernel). The rationale being: under a zero-mean GP, for  training data $\bx_{obs}$ and testing data $\bx^*$, the marginal distribution of $[\epsilon(\bx_{obs}), \epsilon(\bx^*)] \sim N(\bzero, \bK)$ is still zero-mean. It is just that the mean for conditional posterior $\epsilon(\bx^*)|\epsilon(\bx_{obs})$ is non-zero. Since our focus in this step is just to recover a probability distribution, the focus should be learn hyperparameter of the kernel functions rather than looking at conditional distribution between observations. (However, still a good idea to make the kernel restrictive)
\item \textbf{Step 3, Enhance prediction by adding a model for the overall residual}\\
Model the residual process of overall ensemble $\bepsilon(\bx)$ using a (infinite) ensemble gradient boosting trees/ random feature \citep{rahimi_weighted_2009} to enhance prediction. \\
This approach leads to enhanced model prediction, and also  opportunity to develope new theory since by adding infinitely many random base models to the ensemble, the space of model index $k \in \Ksc$ becomes countably infinite. In this case, we can model $w_k(\bx)$ with a, say, Dirichlet process prior (e.g. use the GP-based dependent DP construction \citep{jara_class_2011}) following the Bayesian nonparametric formalism , making $\sum_{k=1}^\infty w_k(\bx)y_k(\bx)$ a weak-limit approximation of $\int_{k \in \Ksc} w_k(\bx)y(\bx|k) dk = y_{ensemble}(\bx)$. 

Our approach is distinct from traditional Bayesian model averaging (BMA) in that:
\begin{enumerate}
\item requires only model prediction from the base model (instead of the full posterior of base models, as required by BMA)
\item allow ensemble weights to vary across space and time (i.e. the feature space). (and also model sparsity).
\item opportunity to improve upon the base-model-only ensemble (due to incorporation of random feature for modeling the residual process).
\item potential for elegant (Bayesian nonparametrics) interpretation, in case we seek to  publish in statistics journal.For marketing purpose, maybe we can call this\\ \textit{Bayesian nonparametric integration of infinite spatio-termporal ensemble}? :).
\end{enumerate}


\end{enumerate}

\clearpage
\section{\textbf{Timeline}}
\begin{enumerate}
\item (April. Week 2) \\
Initial implementation. 
\item (April. Week 3-4) \\
Initial experiment on simulated data.
Investigating:
\begin{enumerate}
\item Sensitivity to model and kernel specification
\item Whether improved performance for overall ensemble, how it is tighed to the property of the base models. More specifically:
\begin{enumerate}
\item Variance among model prediction
\item Number and bias of models (few strong model with good prediction for different aspect of the data, or large collection of weak models)
\end{enumerate}
\item Identifiability of individual weights
\end{enumerate} 
\item (May. Week 1-2) \\
More realistic experiment, gradually increase complexity of data-generation mechanism toward (using the mean-surface from QD or Itai's prediction)
\item (May. Week 3) Initial result on real data
\end{enumerate}

\clearpage
\bibliography{../report}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
